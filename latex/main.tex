% ------------------------------------------------------------------------
% Senac Tex: Modelo de Trabalho Academico para o Centro Universitário
% Senac
% ------------------------------------------------------------------------

% ========================================================================
% CONFIGURAÇÃO DO DOCUMENTO
% ========================================================================


\documentclass[
% -- opções da classe memoir --
    12pt,                % tamanho da fonte
    openright,            % capítulos começam em pág ímpar (insere página vazia caso preciso)
    oneside,            % para impressão em verso e anverso. Oposto a oneside
    a4paper,            % tamanho do papel.
% -- opções da classe abntex2 --
%chapter=TITLE,		% títulos de capítulos convertidos em letras maiúsculas
%section=TITLE,		% títulos de seções convertidos em letras maiúsculas
%subsection=TITLE,	% títulos de subseções convertidos em letras maiúsculas
%subsubsection=TITLE,% títulos de subsubseções convertidos em letras maiúsculas
% -- opções do pacote babel --
    english,            % idioma adicional para hifenização
    brazil                % o último idioma é o principal do documento
]{abntex2}

% ---
% Pacotes básicos
% ---
\usepackage{lmodern}            % Usa a fonte Latin Modern
\usepackage[T1]{fontenc}        % Selecao de codigos de fonte.
\usepackage[utf8]{inputenc}        % Codificacao do documento (conversão automática dos acentos)
\usepackage{lastpage}            % Usado pela Ficha catalográfica
\usepackage{indentfirst}        % Indenta o primeiro parágrafo de cada seção.
\usepackage{color}                % Controle das cores
\usepackage{graphicx}            % Inclusão de gráficos
\usepackage{microtype}            % para melhorias de justificação
\usepackage{listings}
\usepackage[top=3cm, bottom=2cm, left=3cm, right=2cm]{geometry}
\usepackage{amsmath}
\usepackage{float}

% ---
% Pacotes de citações
% ---
\usepackage[brazilian,hyperpageref]{backref}     % Paginas com as citações na bibl
\usepackage[alf]{abntex2cite}    % Citações padrão ABNT

% CONFIGURAÇÕES DE PACOTES

% Configurações do pacote backref
% Usado sem a opção hyperpageref de backref
\renewcommand{\backrefpagesname}{Citado na(s) página(s):~}
% Texto padrão antes do número das páginas
\renewcommand{\backref}{}
% Define os textos da citação
\renewcommand*{\backrefalt}[4]{
    \ifcase
        #1 %
        Nenhuma citação no texto.%
        \or
        Citado na página #2.%
    \else
        Citado #1 vezes nas páginas #2.%
    \fi}%

% Informações de dados para CAPA e FOLHA DE ROSTO
\titulo{Avaliação de métodos estatísticos na busca de correspondência textual jurídica frente a \textit{embeddings}}
\autor{Arthur Andrade e Davi Henrique}
\local{São Paulo - Brasil}
\data{2025}
\orientador{Afonso Lelis}
%\coorientador{Nome do Coorientador}
\instituicao{
    Centro Universitário Senac - Santo Amaro
    \par
    Bacharelado em Ciência da Computação
}
\tipotrabalho{Trabalho de Conclusão de Curso}
% O preambulo deve conter o tipo do trabalho, o objetivo,
% o nome da instituição e a área de concentração
\preambulo{Monografia apresentada na disciplina Trabalho de Conclusão de Curso, como parte dos requisitos para obtenção do título de Bacharel em Ciência da Computação.}

% Configurações de aparência do PDF final

% alterando o aspecto da cor azul
\definecolor{blue}{RGB}{41,5,195}

% informações do PDF
\makeatletter
\hypersetup{
%pagebackref=true,
    pdftitle={\@title},
    pdfauthor={\@author},
    pdfsubject={\imprimirpreambulo},
    pdfcreator={LaTeX with abnTeX2},
    pdfkeywords={NLP}{Direito}{Recuperação de Informação}{BM25}{Embeddings},
    colorlinks=true,            % false: boxed links; true: colored links
    linkcolor=black,            % color of internal links
    citecolor=black,                % color of links to bibliography
    filecolor=magenta,            % color of file links
    urlcolor=black,
    bookmarksdepth=4
}
\makeatother

% Espaçamentos entre linhas e parágrafos

% O tamanho do parágrafo é dado por:
\setlength{\parindent}{1.25cm}

% Controle do espaçamento entre um parágrafo e outro:
\setlength{\parskip}{0.2cm}

\SingleSpacing
\makeatletter
\let\@fnsymbol\@arabic
\makeatother

% compila o indice
\makeindex

\begin{document}

% Retira espaço extra obsoleto entre as frases.
    \frenchspacing

% ========================================================================
% CAPA
% ========================================================================
    \imprimircapa

% ========================================================================
% FOLHA DE ROSTO
% ========================================================================
    \imprimirfolhaderosto

% ========================================================================
% DEDICATÓRIA
% ========================================================================
    \begin{dedicatoria}
        \vspace*{\fill}
        \centering
        \noindent
        \textit{ Dedicamos este trabalho aos nossos pais, que sempre foram nosso alicerce, oferecendo o apoio incondicional e o incentivo necessário para que pudéssemos trilhar este caminho de aprendizado. A todos que acreditaram no nosso potencial e nos motivaram a nunca desistir dos nossos sonhos.} \vspace*{\fill}
    \end{dedicatoria}

% ========================================================================
% AGRADECIMENTOS
% ========================================================================
    \begin{agradecimentos}
        Primeiramente, agradecemos aos nossos pais, nossa eterna gratidão pelo amor, paciência e apoio incondicional. Vocês são a base de todas as nossas conquistas.

        Ao nosso orientador, Professor Afonso Lelis, e ao Centro Universitário Senac, agradecemos pela orientação segura, pela estrutura oferecida e pelos conhecimentos compartilhados, que foram essenciais para o desenvolvimento deste trabalho e para a nossa formação profissional.

        Por fim, aos amigos e colegas de curso, obrigado pelo companheirismo e pela troca de experiências que tornaram essa caminhada mais leve.
    \end{agradecimentos}

% ========================================================================
% EPÍGRAFE
% ========================================================================
    \begin{epigrafe}
        \vspace*{\fill}
        \begin{flushright}
            \textit{``A beleza é realmente um bom dom de Deus; mas que os bons não pensem que ela é um grande bem, pois Deus a distribui mesmo para os maus. \\
            (Santo Agostinho)``}
        \end{flushright}
    \end{epigrafe}

% ========================================================================
% RESUMO
% ========================================================================
\setlength{\absparsep}{18pt} % ajusta o espaçamento dos parágrafos do resumo
\begin{resumo}

    A digitalização massiva do setor jurídico brasileiro impõe desafios críticos de recuperação de informação. Enquanto modelos de Inteligência Artificial baseados em \textit{embeddings} oferecem alta precisão semântica, seu custo computacional é elevado. Este trabalho investiga o \textit{trade-off} pragmático entre complexidade semântica e eficiência estatística. Para isso, foi desenvolvido um sistema comparativo integralmente em linguagem Go, implementando os algoritmos TF-IDF e Okapi BM25, confrontados com um modelo \textit{transformer} (Sentence-BERT). Os testes, realizados sobre um \textit{corpus} legislativo de aproximadamente 5.000 documentos, revelaram que a busca estatística é significativamente mais eficiente: o uso de Unigramas exigiu apenas 143 MB de memória RAM, contra 4,7 GB na configuração de Trigramas. Em termos de velocidade, o algoritmo BM25 mostrou-se até 10 vezes mais rápido que o TF-IDF em cenários complexos. A análise qualitativa via Coeficiente de Spearman indicou uma correlação baixa (entre 0,02 e 0,06) entre os \textit{rankings} estatísticos e semânticos, sugerindo que as abordagens capturam aspectos distintos da relevância. Conclui-se que, para o domínio jurídico, métodos estatísticos oferecem um custo-benefício superior para filtragem inicial, sendo a arquitetura híbrida a solução ideal.

    \textbf{Palavras-chave}: Processamento de Linguagem Natural. Recuperação de Informação. Busca Jurídica. TF-IDF. BM25. Embeddings.
\end{resumo}

% ========================================================================
% ABSTRACT
% ========================================================================
\begin{resumo}[Abstract]
    \begin{otherlanguage*}{english}
        The massive digitization of the Brazilian legal sector imposes critical information retrieval challenges. While AI models based on embeddings offer high semantic precision, their computational cost is high. This work investigates the pragmatic trade-off between semantic complexity and statistical efficiency. To this end, a comparative system was developed entirely in Go, implementing TF-IDF and Okapi BM25 algorithms, confronted with a transformer model (Sentence-BERT). Tests performed on a legislative corpus of approximately 5,000 documents revealed that statistical search is significantly more efficient: using Unigrams required only 143 MB of RAM, versus 4.7 GB in the Trigram configuration. In terms of speed, the BM25 algorithm proved to be up to 10 times faster than TF-IDF in complex scenarios. Qualitative analysis via Spearman's Coefficient indicated a low correlation (between 0.02 and 0.06) between statistical and semantic rankings, suggesting that the approaches capture distinct aspects of relevance. It is concluded that, for the legal domain, statistical methods offer superior cost-benefit for initial filtering, with hybrid architecture being the ideal solution.

        \textbf{Keywords}: Natural Language Processing. Information Retrieval. Legal Search. TF-IDF. BM25. Embeddings.
    \end{otherlanguage*}
\end{resumo}

% ========================================================================
% LISTA DE ILISTRAÇÕES
% ========================================================================
    \pdfbookmark[0]{\listfigurename}{lof}
    \listoffigures*
    \cleardoublepage

% ========================================================================
% LISTA DE TABELAS
% ========================================================================
% \pdfbookmark[0]{\listtablename}{lot}
% \listoftables*
% \cleardoublepage

% ========================================================================
% LISTA DE ABREVIATURAS E SIGLAS
% ========================================================================
    \begin{siglas}
        \item[API] Application Programming Interface
        \item[BERT] Bidirectional Encoder Representations from Transformers
        \item[CBOW] Continuous Bag-of-Words
        \item[CNJ] Conselho Nacional de Justiça
        \item[CPU] Computing Processing Unit
        \item[GPU] Graphics Processing Unit
        \item[CRF] Conditional Random Field
        \item[ELMo] Embeddings from Language Model
        \item[GO] Golang Programming Language
        \item[GloVe] Global Vectors for Word Representation
        \item[HTTP] Hypertext Transfer Protocol
        \item[IDF] Inverse Document Frequency
        \item[JSON] JavaScript Object Notation
        \item[NLP] Natural Language Processing
        \item[NER] Reconhecimento de Entidade Nomeada
        \item[POS] Part-of-Speech
        \item[PJe] Processo Judicial Eletrônico
        \item[SVM] Support Vector Machine
        \item[RAG] Retrieval-Augmented Generation
        \item[REST] Representational State Transfer
        \item[TF-IDF] Term Frequency-Inverse Document Frequency
        \item[ORM] Object-Relational Mapping
        \item[GORM] Golang ORM
        \item[I/O] Input/Output
        \item[HTML] HyperText Markup Language
        \item[PL] Proposta de lei
        \item[PEC] Proposta de Emenda à Constituição
        \item[SSD] Solid-State Drive
        \item[HDD] Hard Disk Drive
        \item[RAM] Random Access Memory
        \item[NVMe] Non-Volatile Memory Express
        \item[SQL] Structured Query Language
    \end{siglas}
% ========================================================================
% SUMÁRIO
% ========================================================================
    \pdfbookmark[0]{\contentsname}{toc}
    \tableofcontents*
    \cleardoublepage

    \textual
% ========================================================================
% INTRODUÇÃO
% ========================================================================


    \chapter{Introdução}


    \section{Contexto}

    A era digital transformou radicalmente o acesso à informação, gerando um volume de dados sem precedentes em praticamente todos os setores da sociedade. No campo do Direito, essa realidade se manifesta de forma contundente na digitalização massiva de processos, leis, jurisprudências e pareceres. O Judiciário brasileiro, por exemplo, vivenciou uma explosão documental ao receber mais de 250 milhões de processos em formato eletrônico nos últimos 15 anos \cite{citacao59}. Se por um lado essa digitalização representa um avanço inegável em termos de transparência e preservação, por outro, ela impõe um desafio monumental: a simples capacidade de armazenamento tornou-se insuficiente. O problema agora desloca-se para a navegabilidade e a extração de valor desse oceano de textos complexos e interconectados, criando uma demanda urgente não apenas por memória digital, mas por capacidade de processamento inteligente.

    Para responder a essa necessidade de inteligência interpretativa, as técnicas de Processamento de Linguagem Natural (NLP), especialmente aquelas baseadas em Inteligência Artificial e aprendizado de máquina, emergiram como a fronteira tecnológica mais promissora. Diferentemente dos sistemas antigos de busca exata, os modelos de linguagem avançados — notadamente aqueles baseados na arquitetura \textit{Transformer}, como o BERT e o LEGAL-BERT — prometem "ler" os documentos, compreendendo nuances semânticas e contextuais que escapam à busca por palavras-chave. Ferramentas que utilizam arquiteturas de \textit{transformers} e representações vetoriais (\textit{embeddings}) representam o atual estado da arte, buscando emular uma compreensão quase humana dos textos legais para resolver tarefas complexas, como a identificação de similaridade entre decisões e divergências jurisprudenciais \cite{citacao60}.

    No entanto, o entusiasmo com a sofisticação da Inteligência Artificial muitas vezes ofusca uma barreira prática significativa: o custo computacional. A adoção dessas tecnologias de ponta não é gratuita; ela exige um esforço financeiro, técnico e energético considerável. O treinamento e, principalmente, a inferência de grandes modelos de linguagem demandam hardware especializado (GPUs) e infraestrutura robusta, o que pode tornar a solução inviável para muitas instituições. Conforme o uso de IA se expande, torna-se crítico não apenas buscar o modelo mais "inteligente", mas também aplicar métodos para otimizar e reduzir os custos de operação desses sistemas, garantindo sua sustentabilidade \cite{citacao61}.

    Essa tensão entre a performance semântica e o custo computacional nos leva diretamente ao cerne da investigação proposta neste trabalho. Diante de um cenário onde recursos são finitos, é imperativo questionar até que ponto a complexidade dos modelos de \textit{machine learning} é estritamente indispensável para a tarefa de busca em documentos jurídicos. Surge, então, uma dúvida metodológica relevante: será que métodos estatísticos e algoritmos de busca mais clássicos, se bem parametrizados e aplicados, não poderiam alcançar resultados comparáveis aos da IA moderna, mas consumindo apenas uma fração dos recursos?

    Este estudo se propõe, portanto, a explorar essa fronteira de eficiência. A literatura já aponta indícios nessa direção, como observado por \citeonline{citacao62}, que demonstra que modelos distribucionais complexos como o BERT não apresentam, necessariamente, um desempenho superior em todas as tarefas de classificação jurídica quando comparados a abordagens mais simples. O objetivo deste trabalho não é desenvolver uma nova ferramenta, mas sim realizar uma análise crítica e comparativa rigorosa, buscando compreender e quantificar o verdadeiro \textit{trade-off} entre a precisão semântica oferecida pela IA e a eficiência computacional dos métodos estatísticos no domínio específico do Direito.


    \section{Justificativa}

    A busca por maior precisão impulsionou o avanço da tecnologia jurídica e estimulou uma dependência crescente de modelos de Inteligência Artificial. Esse movimento ganhou força no Brasil, onde o uso dessas soluções no Judiciário aumentou 26\% entre 2022 e 2023 \cite{citacao63}. A ideia que sustenta esse crescimento é simples. Quanto mais sofisticado o modelo, melhores seriam os resultados. Só que essa expectativa costuma vir acompanhada de um custo elevado. O treinamento de grandes modelos depende de processamento intenso e de hardware especializado, o que envolve investimentos altos em infraestrutura e manutenção. Em muitos casos, o peso financeiro impede que instituições menores entrem nessa corrida tecnológica, mesmo quando há interesse.

    Ao mesmo tempo, a evolução do hardware abriu caminhos que nem sempre são lembrados. O avanço das arquiteturas \textit{multicore} não transformou apenas os modelos de IA. Ele também ampliou o alcance de algoritmos clássicos que antes operavam de forma limitada. Hoje esses métodos podem explorar vários núcleos de processamento e alcançar um desempenho muito superior ao que entregavam no passado \cite{citacao64}. Técnicas como TF-IDF, assim como algoritmos de busca de padrões como o Boyer-Moore, não exigem estruturas complexas e ainda assim se beneficiam de forma notável da paralelização. Mesmo com esse potencial, costumam ser vistos como alternativas inferiores simplesmente por não pertencerem ao universo dos modelos semânticos mais recentes.

    Essa percepção costuma levar a decisões pouco equilibradas. Por isso, a justificativa deste trabalho parte da necessidade de um olhar mais pragmático. Nem sempre o ganho marginal oferecido por um modelo semântico avançado compensa seu custo total de operação. Em ambientes com linguagem estável e previsível, como ocorre em grande parte dos documentos jurídicos, soluções mais simples podem alcançar resultados muito satisfatórios \cite{citacao65}. Em várias atividades do dia a dia, responder rápido e gastar menos pode ser mais importante do que captar nuances raras de ambiguidade.

    Dentro desse cenário, esta pesquisa busca oferecer uma análise quantitativa do equilíbrio entre custo e benefício. A proposta é comparar diferentes abordagens em termos de desempenho, tempo de resposta e uso de recursos. Trabalhos que analisam modelos como BERT e ChatGPT já fazem esse tipo de avaliação usando métricas como acurácia, precisão e F1-score \cite{citacao66}. Seguindo essa linha, pretendemos produzir dados concretos que ajudem a escolher a tecnologia mais adequada para cada necessidade do ecossistema jurídico. A intenção é mover a discussão para um campo mais objetivo, onde a escolha não dependa apenas do que é mais moderno, mas do que realmente funciona melhor dentro de cada contexto.


    \section{Objetivos}

    \subsection{Objetivo Geral}

    Avaliar comparativamente o desempenho computacional e a relevância semântica de métodos estatísticos, especificamente o \textbf{TF-IDF e o BM25}, frente a modelos baseados em \textit{embeddings} como o \textbf{Sentence-BERT}, visando analisar o \textit{trade-off} entre eficiência e precisão na recuperação de informação em documentos legislativos.

    \subsection{Objetivos Específicos}

    \begin{enumerate}
        \item Construir um \textit{corpus} textual robusto a partir da coleta automatizada e higienização de documentos legislativos, com foco em Propostas de Lei e Propostas de Emenda à Constituição obtidas do portal da Câmara dos Deputados.
        \item Implementar e parametrizar os algoritmos de busca estatística \textbf{TF-IDF e BM25}, bem como a abordagem semântica via \textbf{Sentence-BERT}, estruturando estratégias de indexação com diferentes configurações de N-gramas.
        \item Definir um protocolo experimental para mensurar o consumo de recursos, incluindo tempo de CPU e uso de memória RAM, bem como a latência de indexação e busca em diferentes cenários de carga.
        \item Aplicar o Coeficiente de Correlação de Spearman para quantificar o grau de concordância entre os \textit{rankings} gerados pelos métodos estatísticos e o gabarito semântico.
        \item Analisar quantitativamente os resultados para determinar em quais cenários o custo computacional dos modelos de IA se justifica frente à eficiência dos métodos clássicos.
    \end{enumerate}

    \section{Metodologia}

    Esta pesquisa segue um procedimento metodológico estruturado e replicável, detalhado nas etapas a seguir.

    \subsection{Materiais e Ferramentas}
    Para a execução dos experimentos e o processamento do volume de dados, foram utilizados os seguintes recursos:
    \begin{itemize}
        \item \textbf{Hardware:} MacBook Pro 2020, Chip M1, 16 GB de RAM (4266 MHz).
        \item \textbf{Dados:} Base de dados pública da Câmara dos Deputados.
        \item \textbf{Software:} Linguagem Go (Golang), versão 1.23.2.
    \end{itemize}

    \subsection{Configurações e Parâmetros}
    Os modelos e algoritmos foram configurados com os seguintes parâmetros:
    \begin{itemize}
        \item O modelo BERT selecionado foi o \texttt{paraphrase-multilingual-MiniLM-L12-v2}.
        \item Para o algoritmo BM25, os hiperparâmetros foram fixados em $k_1 = 1.5$ e $b = 0.75$.
        \item Todos os resultados de \textit{embeddings} e os cálculos de similaridade de cosseno foram normalizados.
        \item Na geração de n-gramas, definiu-se um limite de até 2 saltos (\textit{jumps}) para trigramas e 4 saltos para bigramas.
    \end{itemize}

    \subsection{Tratamento do Corpus}
    \begin{enumerate}
        \item \textbf{Definição:} O corpus é composto por 5 mil páginas de documentos em PDF, abrangendo Projetos de Lei (PL) e Propostas de Emenda à Constituição (PEC).
        \item \textbf{Extração:} O conteúdo dos PDFs foi extraído e convertido para texto puro.
        \item \textbf{Padronização:} Todo o texto foi convertido para letras minúsculas.
        \item \textbf{Limpeza:} O corpus passou por um processo de limpeza seguindo esta ordem:
        \subitem Remoção de URIs (conforme RFC 3986) e protocolos HTTP/TLS.
        \subitem Remoção de numerais romanos, inteiros e decimais.
        \subitem Substituição de travessões e \textit{underlines} por espaços.
        \subitem Eliminação de pontuações e caracteres especiais restantes.
        \item \textbf{Finalização:} Realizou-se a normalização de caracteres e a remoção de \textit{stop-words}.
    \end{enumerate}

    \subsection{Geração das Embeddings}
    \begin{enumerate}
        \item Foram elaborados 50 \textit{prompts} com tamanhos de 10, 20 e 40 palavras, simulando buscas reais de um operador do Direito.
        \item Cada \textit{prompt} foi vetorizado via BERT em vetores de 384 pontos flutuantes. O tempo de cada operação foi registrado para extração de médias.
        \item O mesmo processo de vetorização foi aplicado a todos os documentos do corpus.
        \item A comparação entre as \textit{embeddings} dos documentos e dos \textit{prompts} foi realizada por meio da similaridade de cosseno.
        \item Os documentos foram ordenados por relevância para cada \textit{prompt}, gerando um ranking final.
    \end{enumerate}

    \subsection{Geração de Vetores TF-IDF e BM25}
    \begin{enumerate}
        \item Os ambientes de teste foram isolados por tamanho de n-grama (uni, bi e trigramas), com o banco de dados sendo zerado a cada rodada para evitar interferências.
        \item Foram gerados identificadores únicos para documentos, palavras e n-gramas.
        \item Os n-gramas foram formados pela combinação dos IDs das palavras, saltos realizados, documento de origem e frequência.
        \item Implementou-se um cache em duas camadas (\textit{Map[Chave do n-grama] $\rightarrow$ Map[ID do documento] $\rightarrow$ N-grama}) para otimização de performance.
        \item Após o processamento do corpus e dos \textit{prompts}, os vetores TF-IDF e BM25 foram gerados, registrando-se os tempos de execução.
        \item A similaridade de cosseno foi aplicada para ordenar os documentos conforme sua relevância em relação aos \textit{prompts}.
    \end{enumerate}

    \subsection{Análise dos Dados}

    A análise dos resultados será realizada através de duas abordagens complementares, permitindo uma visão detalhada sobre a eficácia de cada método.

    \subsubsection{Análise Qualitativa e Comparação de Rankings}
    Para avaliar a proximidade entre os métodos (BERT, TF-IDF e BM25), utilizaremos o \textbf{Coeficiente de Correlação de Spearman}. Esta métrica é ideal para este estudo pois ela não olha apenas para os valores brutos de similaridade, mas sim para a ordenação (\textit{ranking}) dos documentos. Ao mensurar a diferença de posição dos documentos entre as listas geradas, o coeficiente de Spearman permite identificar o grau de convergência entre os algoritmos. Dessa forma, conseguimos medir qualitativamente se um método baseado em semântica (BERT) entrega resultados hierarquicamente próximos aos métodos baseados em frequência de termos (BM25/TF-IDF).

    \subsubsection{Análise Quantitativa de Desempenho}
    A vertente quantitativa focará na eficiência computacional. Utilizaremos os registros de tempo coletados durante as etapas de vetorização e comparação para analisar:
    \begin{itemize}
        \item O tempo médio de processamento por documento e por \textit{prompt}.
        \item A soma total de tempo necessária para processar o corpus completo em cada abordagem.
        \item A escalabilidade dos métodos em relação ao tamanho dos textos (10, 20 e 40 palavras).
    \end{itemize}

    O cruzamento desses dados permitirá identificar o custo-benefício de cada modelo, determinando qual técnica oferece a melhor relação entre precisão na busca e latência de resposta.

    \chapter{Revisão bibliográfica}


    \section{Processamento de Linguagem Natural no Domínio Jurídico}

    A crescente produção textual na sociedade contemporânea transformou a informação em um recurso abundante, porém de difícil processamento. No universo jurídico, esse fenômeno é ainda mais crítico, visto que contratos, decisões e legislações são produtos essencialmente linguísticos. O Direito, enquanto sistema normativo, é construído e operado exclusivamente por meio da linguagem, o que torna a precisão interpretativa um requisito fundamental \cite{citacao42, citacao43}.

    Para lidar com esse volume exponencial de documentos, que excede as limitações cognitivas humanas, o uso de ferramentas computacionais deixou de ser opcional para se tornar uma necessidade. Nesse cenário, as técnicas de Processamento de Linguagem Natural (NLP) surgem não apenas como uma ferramenta de automação, mas como a abordagem consolidada para enfrentar o desafio de interpretar grandes massas de dados textuais \cite{citacao17, citacao44}.

    Diferentemente de sistemas rígidos baseados em regras, o NLP permite que as máquinas compreendam e operem diretamente sobre a base textual do Direito. Essas técnicas viabilizam desde a extração de informações relevantes e identificação de padrões até a sumarização de documentos e suporte à tomada de decisão, incorporando uma camada de "inteligência" ao processamento bruto de dados \cite{citacao45, citacao43}.

    Essa evolução tecnológica já se traduz em aplicações práticas robustas no ambiente jurídico, como a análise preditiva de jurisprudência e a busca semântica em bases normativas. No contexto brasileiro, estudos como os de \citeonline{citacao17} evidenciam que essas tecnologias já estão consolidadas, estabelecendo um novo padrão de referência para a recuperação e análise de informação no setor.

    \subsection{Características dos textos jurídicos}

    A aplicação de técnicas de recuperação de informação no domínio jurídico envolve desafios particulares, uma vez que os textos legais apresentam propriedades distintas de outros gêneros textuais. Em primeiro lugar, são documentos altamente especializados, redigidos com vocabulário técnico e linguagem formal. Termos como \textit{vossa excelência}, \textit{ex positis} ou \textit{ad nutum} desempenham papel semântico central \cite{citacao42,citacao44}. Para sistemas de busca, essa especificidade lexical gera efeitos distintos dependendo da abordagem: favorece métodos baseados em palavras-chave (devido à raridade dos termos), mas pode confundir modelos semânticos genéricos que não foram treinados nesse vocabulário específico.

    Além disso, esses textos apresentam um alto grau de ambiguidade controlada. Palavras como \textit{poderá}, \textit{deverá} ou \textit{caberá} têm significados normativos específicos que impactam diretamente a interpretação das normas. Essa polissemia representa um desafio de desambiguação: métodos estatísticos tendem a tratar essas palavras apenas como \textit{tokens} distintos, ignorando sua carga normativa, enquanto modelos semânticos precisam de um ajuste fino robusto para diferenciar o "sentido jurídico" do uso coloquial \cite{citacao17,citacao45}.

    Outro aspecto crítico, e talvez o mais desafiador para a recuperação de informação, é a intertextualidade jurídica. Leis, contratos e sentenças não existem no vácuo; eles referenciam constantemente outros documentos. Isso cria um cenário onde a relevância de um documento muitas vezes depende de uma referência externa exata, e não apenas do seu conteúdo semântico interno. Esse fenômeno desafia a lógica dos \textit{embeddings}, que buscam similaridade de significado e podem perder a precisão da referência exata, e também impõe dificuldades aos métodos estatísticos, que tratam citações complexas apenas como sequências de caracteres \cite{citacao17}.

    Também é importante considerar a estrutura argumentativa dos textos. Documentos como sentenças possuem uma lógica discursiva específica. A informação mais relevante para uma busca nem sempre está distribuída uniformemente pelo texto. Isso afeta diretamente a performance dos algoritmos: o TF-IDF pode supervalorizar a repetição de termos na fundamentação, enquanto um modelo de \textit{embedding} que faz a média vetorial do documento inteiro pode diluir o "sinal" da decisão final contida na conclusão \cite{citacao43}.

    \subsection{Desafios de Processamento e Representação em Português}

    Ao aplicar técnicas de NLP à língua portuguesa, especialmente no domínio jurídico, surgem obstáculos que permeiam desde a análise morfossintática até a representação semântica vetorial. Diferentemente do inglês, o português brasileiro apresenta uma morfologia altamente flexiva, com ampla variação de formas verbais, nominais e pronominais, o que eleva a complexidade de tarefas fundamentais como a tokenização e a lematização \cite{citacao17}.

    No contexto jurídico, essa complexidade é intensificada pelo uso recorrente de locuções e expressões idiomáticas. Construções como \textit{sem prejuízo do disposto}, \textit{salvo disposição em contrário} ou \textit{nos termos da legislação aplicável} funcionam como unidades semânticas coesas. Analisá-las isoladamente, palavra por palavra, pode fragmentar o sentido original, exigindo abordagens de pré-processamento capazes de identificar e preservar o significado completo dessas expressões \cite{citacao44}.

    Essa especificidade linguística impõe desafios práticos imediatos na etapa de tokenização. O tratamento da pontuação, por exemplo, exige regras de exceção: elementos como números de processo (ex: 1234567-89.2023.8.26.0100) devem ser preservados como um único \textit{token}, dada sua importância identificadora, enquanto a pontuação gramatical comum deve ser removida \cite{citacao17}. Da mesma forma, o uso de hífens em palavras compostas (ex: \textit{auto-aplicável}) e a presença de entidades nomeadas complexas (ex: \textit{Ministério Público}, \textit{Poder Judiciário}) demandam estratégias de segmentação que evitem a perda de informação semântica, um problema bem documentado na literatura de NLP aplicada ao direito brasileiro \cite{citacao17}.

    Além da segmentação e limpeza, há o desafio da representação vetorial (\textit{embeddings}). Para que algoritmos possam realizar tarefas como busca jurisprudencial ou classificação, os textos precisam ser convertidos em vetores numéricos. No entanto, modelos de \textit{embeddings} genéricos, treinados com textos da web, frequentemente falham em capturar as nuances da linguagem técnica e formal do Direito \cite{citacao39}.

    Modelos especializados, como a família LEGAL-BERT \cite{citacao39}, que são pré-treinados ou ajustados (\textit{fine-tuned}) especificamente em corpora jurídicos, mostram-se mais eficazes na captação desse vocabulário especializado e das estruturas discursivas recorrentes. Contudo, o uso desses modelos introduz um novo obstáculo: o alto custo computacional de treinamento e inferência. Isso levanta a questão central do \textit{trade-off} entre a precisão semântica oferecida por modelos de linguagem pesados e a eficiência de métodos mais simples, cerne da investigação deste trabalho \cite{citacao35, citacao40}.

    \subsection{Modelos de Linguagem para o Domínio Jurídico}

    Embora bibliotecas de NLP de propósito geral, como \textit{spaCy} ou \textit{NLTK}, ofereçam suporte para a língua portuguesa, seus modelos genéricos, treinados em textos da web ou literatura, falham em capturar as nuances terminológicas e o contexto semântico específico do universo jurídico. A palavra "competência", por exemplo, tem um significado técnico e distinto no Direito (atribuição de um órgão para julgar) que um modelo genérico não consegue priorizar.

    Para suprir essa lacuna, a comunidade acadêmica tem se concentrado no desenvolvimento de modelos de linguagem especializados. Um trabalho central nesse contexto para o português brasileiro é o \citeonline{citacao17} (LegalNLP). Os autores demonstram que modelos como Word2Vec e BERT, quando pré-treinados ou ajustados (\textit{fine-tuning}) em um vasto e específico corpus de documentos jurídicos nacionais, superam significativamente o desempenho de modelos genéricos em tarefas-chave de NLP.

    Essa abordagem de especialização é uma tendência global. O trabalho de \citeonline{citacao39}, por exemplo, introduziu o LEGAL-BERT, um modelo da família BERT treinado inteiramente em textos legais. A pesquisa comprovou que essa especialização de domínio permite uma captação muito mais precisa do vocabulário técnico e das estruturas discursivas do Direito do que o modelo BERT original.

    Portanto, a literatura estabelece que o estado da arte para a busca semântica de alta precisão no domínio jurídico envolve o uso de modelos de linguagem contextuais, pesados e de domínio específico. É justamente essa abordagem, que representa o polo de maior complexidade e custo computacional, que este trabalho utilizará como referência para a comparação com os métodos estatísticos tradicionais.


    \section{Técnicas de Pré-processamento Textual}

    Em um cenário de grande volume textual, como o jurídico, a extração de informação relevante demanda um tratamento prévio sobre os documentos. Essa etapa é conhecida como pré-processamento, e tem como objetivo transformar textos brutos em representações mais estruturadas e significativas, facilitando tanto a análise semântica quanto a indexação e recuperação posterior. Trata-se, portanto, de uma fase indispensável para qualquer sistema de recuperação de informação textual, sobretudo em domínios especializados, já que a escolha adequada das técnicas de pré-processamento pode influenciar significativamente a relevância das informações extraídas \cite{citacao16}.

    Além de normalizar o conteúdo textual, o pré-processamento também é responsável pela definição do vocabulário de termos que será utilizado na indexação e na construção de representações computacionais. Um bom pré-processamento reduz o tamanho do vocabulário, melhora o desempenho e aumenta a precisão das buscas \cite{citacao67}. Essa etapa funciona, portanto, como uma ponte entre a linguagem natural e os algoritmos, permitindo que informações relevantes sejam extraídas de forma estruturada, eficiente e precisa. No contexto deste trabalho, ela se mostra essencial para a identificação de elementos fundamentais nos documentos jurídicos, como nomes das partes, dispositivos legais e decisões.

    \subsection{Tokenização}

    No contexto do processamento de linguagem natural, um \textit{token} é definido como uma unidade básica de texto, normalmente correspondente a uma palavra, número, pontuação ou símbolo significativo. A forma como os \textit{tokens} são extraídos influencia diretamente a eficácia dos modelos computacionais, especialmente em contextos especializados como o jurídico, onde há presença de construções formais e terminologias específicas.

    O processo de tokenização refere-se, portanto, à separação do texto contínuo em unidades menores, os \textit{tokens}, que geralmente representam palavras. Essa tarefa vai além de simplesmente dividir o texto por espaços; por exemplo, expressões como “São Paulo” ou “Art. 5º” exigem regras específicas para não serem segmentadas incorretamente, especialmente em textos técnicos como os jurídicos \cite{citacao67}.

    \begin{figure}[H]
        \centering
        \caption{\label{fig:tokenizacao}Exemplo de \textit{tokenização}.}
        \includegraphics[width=\textwidth]{assets/tokenizacao.png}
        \legend {Fonte: Elaboração própria.}
    \end{figure}

    \subsection{Remoção de Stopwords}

    A remoção de \textit{stopwords} é uma das etapas principais do pré-processamento textual e consiste na exclusão de palavras que ocorrem com alta frequência, mas que carregam pouco ou nenhum valor semântico individual — como \textit{de}, \textit{que}, \textit{em}, \textit{o} e \textit{a}. Esse tipo de filtragem é comumente utilizado para reduzir o vocabulário e acelerar os algoritmos, sem perda significativa de informação relevante para a maioria das tarefas de recuperação e classificação textual. \cite{citacao67}

    Apesar de a remoção de \textit{stopwords} ser uma prática comum em tarefas de processamento de linguagem natural, no contexto jurídico é necessário cautela. Isso porque palavras funcionais (geralmente ignoradas em outros domínios), podem conter informações fundamentais para a interpretação de dispositivos legais, como apontado por \citeauthor{citacao17} (\citeyear{citacao17}), que optaram por não removê-las durante o treinamento de seus modelos linguísticos, justamente para preservar a integridade semântica dos textos judiciais.

    \begin{figure}[H]
        \centering
        \caption{Exemplo de remoção de \textit{stopwords}}
        \includegraphics[width=\textwidth]{assets/Stopwords.png}
        \legend{Fonte: Elaboração própria.}
    \end{figure}

    \subsection{Lematização e Stemização}

    A lematização reduz palavras à sua forma canônica, conhecida como \textit{lema}. Por exemplo, termos como \textit{decidir}, \textit{decisão} e \textit{decidido} são todos associados ao verbo \textit{decidir}. Esse processo considera a análise morfológica e o contexto gramatical, sendo essencial para manter o sentido das palavras nos textos jurídicos, onde pequenas variações morfológicas podem alterar significativamente o significado \cite{citacao10}.

    Por outro lado, a stemização é uma técnica mais simples que remove afixos (prefixos e sufixos) para obter uma raiz. No entanto, essa raiz nem sempre corresponde a uma palavra válida no idioma. Por exemplo, as palavras \textit{decidir}, \textit{decidindo} e \textit{decidido} podem ser reduzidas a \textit{decid}.

    \begin{figure}[H]
        \centering
        \caption{Exemplo de diferenças entre Stemização e Lematização.}
        \includegraphics[width=\textwidth]{assets/lem_stem.png}
        \legend{Fonte: Elaboração própria, inspirada em \cite{citacao18}.}
    \end{figure}

    A lematização proporciona maior fidelidade semântica, sendo mais indicada para tarefas que exigem compreensão precisa dos textos, como análise e busca em documentos jurídicos. Essa abordagem, por depender de análise linguística completa e regras morfológicas, costuma ter maior custo computacional e necessidade de recursos linguísticos bem estruturados, especialmente em idiomas como o português, que possui alta complexidade morfológica \cite{citacao17}.

    Por outro lado, a stemização é uma solução mais simples e eficiente em termos computacionais, baseada em regras heurísticas para remoção de afixos. Embora menos precisa, pode ser útil em tarefas onde a sensibilidade semântica não é tão crítica \cite{citacao18}.

    No contexto jurídico brasileiro, a adoção de lematização se mostra mais vantajosa, uma vez que ferramentas linguísticas genéricas não são suficientes para capturar as nuances da linguagem jurídica em português \cite{citacao17}.

    \subsection{Segmentação de Sentenças}

    A segmentação de sentenças consiste em dividir um texto contínuo em unidades menores, geralmente delimitadas por frases. Esse processo é essencial para permitir que modelos de processamento de linguagem operem com unidades textuais semanticamente completas, sobretudo em tarefas que requerem maior nível de precisão, como extração de fundamentos jurídicos, sumarização automática, análise de jurisprudências ou geração de relatórios.

    Embora seja considerado um processo relativamente simples no processamento de linguagem natural, a segmentação de sentenças apresenta desafios em textos jurídicos. A alta frequência de abreviações, siglas e expressões formais, como “Art.”, “Inc.”, “V. Exa.” ou “D.J.E.”, gera ambiguidades na definição dos pontos que efetivamente representam o final de uma sentença. Pontuações como ponto final, dois-pontos e até ponto e vírgula podem assumir funções distintas dependendo do contexto \cite{citacao17}.


    \section{Representação de Documentos}

    \subsection{TF-IDF}

    Após o pré-processamento, uma das técnicas mais consolidadas para representar documentos como vetores numéricos é o TF-IDF (\textit{Term Frequency--Inverse Document Frequency}). Esse método associa a cada termo um peso que reflete tanto sua importância local(baseada na frequência com que aparece no documento), quanto sua importância global, que é determinada pela raridade do termo na coleção como um todo \cite{citacao10, citacao67}.

    O cálculo do TF--IDF é composto por duas etapas fundamentais. A primeira consiste na determinação da frequência de termo \textit{Term Frequency} (TF), que quantifica o número de vezes que um termo \(t\) aparece em um documento \(d\), representado por \(\mathit{tf}_{t,d}\). Para evitar que termos muito frequentes dominem a representação, especialmente em documentos longos, é comum aplicar uma normalização logarítmica, definida como:
    \[
        \mathit{tf}_{t,d} =
        \begin{cases}
            1 + \log(\mathit{tf}_{t,d}), & \text{se } \mathit{tf}_{t,d} > 0, \\
            0, & \text{caso contrário}.
        \end{cases}
    \]
    Essa transformação suaviza o impacto de termos com alta contagem, tornando a representação mais robusta e comparável entre documentos de diferentes tamanhos \cite{citacao67}.

    A segunda etapa envolve o cálculo da \textit{Inverse Document Frequency} (IDF), que mede a capacidade de um termo discriminar documentos. Dado \(N\) como o número total de documentos na coleção e \(df_t\) como o número de documentos em que o termo \(t\) aparece, a IDF é calculada como:
    \[
        \mathit{idf}_t = \log \left( \frac{N}{df_t} \right)
    \]
    Esse fator tem como objetivo reduzir o peso de termos comuns na coleção, como artigos, pronomes ou palavras genéricas, e aumentar o peso de termos mais raros, que são mais úteis para diferenciar documentos \cite{citacao67}.

    Assim, o peso final de um termo em um documento, chamado de TF--IDF, é dado pela multiplicação dos dois componentes:
    \[
        \mathrm{tf\text{-}idf}(t,d) = \mathit{tf}_{t,d} \times \mathit{idf}_t
    \]
    Esse valor será mais alto para termos que ocorrem frequentemente em um documento específico, mas que são raros na coleção como um todo, refletindo sua importância tanto local quanto global \cite{citacao67}.

    A representação de documentos no modelo vetorial é então construída a partir dos pesos TF--IDF de cada termo. Formalmente, um documento é representado como um vetor \(\mathbf{v}(d) = [\mathrm{tf\text{-}idf}(t_1,d), \dots, \mathrm{tf\text{-}idf}(t_M,d)]\), onde \(M\) é o número total de termos únicos na coleção. Consultas podem ser representadas de maneira análoga, permitindo o uso de métricas como a similaridade do cosseno para calcular a proximidade entre consultas e documentos no espaço vetorial \cite{citacao67}.

    \begin{figure}[H]
        \centering
        \caption{Exemplo da aplicação do TF-IDF em frases.}
        \includegraphics[width=\textwidth]{assets/TF_IDF.png}
        \legend{Fonte: Elaboração própria, inspirada em \cite{citacao41}}
    \end{figure}

    O modelo TF--IDF é amplamente utilizado em tarefas de recuperação de informação, classificação e agrupamento de textos, justamente por sua simplicidade e eficácia. Seu principal mérito reside na combinação da frequência local com a raridade global dos termos, permitindo destacar palavras que são relevantes para o conteúdo específico de um documento, enquanto penaliza termos genéricos e pouco informativos \cite{citacao10}.

    \subsection{BM25}

    Enquanto o TF-IDF estabeleceu um modelo fundamental para a recuperação de informação, ele possui limitações intrínsecas, como a tendência de favorecer desproporcionalmente termos de alta frequência e a falta de uma normalização robusta para o tamanho dos documentos. Para superar essas questões, foi desenvolvido o algoritmo Okapi BM25 (doravante chamado apenas de BM25), um modelo de ranqueamento probabilístico que se tornou um padrão de fato em sistemas de busca modernos devido à sua eficácia e robustez \cite{citacao67}.

    O BM25 não trata a frequência de um termo (\textit{term frequency}) de forma linear. Em vez disso, ele introduz um componente de saturação, partindo da premissa de que a relevância de um termo para um documento não cresce infinitamente com sua frequência. A primeira vez que uma palavra aparece é muito significativa; a décima, um pouco menos; e a centésima vez adiciona uma relevância marginal muito pequena. O algoritmo modela esse comportamento para evitar que documentos longos e repetitivos dominem os resultados da busca.

    A pontuação de relevância de um documento \textit{D} para uma consulta \textit{Q}, que contém os termos \(q_1, q_2, \dots, q_n\), é calculada pela seguinte fórmula:

    \[
        \text{score}(D, Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
    \]

    Para entender seu funcionamento, podemos decompor a fórmula em suas três partes principais. O primeiro componente é o \textit{Inverse Document Frequency} (IDF), que, assim como no TF-IDF, mede a raridade de um termo na coleção de documentos. O BM25 utiliza uma variante da fórmula padrão de IDF, geralmente com um ajuste para evitar valores nulos ou negativos para termos que aparecem em mais da metade dos documentos \cite{citacao67}. A lógica central, no entanto, permanece a mesma: termos que são raros em toda a coleção recebem um peso maior, pois são considerados mais discriminativos.

    O segundo e mais inovador componente da fórmula lida com a \textit{saturação da frequência do termo}. Diferente do TF-IDF, o BM25 parte do princípio de que a relevância de um termo não cresce infinitamente com sua frequência. A primeira ocorrência é muito significativa, mas cada aparição subsequente contribui progressivamente menos para a pontuação final. Esse efeito de saturação é controlado pelo parâmetro de calibração \textit{k1} (geralmente entre 1.2 e 2.0), que define quão rapidamente a pontuação de um termo se estabiliza. Um valor de \textit{k1} baixo faz com que a relevância sature rapidamente, enquanto um valor mais alto permite que a frequência do termo continue a ter um impacto maior na pontuação \cite{citacao67}.

    Por fim, o BM25 introduz uma \textit{normalização pelo comprimento do documento} muito mais sofisticada. O algoritmo compara o tamanho do documento atual, \(|D|\), com o tamanho médio de todos os documentos na coleção, \textit{avgdl}. O parâmetro \textit{b} (geralmente em torno de 0.75) controla o grau de influência que o tamanho do documento tem na pontuação final. Quando \textit{b} é 1, o efeito da normalização é máximo; quando é 0, o tamanho do documento é completamente ignorado. Essa abordagem permite penalizar documentos que são muito mais longos que a média, pois eles têm uma probabilidade estatisticamente maior de conter os termos da busca por acaso, sem serem necessariamente mais relevantes \cite{citacao67}.

    Em síntese, o BM25 aprimora os conceitos do modelo vetorial ao incorporar uma visão probabilística e heurísticas mais refinadas sobre como a frequência de termos e o tamanho dos documentos influenciam a relevância. Sua capacidade de ser calibrado pelos parâmetros \textit{k1} e \textit{b} o torna altamente adaptável a diferentes tipos de coleções de texto. Essa relevância é comprovada por sua aplicação prática em diversos domínios, incluindo estudos recentes no contexto jurídico brasileiro para a identificação de similaridade em documentos oficiais \cite{citacao68}.

    \subsection{Índice inverso}

    O cálculo eficiente de medidas como o TF-IDF e o BM25, bem como a execução de buscas rápidas em grandes volumes de texto, dependem de uma estrutura de dados mais sofisticada do que a simples lista de documentos. A cada nova consulta ou para a construção dos próprios modelos estatísticos, seria computacionalmente inviável reanalisar todo o corpus para encontrar termos, suas frequências (\textit{term frequency}) e em quantos documentos eles aparecem (\textit{document frequency}). Para resolver esse problema, utiliza-se o \textbf{índice inverso} (ou \textit{inverted index}), que funciona como a principal estrutura de indexação para sistemas de recuperação de informação \cite{citacao67}.

    O índice inverso é uma estrutura de dados amplamente utilizada em sistemas de recuperação de informação, projetada para otimizar a busca de termos em grandes coleções de documentos \cite{citacao67, citacao69}. Diferentemente de uma abordagem direta, que examinaria cada documento sequencialmente (uma operação de complexidade \(O(N)\), onde N é o tamanho do corpus), o índice invertido organiza os termos do corpus de forma a mapear cada palavra ou unidade lexical às suas respectivas ocorrências. Esse mapeamento inclui informações como a identificação dos documentos e, comumente, a posição ou frequência de cada termo. Essa organização permite consultas eficientes e rápidas, fundamentais para o funcionamento de sistemas como motores de busca \cite{citacao67}. Segue uma imagem que exemplifica o índice inverso.

    \begin{figure}[H]
        \centering
        \caption{\label{fig:Indice} No índice inverso quem mapeia são os termos e n.}
        \includegraphics[width=\textwidth]{assets/Indice.png}
        \legend {Fonte: Elaboração própria.}
    \end{figure}

    Uma forma de otimizar o uso de memória do índice inverso é por meio da compressão das listas de ocorrências (\textit{postings lists}). Técnicas comuns incluem a codificação de diferenças (\textit{gap encoding} ou \textit{delta encoding}), onde apenas a diferença entre IDs de documentos consecutivos é armazenada, e codificações de inteiros como \textit{varint} (\textit{Variable Byte coding}) ou \textit{Golomb coding}, que reduzem o espaço ocupado por números pequenos. Além de economizar memória, a compressão pode acelerar consultas, já que menos dados precisam ser carregados da memória ou disco, sendo uma prática padrão em motores de busca de larga escala \cite{citacao67}.

    O índice inverso oferece consultas com complexidade \(O(\log n)\) para a busca de termos no dicionário da estrutura, onde \(n\) é o número de termos únicos, caso o dicionário seja implementado com árvores balanceadas; ou \(O(1)\) em média, se forem usadas tabelas hash \cite{citacao67}. Isso se difere drasticamente da complexidade \(O(N)\) de uma varredura sequencial. Atualizações, como a inserção de novos documentos, têm um custo associado ao processamento de seus termos. No projeto, usamos uma tabelas hash para mapear termos a listas de ocorrências, armazenando para cada termo: ID do documento, frequência e posições. Isso reduz o tempo de vetorização, já que os dados são pré-computados, garantindo eficiência em consultas e escalabilidade para corpora grandes.

    O índice inverso, contudo, apresenta \textit{trade-offs}. Ele consome mais memória do que uma varredura direta, devido ao armazenamento do dicionário de termos e das listas de ocorrências. A construção inicial do índice também é um processo custoso, com complexidade tipicamente linear em relação ao tamanho total do corpus \cite{citacao67}. Em corpora dinâmicos, onde documentos são adicionados ou alterados frequentemente, as atualizações podem exigir estratégias complexas de gerenciamento, como o uso de índices auxiliares ou a fusão periódica de índices incrementais, para manter o desempenho sem reconstruir toda a estrutura \cite{citacao67}.

    \subsection{\textit{Embeddings} baseados em Inteligência Artificial}

    Modelos estatísticos clássicos de recuperação de informação, como o TF-IDF e o BM25, operam em um nível lexical. Eles são extremamente eficientes para determinar a relevância de um documento com base na frequência e raridade de \textit{palavras-chave} exatas, mas falham em capturar o \textit{significado} ou a \textit{intenção} por trás da busca. Por exemplo, uma consulta por "responsabilidade civil em acidentes de trânsito" pode não retornar um documento altamente relevante que use a frase "indenização por colisão de veículos", pois os termos não coincidem. Essa limitação, conhecida como \textit{impedância semântica} (\textit{semantic mismatch}), é o principal desafio que as representações de texto baseadas em Inteligência Artificial buscam resolver.

    Para superar essa barreira, o Processamento de Linguagem Natural (NLP) evoluiu de modelos baseados em contagem para modelos baseados em \textit{predição}, que aprendem a representar palavras em um espaço vetorial contínuo. Essas representações, conhecidas como \textit{word embeddings} (ou apenas \textit{embeddings}), são vetores densos de números que capturam relações semânticas e sintáticas complexas a partir do contexto em que as palavras aparecem em grandes volumes de texto \cite{citacao53}. Em vez de apenas contar palavras, esses modelos aprendem seu significado, impulsionando uma nova geração de sistemas de busca semântica.

    \subsubsection{Processamento de Linguagem Natural (NLP)}

    O Processamento de Linguagem Natural (NLP) é uma área da inteligência artificial que tem como objetivo permitir que computadores compreendam, interpretem e gerem a linguagem humana de forma automática e eficiente. A linguagem natural, usada por pessoas no dia a dia, é cheia de ambiguidades, variações e contextos, o que torna o trabalho do NLP bastante desafiador. Para superar essas dificuldades, o NLP aplica técnicas que vão desde regras gramaticais, análise sintática, tokenização e stemming até modelos probabilísticos, aprendizado supervisionado, redes neurais e arquiteturas avançadas como \textit{transformers} \cite{citacao52}.

    Historicamente, os sistemas de NLP eram baseados em regras linguísticas definidas manualmente, o que exigia muito esforço e apresentava limitações para lidar com a complexidade da linguagem. Com o avanço do aprendizado de máquina, especialmente dos modelos de aprendizado profundo, o campo evoluiu significativamente. Modelos modernos, como os baseados em arquiteturas \textit{transformers}, têm a capacidade de capturar relações contextuais complexas em textos, o que melhorou muito o desempenho em tarefas essenciais do NLP, como tradução automática, análise de sentimentos, reconhecimento de fala, extração de informações, sumarização automática e respostas a perguntas \cite{citacao35}. Essas técnicas permitem que sistemas interpretem o significado de frases, reconheçam intenções e até gerem textos coerentes, facilitando a interação entre humanos e máquinas.

    O NLP precisa lidar com desafios como a ambiguidade das palavras(onde uma mesma palavra pode ter vários significados dependendo do contexto), a variação linguística entre diferentes regiões e falantes, e a necessidade de entender o contexto mais amplo para interpretar corretamente expressões e intenções. Além disso, o desenvolvimento de sistemas eficientes depende da disponibilidade de grandes volumes de dados anotados para treinar esses modelos, o que nem sempre é fácil para línguas menos representadas \cite{citacao52}.

    Na prática, o NLP está presente em muitas aplicações que usamos diariamente, desde assistentes virtuais como Siri e Alexa, passando por sistemas de tradução automática, \textit{chatbots} para atendimento ao cliente, até ferramentas que ajudam a analisar opiniões em redes sociais. Essa popularização faz do NLP uma área essencial para a interação entre humanos e máquinas, impulsionando a transformação digital em diversos setores \cite{citacao35}.

    Nesse contexto de evolução das técnicas de NLP, um dos avanços mais importantes para a representação da linguagem foi o desenvolvimento dos \textit{embeddings}. Eles surgiram como uma solução eficaz para transformar palavras, frases ou documentos em vetores numéricos que preservam relações semânticas e contextuais \cite{citacao52,citacao53}. Essa representação vetorial tornou-se fundamental para alimentar modelos de aprendizado de máquina, especialmente os modelos mais modernos. A seguir, serão explorados o conceito de \textit{embeddings} e os principais modelos utilizados para gerá-los.

    \subsubsection{A Evolução dos Modelos de \textit{Embeddings}}

    Os modelos para geração de \textit{embeddings} evoluíram significativamente, passando de representações estáticas para representações dinâmicas e contextuais.

    \subsubsubsection{Embeddings Estáticos: Word2Vec e GloVe}

    Os primeiros modelos que popularizaram os \textit{embeddings} foram o \textit{Word2Vec} \cite{citacao53} e o \textit{GloVe} \cite{citacao54}. O \textit{Word2Vec} é baseado em aprendizado preditivo local, utilizando duas arquiteturas principais: \textit{Skip-Gram} e \textit{Continuous Bag-of-Words} (CBOW). O modelo \textit{Skip-Gram} tenta prever as palavras de contexto (vizinhas) a partir de uma palavra central. Inversamente, o CBOW tenta prever a palavra central com base em seu contexto. Por outro lado, o \textit{GloVe} adota uma abordagem baseada em contagem, construindo vetores a partir de estatísticas globais de coocorrência de palavras em todo o corpus.

    A principal característica e limitação desses modelos é que eles geram \textit{embeddings estáticos}. Cada palavra no vocabulário é mapeada para um único vetor, independentemente do contexto em que ela é usada. Isso significa que a palavra "banco" teria a mesma representação vetorial em "sentei no banco da praça" e em "fui ao banco depositar dinheiro". Essa incapacidade de lidar com a polissemia (múltiplos significados) foi a principal motivação para o desenvolvimento da próxima geração de modelos.

    \subsubsubsection{Embeddings Contextuais: BERT e a Revolução \textit{Transformer}}

    A grande revolução nos \textit{embeddings} veio com os modelos de linguagem contextualizados, como o ELMo \cite{citacao55} e, principalmente, o BERT \cite{citacao56}. Esses modelos não atribuem um vetor fixo a uma palavra, mas geram um \textit{embedding} dinâmico para cada palavra com base na sentença completa em que ela aparece. A palavra "banco" passa a ter vetores diferentes e apropriados para cada um dos seus significados.

    Essa capacidade foi possibilitada pela arquitetura \textit{Transformer}, que introduziu um mecanismo chamado \textbf{autoatenção} (\textit{self-attention}). Diferente de modelos anteriores que liam o texto sequencialmente (da esquerda para a direita), o mecanismo de autoatenção permite que o modelo "olhe" para todas as outras palavras da sentença simultaneamente, ponderando a importância de cada uma para definir o significado da palavra atual.

    O BERT (\textit{Bidirectional Encoder Representations from Transformers}) utiliza essa arquitetura de forma "bidirecional", lendo todo o contexto de uma vez. Para alcançar essa compreensão profunda, o BERT é pré-treinado em duas tarefas principais:
    \begin{enumerate}
        \item \textbf{Masked Language Model (MLM):} O modelo recebe uma sentença onde algumas palavras foram substituídas por um \textit{token} especial `[MASK]`. A tarefa do modelo é adivinhar qual era a palavra original, forçando-o a entender o contexto gramatical e semântico de ambos os lados (esquerdo e direito) da palavra mascarada.
        \item \textbf{Next Sentence Prediction (NSP):} O modelo recebe dois segmentos de texto, A e B, e deve prever se B é a sentença que realmente se segue a A no texto original ou se é uma sentença aleatória do corpus. Isso ensina o modelo a entender a relação lógica e coesiva entre sentenças.
    \end{enumerate}

    O resultado desse pré-treinamento intensivo é um modelo capaz de gerar \textit{embeddings} contextuais de alta fidelidade, que capturam nuances da linguagem. Modelos subsequentes, como o Sentence-BERT \cite{citacao35}, foram desenvolvidos especificamente para otimizar o BERT para tarefas de comparação de sentenças, gerando representações vetoriais de frases inteiras, ideais para busca semântica.

    \subsubsection{Recuperação e Medidas de Similaridade}
% (Esta seção foi mantida como estava, pois já era muito boa)

    A recuperação de informações em contextos de linguagem natural depende da capacidade que temos de medir similaridade entre todas estas representações textuais. Com o uso de \textit{embeddings}, textos são convertidos em vetores numéricos em espaços de alta dimensão, nos permitindo aplicar técnicas matemáticas e equações para comparar conteúdos. Entre as diversas medidas de similaridade existentes, como a distância euclidiana, de \textit{Jaccard} ou de \textit{Manhattan}, a similaridade de cosseno destaca-se por sua robustez e simplicidade, sendo amplamente adotada em tarefas de busca semântica e ranqueamento de documentos. Na subseção a seguir, detalha-se seu funcionamento e aplicação no contexto de NLP e jurídico.

    \subsubsubsection{Similaridade de cosseno}

    A similaridade de cosseno é uma métrica amplamente utilizada em Processamento de Linguagem Natural (NLP) para medir o grau de similaridade entre dois vetores em um espaço vetorial multidimensional. Quando aplicada a \textit{embeddings} de palavras, frases ou documentos, essa métrica permite quantificar o quão semanticamente próximos dois textos são entre si. A ideia central é comparar a orientação dos vetores e não sua magnitude, o que torna essa medida especialmente útil para dados textuais \cite{citacao67}.

    A fórmula da similaridade de cosseno entre dois vetores ${\vec{A}}$ e ${\vec{B}}$ é dada por:

    \[
        \text{similaridade}_{\cos}(\vec{A}, \vec{B}) = \frac{\vec{A} \cdot \vec{B}}{||\vec{A}|| \cdot ||\vec{B}||}
    \]

    onde ${\vec{A} \cdot \vec{B}}$ é o produto escalar entre os vetores, e ${||\vec{A}||}$ e ${||\vec{B}|| }$ são as normas (módulos) dos respectivos vetores. O resultado varia entre -1 e 1, mas no contexto de NLP, os valores geralmente estão entre 0 (sem similaridade) e 1 (similaridade máxima), já que os vetores de \textit{embeddings} costumam estar em um espaço de dimensão positiva.

    Valores próximos de 1 indicam que os vetores estão quase na mesma direção, sugerindo alta similaridade. Se o valor estiver perto de 0, significa que os vetores são ortogonais, ou seja, não têm relação direta. Já valores próximos de -1 indicam direções opostas, o que representa dissimilaridade, como mostra a imagem abaixo em uma simplificação 2D:

    \begin{figure}[H]
        \centering
        \caption{\label{fig:vetores} O ângulo entre os vetores é diretamente proporcional à sua similaridade.}
        \includegraphics[width=\textwidth]{assets/vetores.png}
        \legend {Fonte: Elaboração própria, inspirada em \cite{citacao38}}
    \end{figure}

    A vantagem da similaridade cosseno sobre outras métricas, como a distância euclidiana, está no fato de que ela é invariável à magnitude dos vetores. Isso significa que dois vetores com direções similares, mas comprimentos diferentes (por exemplo, textos curtos vs. longos com mesmo conteúdo), ainda podem ser considerados semanticamente similares. Isso é particularmente útil em tarefas de busca semântica e recuperação de informações, onde a intenção é encontrar conteúdos com sentido próximo, independentemente da sua extensão textual \cite{citacao67}.

    Na prática, ao aplicar essa métrica, os textos são primeiro convertidos em vetores numéricos usando técnicas de \textit{embeddings} (como Word2Vec, BERT ou Sentence-BERT), e depois esses vetores são comparados utilizando a similaridade cosseno para ranquear ou agrupar documentos. Essa abordagem é a base de diversos sistemas modernos de busca jurídica, recomendação de jurisprudência e agrupamento de decisões semelhantes \cite{citacao35}.

    É importante ressaltar que o cosseno não capta relações mais complexas como ironia, polissemia ou implicaturas contextuais, para esses casos, a escolha do modelo de \textit{embedding} (estático ou contextualizado) é determinante para que os vetores usados na comparação contenham essas nuances semânticas. Ainda assim, a similaridade cosseno continua sendo uma escolha padrão e eficaz em aplicações de NLP por sua simplicidade, desempenho e capacidade de generalização \cite{citacao10}.


    \section{Algoritmos}

    Algoritmos são conjuntos finitos de instruções bem definidas, ordenadas e executáveis que visam resolver um problema ou realizar uma tarefa específica. Conforme definido por Cormen et al. \cite{citacao26}, “um algoritmo é qualquer procedimento computacional bem definido que toma algum valor ou conjunto de valores como entrada e produz algum valor ou conjunto de valores como saída”. Segundo o dicionário Michaelis \cite{citacao27}, algoritmo é “Conjunto de regras e operações e procedimentos, definidos e ordenados usados na solução de um problema”. Dessa forma, os algoritmos constituem a base do raciocínio computacional e do desenvolvimento de software, estruturando a lógica para a execução de tarefas de maneira clara e eficiente.

    \subsection{Avaliação de algoritmos: Coeficiente de \textit{Spearman}}

    Neste trabalho, após ordenarmos os documentos do mais ao menos importante com base nos algoritmos analisados, é fundamental dispor de uma métrica que nos permita comparar essas listas. Essa comparação é essencial para medir a consistência dos algoritmos e avaliar quão semelhantes são os rankings que eles produzem \cite{citacao71}. Para isso, utilizaremos o coeficiente de correlação de \textit{Spearman}, uma técnica estatística não paramétrica que quantifica a força e a direção da relação entre duas variáveis ordenadas \cite{citacao70}.

    Para entender o coeficiente de \textit{Spearman}, é útil primeiro diferenciá-lo do coeficiente de correlação de \textit{Pearson}. O \textit{Pearson} é uma medida estatística que indica a força e a direção da relação \textit{linear} entre duas variáveis contínuas. Ele avalia se os pontos de dados se aproximam de uma linha reta, mas é sensível a \textit{outliers} e assume que os dados seguem uma distribuição normal \cite{citacao70}.

    O \textit{Pearson} avalia diretamente os valores dos \textit{scores} de relevância, o que não é ideal para o nosso problema. Não nos importa se o \textit{score} do BM25 foi 0.8 e o do \textit{embedding} foi 0.9; o que nos importa é se ambos concordam que aquele documento é o "primeiro mais relevante". É justamente nesse cenário que o coeficiente de \textit{Spearman} se torna mais apropriado. Ele não olha para os \textit{scores} brutos, mas sim para os \textit{postos} (ou \textit{ranks}) dos elementos, medindo a correlação monotônica entre eles \cite{citacao70, citacao71}.

    Na prática, o coeficiente de \textit{Spearman} é simplesmente definido como o \textbf{coeficiente de correlação de \textit{Pearson} calculado sobre os postos das variáveis}. O processo de cálculo envolve transformar os \textit{scores} de cada lista em uma sequência de postos (1º, 2º, 3º, ...). Caso ocorram empates (por exemplo, dois documentos com o mesmo \textit{score}), atribui-se a eles a média de suas posições. Por exemplo, se dois documentos empatam nas posições 2 e 3, ambos recebem o posto 2,5. Após a transformação, o coeficiente de \textit{Pearson} é aplicado sobre essas duas novas listas de postos \cite{citacao70}.

    O resultado, assim como o \textit{Pearson}, varia de -1 a +1. Valores próximos de +1 indicam uma forte concordância (os algoritmos produziram rankings muito semelhantes). Valores próximos de -1 indicam uma forte discordância (um algoritmo ranqueou na ordem inversa do outro). Valores próximos de 0 apontam para uma ausência de correlação, sugerindo que os rankings são aleatórios um em relação ao outro \cite{citacao70}.

    No contexto deste trabalho, o coeficiente de \textit{Spearman} fornecerá uma medida objetiva do grau de concordância entre as listas produzidas pelas abordagens estatísticas (TF-IDF, BM25) e semânticas (\textit{embeddings}). Conforme demonstrado em \citeonline{citacao71}, que utilizou essa métrica para comparar algoritmos de ranqueamento textual, o \textit{Spearman} é uma ferramenta eficaz para avaliar a correlação entre os resultados de diferentes sistemas de busca.

    \begin{figure}[H]
        \centering
        \caption{\label{fig:SpearmanCalc}Demonstração do coeficiente de \textit{Spearman} aplicado à sequência de cores}
        \includegraphics[width=0.8\textwidth]{assets/SpearmanCalc}
        \legend {Fonte: Elaboração própria.}
    \end{figure}


    \section{Programação Paralela e Escalabilidade}

    A programação paralela vem como uma das soluções propostas para os desafios modernos da computação. A ideia por trás dela foi impulsionada pela necessidade de otimizar o tempo de processamento através algoritmos eficientes para criar aplicações que sejam capazes de lidar com um grande volume de dados e cálculos intensivos \cite{citacao12}, coisa que os algoritmos sequenciais não são tão adequados.

    \subsection{Algoritmos Sequenciais e Limitações}

    Algoritmos sequenciais representam o início da computação, onde processadores e algoritmos tinham a premissa da restrição a ordem das instruções, ou seja, uma operação de cada vez, o fim de uma representa o início da próxima. Seguindo assim uma sequência lógica e clara \cite{citacao13} como é representado na Figura \ref{fig:AlgSequenciais}.

    \begin{figure}[H]
        \centering
        \caption{\label{fig:AlgSequenciais}Algoritmos sequenciais}
        \includegraphics[width=\textwidth]{assets/Algoritmos sequenciais.png}
        \legend {Fonte: Elaboração própria.}
    \end{figure}

    \subsection{Algoritmos Paralelos: Modelos e Bibliotecas}

    Algoritmos paralelos são a forma de programação que se utiliza de arquiteturas paralelas e suas características para que várias operações e instruções possam ser executadas simultaneamente dentro da estrutura, podendo esta ser, \textit{multicore}, multiprocessador ou uma rede de computadores (cluster) \cite{citacao14}.

    \begin{figure}[H]
        \centering
        \caption{\label{fig:AlgParalelos}Algoritmos paralelos}
        \includegraphics[width=\textwidth]{assets/Algoritmos paralelos.png}
        \legend {Fonte: Elaboração própria.}
    \end{figure}

    Com o avanço da computação, da engenharia de software, o surgimento de várias arquiteturas \textit{multicore} e sistemas distribuídos, a paralelização dentro dessas estruturas nos permite dividir tarefas complexas em subtarefas executadas simultaneamente, maximizando a eficiência e o desempenho. Contudo, o desenvolvimento de algoritmos paralelos exige lidar com questões como sincronização, concorrência e balanceamento de carga, tornando essa prática essencial para atender às demandas de rapidez e escalabilidade atualmente \cite{citacao13}.


    \section{Trabalhos Relacionados}

    \subsection{A Consolidação do BM25}

    O trabalho fundamental de \citeonline{robertson1995okapi} na conferência TREC-3 estabeleceu o Okapi BM25 como o estado da arte em modelos probabilísticos. Ao comparar o BM25 com o tradicional TF-IDF em um corpus diversificado, os autores demonstraram que a normalização pelo tamanho do documento e a saturação da frequência de termos corrigiam distorções de relevância em textos longos.

    \subsection{Eficiência em Representações Vetoriais}

    Em “Efficient Estimation of Word Representations in Vector Space”, \citeonline{mikolov2013efficient} mudaram o paradigma da busca semântica ao propor o Word2Vec. O estudo focou no \textit{trade-off} computacional, comparando as arquiteturas CBOW (mais rápida) e Skip-Gram (mais precisa semanticamente). Os autores concluíram que é possível treinar modelos de alta qualidade em corpora massivos com custos de CPU aceitáveis, desde que a arquitetura seja simplificada (removendo camadas ocultas não lineares).

    \subsection{Comparação no Domínio Jurídico Brasileiro}

    Mais recentemente, no contexto nacional, \citeonline{citacao62} investigou a classificação de petições jurídicas comparando modelos clássicos, como Naive Bayes e SVM com TF-IDF, contra modelos baseados em \textit{Transformers} (BERT). Os resultados mostraram que, embora o BERT ofereça vantagens em tarefas complexas, modelos mais simples baseados em frequência de termos ainda apresentam desempenho competitivo com um custo computacional drasticamente menor.

% --- CAPÍTULO 4: DESENVOLVIMENTO ---


\chapter{Desenvolvimento}

O desenvolvimento da solução foi guiado por requisitos não funcionais de alto desempenho e eficiência no uso de recursos, essenciais para o processamento de grandes volumes de dados jurídicos. A arquitetura do sistema foi projetada de forma monolitica e autocontida, visando eliminar a latência de rede típica de microsserviços e maximizar o aproveitamento da memória RAM.

Para materializar essa arquitetura, optou-se pela utilização da linguagem Go. Essa escolha foi estritamente técnica, motivada pela capacidade da linguagem de gerar binários compilados nativos e pelo seu modelo eficiente de concorrência. No entanto, os conceitos e algoritmos aqui apresentados são agnósticos à tecnologia e poderiam ser reproduzidos em outras linguagens de sistema.

As seções seguintes descrevem o ciclo de vida da informação dentro do sistema, desde a estruturação do corpus até a execução dos algoritmos de similaridade.

\section{Visão Geral do Ciclo de Desenvolvimento}

Para atingir o objetivo de comparar a eficiência entre métodos estatísticos e semânticos, o desenvolvimento do sistema seguiu um fluxo linear de engenharia de dados, dividido em cinco etapas fundamentais. Esta abordagem garantiu que a comparação fosse realizada sobre uma base controlada e auditável:

\begin{enumerate}
    \item \textbf{Construção do Corpus (Etapa de Carga):} Obtenção dos documentos brutos (PLs e PECs) e sua persistência segura em disco, garantindo que ambos os métodos (estatístico e semântico) operassem sobre exatamente o mesmo conjunto de dados.
    \item \textbf{Pipeline de Normalização (Etapa de Limpeza):} Transformação de arquivos PDF não estruturados em texto puro higienizado, removendo ruídos (cabeçalhos, rodapés, numerações) que poderiam distorcer os cálculos de frequência de termos.
    \item \textbf{Estruturação de Índices (Etapa de Organização):} Implementação de estruturas de dados em memória e banco de dados (SQLite) para armazenar vocabulários e N-gramas, otimizando o tempo de acesso para a etapa de busca.
    \item \textbf{Implementação dos Algoritmos (Etapa de Construção):} Codificação manual ("\textit{from scratch}") dos algoritmos TF-IDF e BM25 em linguagem Go, e integração com o modelo de IA (Sentence-BERT), permitindo controle total sobre métricas de tempo e memória.
    \item \textbf{Experimentação e Coleta de Métricas (Etapa de Validação):} Execução de cenários de teste controlados para mensurar o tempo de resposta, o consumo de RAM e a correlação de rankings (Spearman).
\end{enumerate}

As seções a seguir detalham a implementação técnica de cada uma dessas etapas.


    \section{Definição do Corpus Experimental}

Para a realização dos experimentos comparativos, foi estabelecido um \textit{corpus} legislativo composto por documentos públicos oriundos do Portal da Câmara dos Deputados. A escolha por dados oficiais garante que os testes de desempenho e relevância reflitam a complexidade vocabular e estrutural real enfrentada por sistemas jurídicos em produção.

O conjunto de dados totaliza aproximadamente 5.000 documentos, abrangendo duas classes principais de proposições legislativas:
\begin{itemize}
    \item \textbf{Projetos de Lei (PL):} Textos normativos que propõem novas leis ou alterações na legislação existente.
    \item \textbf{Propostas de Emenda à Constituição (PEC):} Textos de teor constitucional com alta densidade jurídica.
\end{itemize}

Os arquivos foram obtidos e processados em seu formato original (PDF), preservando as características de dados não estruturados típicas do ambiente legal. Essa massa documental serviu como base idêntica tanto para a construção dos índices invertidos (nos algoritmos estatísticos) quanto para a geração dos vetores de \textit{embeddings} (na abordagem semântica), garantindo a validade da comparação.

    \section{Pré-processamento e Normalização Textual}

    Após a etapa de aquisição, o \textit{corpus} bruto constitui-se de milhares de arquivos em formato PDF. A utilização direta desses arquivos para fins de indexação ou vetorização é inviável devido à presença de formatação binária e, principalmente, de "ruído textual", elementos que, embora necessários para a validade jurídica do documento (como cabeçalhos, numeração de autos, datas e assinaturas), não contribuem para a identificação do conteúdo semântico da matéria legislativa.

    Para mitigar esse problema, foi implementado um módulo robusto de processamento textual em Go, localizado no pacote \texttt{corpus}. Este módulo opera como um \textit{pipeline} de transformação, convertendo documentos não estruturados em sequências de \textit{tokens} normalizados.

    \subsection{Arquitetura de Processamento Concorrente}

    Dada a alta carga de I/O exigida para a leitura e conversão de milhares de arquivos PDF, a implementação não seguiu uma abordagem sequencial. Em vez disso, utilizou-se o padrão de projeto \textit{Worker Pool} nativo da linguagem Go.

    A função orquestradora, \texttt{TextProcessor}, define um limite de concorrência controlado pela variável \texttt{maxWorkers}.

    O funcionamento do sistema baseia-se em três primitivas de sincronização essenciais. Um canal buferizado (\texttt{workerLimit}) atua como um semáforo para controlar a concorrência, bloqueando a criação de novas \textit{goroutines} quando o limite de processos simultâneos é atingido, o que previne a exaustão de recursos do sistema operacional como memória RAM e descritores de arquivos. O processamento de cada documento ocorre em sua própria \textit{thread} leve, ou \textit{goroutine}, permitindo que a CPU realize a limpeza textual de um arquivo enquanto outro aguarda operações de disco. Para garantir a completude do fluxo, o mecanismo \texttt{sync.WaitGroup} assegura que o processo principal aguarde a finalização de todas as tarefas de limpeza antes de encerrar a execução.

    \subsection{Extração de Texto}

    A primeira etapa do \textit{pipeline} é a extração do conteúdo textual. Devido à complexidade do \textit{layout} de documentos legislativos (frequentemente diagramados em colunas ou contendo quebras de página irregulares), bibliotecas nativas de leitura de PDF muitas vezes falham em manter a ordem lógica do texto.

    Para garantir a fidelidade da extração, o sistema realiza uma chamada de sistema para a ferramenta externa \texttt{pdftotext} (parte do pacote \textit{poppler-utils}). A função \texttt{processPDF} executa este comando via \texttt{os/exec}, direcionando o fluxo de texto extraído diretamente para a memória da aplicação, evitando escritas intermediárias desnecessárias em disco nesta fase.

    \subsection{Pipeline de higienização}

    O texto bruto é processado pela função \texttt{CleanText}, no pacote \texttt{utils}. A função aplica uma sequência de transformações destrutivas e normalizadoras pensadas para documentos jurídicos.

    \subsubsection{Minúsculas}

    Todo o conteúdo é convertido para letras minúsculas com \texttt{strings.ToLower}, reduzindo variação do vocabulário e unificando termos como “LEI”, “Lei” e “lei”.

    \subsubsection{Remoção por regex}

    Usamos expressões regulares para identificar e remover padrões que não contribuem para a busca por assunto:
    \begin{enumerate}
        \item \textbf{Numerais romanos:} elementos como “Inciso IV” ou “Capítulo XX” são removidos, já que a posição numérica não ajuda na similaridade semântica.
        \item \textbf{URLs e links:} endereços (\texttt{https?://...}) são eliminados para evitar indexar rodapés e metadados.
        \item \textbf{Datas e números:} datas e valores numéricos são retirados, pois o foco está na similaridade temática e não em dados específicos.
    \end{enumerate}

    \subsubsection{Normalização de pontuação e caracteres}

    O texto é percorrido caractere a caractere, removendo pontuação e símbolos com \texttt{unicode.IsPunct} e \texttt{unicode.IsSymbol}. Hífens, travessões e \texttt{underscores} viram espaços para evitar palavras unidas.
    A normalização de acentos usa um mapa carregado de \texttt{misc/replaces.json}, convertendo caracteres estendidos para equivalentes ASCII, o que reduz problemas de codificação e digitação.

    \subsubsection{Remoção de stopwords}

    Por fim, a biblioteca \texttt{bbalet/stopwords} remove palavras funcionais do português, evitando que conectivos e artigos aumentem artificialmente a similaridade entre documentos e afetem TF-IDF ou BM25.

    O resultado é um arquivo \texttt{\_clean.txt} contendo apenas os termos relevantes, pronto para indexação e vetorização.

    \subsection{Geração de N-Gramas e Skip-Grams}

    Uma vez que o conteúdo textual foi higienizado e normalizado, o documento deixa de ser tratado como uma \textit{string} única e passa a ser processado como uma sequência vetorial de \textit{tokens}. A etapa subsequente, e uma das mais críticas para a qualidade da indexação, é a segmentação dessa sequência em unidades de informação indexáveis, conhecidas como N-gramas.

    Para realizar essa tarefa, foi implementada a função utilitária \texttt{GetGramsLim}, localizada no pacote \texttt{utils}. Diferente de implementações genéricas que utilizam recursão para gerar combinações de qualquer tamanho, o que poderia ocasionar estouro de pilha ou uso excessivo de memória em textos muito longos, optou-se por uma abordagem iterativa e explícita, otimizada especificamente para os limites operacionais do projeto: Unigramas, Bigramas e Trigramas, com uma distância máxima de dois saltos (\textit{jumps}) entre as palavras.

    A função foi desenvolvida utilizando o recurso de \textit{Generics} do Go (\texttt{[T any]}), o que confere flexibilidade arquitetural ao sistema: o mesmo algoritmo é capaz de processar vetores de \textit{strings} (durante a fase de testes rápidos) ou vetores de inteiros (\texttt{uint16}), que representam os IDs das palavras no banco de dados de produção.

    A lógica de segmentação adapta-se dinamicamente à cardinalidade do N-grama desejado. Para o caso dos Unigramas ($N=1$), o tratamento é linear e trivial, percorrendo o vetor de entrada de $0$ a $N$ para encapsular cada elemento como uma unidade independente. Já na geração de Bigramas ($N=2$), o algoritmo expande-se para capturar relações entre palavras não necessariamente adjacentes, aplicando o conceito de \textit{skip-gram}. Utiliza-se um laço aninhado onde, enquanto o índice principal $i$ percorre o documento, um índice secundário $j$ projeta-se à frente até o limite de \textit{jumps} ($jump+1$). Isso possibilita a criação de pares adjacentes $(w_i, w_{i+1})$ ou distantes $(w_i, w_{i+2})$, armazenando explicitamente a distância vetorial $j$ para normalização posterior.

    A complexidade aumenta significativamente para os Trigramas ($N=3$), visando a captura de um contexto estendido. Para isso, emprega-se uma estrutura de três laços encadeados ($i, j, k$), onde $i$ atua como a palavra pivô, $j$ define a distância para o segundo termo e $k$ determina a distância para o terceiro. Essa abordagem, que pode ser descrita como uma "força bruta controlada", garante que todas as combinações válidas dentro da janela de saltos — desde as sequenciais $(w_i, w_{i+1}, w_{i+2})$ até as mais esparsas $(w_i, w_{i+2}, w_{i+4})$ — sejam geradas de forma determinística, eliminando a sobrecarga de memória que ocorreria com o uso de chamadas recursivas.

    O retorno da função é uma estrutura dupla contendo tanto os gramas gerados quanto uma matriz de metadados (\texttt{retJumps}), que registra as distâncias relativas (\texttt{int8}) entre cada componente do grama. Esses metadados são essenciais para a estratégia de indexação, pois permitem ao motor de busca diferenciar, por exemplo, uma citação direta de uma menção espaçada no texto.


    \section{Modelagem de Dados e Persistência}

    A eficiência de um sistema de recuperação de informação depende intrinsecamente de como os dados são estruturados e persistidos. Para este projeto, optou-se por uma abordagem híbrida que combina a rigidez de um esquema relacional para manter a integridade dos documentos com a flexibilidade de estruturas de índice invertido para a busca rápida.

    A camada de persistência foi construída sobre o banco de dados SQLite, utilizando o GORM para a manipulação das entidades em Go. A escolha do SQLite justifica-se pela sua arquitetura \textit{serverless} de arquivo único, eliminando a latência de rede típica de conexões cliente-servidor (como em PostgreSQL ou MySQL) e permitindo um desempenho de leitura extremamente elevado quando o arquivo do banco reside em discos SSD NVMe modernos.

    \subsection{Definição das Estruturas de Dados (Structs)}

    A modelagem orientada a objetos do sistema reflete-se nas \textit{structs} definidas no pacote \texttt{models}. Cada \textit{struct} mapeia diretamente para uma tabela no banco de dados, seguindo as convenções do GORM.

    \subsubsection{Entidade Documento e Estratégia de Armazenamento}

    A unidade fundamental do \textit{corpus} é representada pela \textit{struct} \texttt{Document}, definida no arquivo \texttt{models/Document.go}. Contudo, diferentemente de sistemas tradicionais que armazenam o conteúdo completo (BLOBs ou Textos Longos) no banco de dados, este projeto adota uma abordagem híbrida focada em performance.

    \begin{verbatim}
        type Document struct {
            gorm.Model
            Title        string
            Link         string
            CleanTitle   string
            // Content e CleanContent são processados em memória ou lidos do disco
        }
    \end{verbatim}

    Para evitar o crescimento exponencial do arquivo do banco de dados SQLite, o que degradaria o tempo de resposta das consultas de índice, o conteúdo textual integral dos documentos (\textit{Full Text}) \textbf{não é persistido no banco de dados} de forma permanente.

    A estratégia adotada fundamenta-se na persistência em disco, onde os documentos originais e suas versões higienizadas (\texttt{.txt}) são salvos diretamente no sistema de arquivos local (\textit{File System}). Essa abordagem aproveita a velocidade de leitura sequencial de dispositivos SSD/HDD e libera o banco de dados da tarefa de gerenciar grandes volumes de dados não estruturados. Nesse arranjo, a tabela \texttt{documents} no SQLite funciona exclusivamente como um índice de metadados, armazenando apenas informações essenciais como Título, Link original e IDs de controle, atuando como um catálogo leve que referencia os arquivos físicos.

    Complementarmente, utiliza-se a técnica de leitura sob demanda (\textit{Lazy Loading}) para otimizar o consumo de recursos. Durante a execução dos testes de similaridade, o sistema carrega o conteúdo necessário dos arquivos para a memória RAM estritamente para o cálculo dos vetores e a geração dos N-gramas. Imediatamente após a indexação do documento e a geração das tabelas de Unigramas ou Trigramas, o texto bruto é descartado da memória, assegurando que o \textit{footprint} de RAM do processo permaneça eficiente.

    Essa decisão arquitetural garante que o banco de dados permaneça compacto, contendo apenas as estruturas otimizadas de busca (Índice Invertido), enquanto o armazenamento "pesado" é delegado ao sistema operacional.

    \subsection{Dicionário de Palavras}

    Para otimizar o armazenamento e as comparações, as palavras não são repetidas nas tabelas de índice. Em vez disso, utiliza-se uma tabela de vocabulário único, representada pela \textit{struct} \texttt{Word}.

    \begin{verbatim}
        type Word struct {
            gorm.Model
            Word string `gorm:"uniqueIndex"`
        }
    \end{verbatim}

    A \textit{tag} \texttt{uniqueIndex} na coluna \texttt{Word} é crucial: ela garante que cada termo apareça apenas uma vez no banco de dados e cria um índice B-Tree físico no SQLite, permitindo que a conversão de uma \textit{string} para seu \texttt{WordID} ocorra em tempo logarítmico $O(\log N)$.

    \subsection{Estrutura do Índice Invertido e Polimorfismo}

    A implementação do índice invertido foge à abordagem trivial de mapear "Palavra $\rightarrow$ Documentos". Para capturar o contexto local e permitir buscas por frases e proximidade, o sistema modela o índice através de três entidades distintas: \textbf{Unigramas}, \textbf{Bigramas} e \textbf{Trigramas}.

    Para que os algoritmos de cálculo de relevância (TF-IDF e BM25) pudessem operar de forma agnóstica em relação ao tamanho do n-grama, definiu-se a interface polimórfica \texttt{IGram}, localizada no pacote \texttt{models/interfaces}.

    \begin{verbatim}
        type IGram interface {
            GetCacheKey(jumps, doc bool) string
            GetDocId() uint16
            Increment()
            GetCount() int
            ApplyWordWheres(db *gorm.DB) *gorm.DB
            ApplyJumpWheres(db *gorm.DB) *gorm.DB
            schema.Tabler
        }
    \end{verbatim}

    Esta interface é crucial para a arquitetura do sistema, pois permite abstrair a complexidade das consultas SQL. Métodos como \texttt{ApplyWordWheres} e \texttt{ApplyJumpWheres} encapsulam a lógica de filtragem do banco de dados, permitindo que uma função de busca receba um \texttt{IGram} genérico e construa a \textit{query} correta dinamicamente, seja ela para um termo único ou uma frase de três palavras.

    A materialização dessa interface ocorre nas \textit{structs} concretas. A estrutura de um trigrama, denominada \texttt{InverseTrigram}, exemplifica a otimização de memória aplicada:

    \begin{verbatim}
        type InverseTrigram struct {
            Wd0Id uint16 `gorm:"uniqueIndex:compositeindex"`
            Wd1Id uint16 `gorm:"uniqueIndex:compositeindex"`
            Wd2Id uint16 `gorm:"uniqueIndex:compositeindex"`
            DocId uint16 `gorm:"uniqueIndex:compositeindex"`
            Jump0 int8   `gorm:"uniqueIndex:compositeindex"`
            Jump1 int8   `gorm:"uniqueIndex:compositeindex"`
            Count int
        }
    \end{verbatim}

    Nesta modelagem, destaca-se primeiramente o uso de um Índice Composto (\textit{Composite Index}). Conforme definido pelas \textit{tags} do GORM (\texttt{uniqueIndex:compositeindex}), os seis primeiros atributos — que abrangem as palavras, o identificador do documento e as distâncias — constituem, em conjunto, uma chave única composta. Essa estratégia assegura a integridade dos dados ao impedir a duplicação de registros para a mesma ocorrência semântica e, simultaneamente, materializa um índice B-Tree otimizado no SQLite, resultando em uma aceleração drástica nas operações de leitura.

    Para otimização de armazenamento, adotou-se o uso de Chaves Estrangeiras Virtuais Leves. Os campos de referência à tabela de vocabulário (\texttt{Wd...Id}) armazenam estritamente inteiros sem sinal de 16 bits (\texttt{uint16}). Embora essa tipagem imponha um limite técnico de 65.535 termos únicos e documentos — quantidade suficiente após a etapa de limpeza agressiva —, ela garante que cada linha do índice ocupe uma fração mínima de espaço em disco quando comparada ao armazenamento de \textit{strings}, maximizando a densidade de informação.

    Por fim, incorpora-se o conceito de "Jumps" (\textit{Skip-Grams}) para superar a limitação da adjacência estrita. As estruturas de \texttt{Bigram} e \texttt{Trigram} utilizam inteiros de 8 bits (\texttt{int8}) para registrar a distância entre palavras (\texttt{Jump}). Isso viabiliza a indexação de padrões não contíguos; por exemplo, a expressão "recurso \textit{de} apelação" pode ser capturada como o bigrama "recurso apelação" com um salto de 1 (\texttt{Jump=1}). Essa abordagem amplia a flexibilidade semântica da busca exata sem sacrificar a precisão posicional dos termos.

    \subsection{Estratégia de Isolamento e Duplicação}

    Para garantir a integridade dos testes comparativos e evitar que a execução de um cenário influenciasse os resultados de outro (por exemplo, cache de páginas do sistema operacional ou fragmentação de índices), implementou-se um mecanismo de duplicação de banco de dados sob demanda.

    Antes de iniciar uma bateria de testes, o sistema verifica a necessidade de criar um ambiente limpo ou reutilizar dados pré-processados. Caso a \textit{flag} \texttt{fromScratch} seja falsa, a função \texttt{InitDB} invoca \texttt{DuplicateFile} para criar uma cópia física do arquivo do banco de dados mestre (contendo o \textit{corpus} limpo) para um novo arquivo identificado pelo ID da execução atual (ex: \texttt{corpus\_12345.db}).

    Essa abordagem de "Sandbox por Processo" elimina a concorrência de escrita no nível do arquivo do banco de dados e permite que múltiplos testes rodem em paralelo ou em sequência sem condições de corrida e com isolamento total de transações.

    \subsection{Otimização por Esquema Único (Single-Gram Schema)}

    Uma decisão arquitetural crítica para a performance do sistema foi a restrição de armazenar \textbf{apenas um tipo de n-grama por instância de banco de dados}.

    Embora fosse possível criar tabelas distintas para Unigramas, Bigramas e Trigramas coexistindo no mesmo banco, isso implicaria em um aumento desnecessário do tamanho do arquivo e na dispersão das páginas de dados no disco, prejudicando a localidade de referência e o desempenho do cache do SQLite.

    Para mitigar isso, a função \texttt{InitDB} utiliza uma estrutura de decisão (\texttt{switch gramSize}) para definir dinamicamente qual modelo será migrado para o banco de dados daquela execução específica:

    \begin{verbatim}
        switch gramSize {
        case 1:
            gramModel = &models.InverseUnigram{}
            index = "(wd0Id)"
        case 2:
            gramModel = &models.InverseBigram{}
            index = "(wd0Id, wd1Id)"
        case 3:
            gramModel = &models.InverseTrigram{}
            index = "(wd0Id, wd1Id, wd2Id)"
        }
    \end{verbatim}

    Dessa forma, se um teste avalia a performance de Trigramas, o banco de dados é instanciado contendo apenas a tabela \texttt{InverseTrigram}. Isso garante que a tabela de índices (\texttt{WORD\_DOC}) seja extremamente compacta e otimizada exclusivamente para as consultas daquele tamanho de grama, eliminando o \textit{overhead} de manutenção de índices não utilizados.

    \subsection{Indexação Dinâmica}

    Complementando a estratégia de esquema único, a criação de índices também é realizada dinamicamente via execução de SQL puro no momento da inicialização. O sistema executa o comando \texttt{CREATE INDEX} ajustado para as colunas específicas do n-grama selecionado (conforme a variável \texttt{index} definida no trecho de código acima).

    Isso assegura que as consultas de busca (\textit{SELECT}), que realizam junções pesadas entre o vocabulário e a tabela de ocorrências, sempre utilizem um índice de cobertura (\textit{Covering Index}) ou um índice B-Tree otimizado, reduzindo a complexidade da busca de $O(N)$ para $O(\log N)$.


    \section{Implementação dos Algoritmos de Busca e Recuperação}
    \label{sec:algoritmos_busca}

    A lógica de recuperação de informação representa o núcleo intelectual do sistema. Enquanto a maioria das soluções comerciais delega essa tarefa para motores de busca prontos (como Elasticsearch ou Solr), neste trabalho optou-se pela implementação manual ("\textit{from scratch}") dos algoritmos TF-IDF e BM25 em Go. Esta decisão permitiu um controle granular sobre a ponderação dos termos e a otimização do uso de memória através de concorrência.

    \subsection{Pipeline Estatístico: TF-IDF}

    O algoritmo TF-IDF (\textit{Term Frequency - Inverse Document Frequency}) foi implementado no pacote \texttt{utils}, especificamente no arquivo \texttt{tf\_idf.go}. A função principal, \texttt{ComputeDocPreIndexedTFIDF}, calcula o vetor de pesos para um documento dado um conjunto de N-gramas.

    A implementação matemática segue a fórmula clássica com suavização:
    \[
        \text{TF-IDF}(t, d) = \left( \frac{f_{t,d}}{\sum_{t' \in d} f_{t',d}} \right) \times \log \left( \frac{N}{1 + df_t} \right)
    \]

    O código implementa esta lógica em duas fases distintas para maximizar a eficiência:

    \begin{enumerate}
        \item \textbf{Cálculo do TF (Local):} O algoritmo itera sobre a lista de trigramas (\texttt{trigramList}) do documento, acumulando a frequência bruta de cada termo em um mapa em memória (\texttt{tf}).
        \item \textbf{Cálculo do DF (Global) Concorrente:} A etapa mais custosa é determinar em quantos documentos do \textit{corpus} cada termo aparece ($df_t$). Para evitar que essa operação bloqueie o processamento, utilizou-se o padrão de \textit{fan-out/fan-in} com \textit{goroutines}.
    \end{enumerate}

    \textbf{Otimização de Concorrência:}
    O trecho de código abaixo ilustra como o paralelismo foi controlado para evitar sobrecarga no banco de dados:

    \begin{verbatim}
        sem := make(chan struct{}, 25) // Semáforo de capacidade 25
        var wg sync.WaitGroup

        for key := range tf {
            wg.Add(1)
            sem <- struct{}{} // Adquire token
            go func(key string) {
                defer wg.Done()
                defer func() { <-sem }() // Libera token
                df := len(cacheN[key])   // Acesso thread-safe ao cache
                dfChan <- dfResult{key: key, df: df}
            }(key)
        }
    \end{verbatim}

    Um canal bufferizado (\texttt{sem}) atua como um semáforo limitador, garantindo que nunca haja mais de 25 \textit{goroutines} acessando o cache ou o banco de dados simultaneamente. Isso estabiliza o \textit{throughput} da CPU e previne condições de corrida em ambientes de alta carga.

    \subsection{Pipeline Probabilístico: Okapi BM25}

    Reconhecido como o estado da arte para buscas baseadas em palavras-chave, o algoritmo BM25 foi implementado no arquivo \texttt{bm25.go}. Diferente do TF-IDF, o BM25 normaliza a frequência do termo pelo tamanho do documento, penalizando textos excessivamente longos que poderiam ter altas contagens de palavras apenas por serem verbosos.

    A função \texttt{ComputeDocPosIndexedBM25} implementa a equação:

    \[
        \text{Score}(D,Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})}
    \]

    \textbf{Parametrização e Ajuste Fino:}
    Os hiperparâmetros do algoritmo foram fixados diretamente no código fonte, seguindo valores recomendados pela literatura de Recuperação de Informação para domínios gerais, mas aplicáveis ao jurídico:
    \begin{itemize}
        \item \textbf{$k_1 = 1.5$:} Controla a saturação da frequência do termo. Indica o quão rapidamente a relevância aumenta com repetições da mesma palavra.
        \item \textbf{$b = 0.75$:} Controla a intensidade da normalização pelo comprimento do documento. Um valor de 0.75 aplica uma penalização moderada a documentos longos.
    \end{itemize}

    Para calcular o comprimento médio dos documentos (\texttt{avgDL}), essencial para o denominador da equação, o sistema executa uma consulta de agregação otimizada no banco de dados SQLite (\texttt{SELECT SUM(count) FROM WORD\_DOC}), aproveitando os índices criados na etapa de modelagem.

    \subsection{Pipeline Semântico: Vetorização via Transformers}

    Enquanto os algoritmos TF-IDF e BM25 operam no nível léxico, identificando a ocorrência exata de termos, a camada de busca semântica exigiu uma abordagem capaz de capturar a intenção e o significado contextual das consultas. Para essa tarefa, integrou-se ao sistema um módulo de inferência baseado em modelos \textit{Transformer}.

    Diferente de abordagens tradicionais que exigiriam serviços externos ou linguagens de \textit{scripting} separadas, a implementação em Go permitiu manter a arquitetura coesa e performática. O sistema carrega o modelo pré-treinado e realiza a tokenização e a vetorização diretamente na memória do processo principal, eliminando a latência de comunicação entre serviços e simplificando o \textit{deploy} da solução.

    \subsubsection{Arquitetura do Componente Semântico}

    O componente de inferência foi desenhado para operar de forma desacoplada dos algoritmos estatísticos, mas compartilhando as mesmas estruturas de dados básicas. O fluxo de execução semântica segue as seguintes etapas:

    \begin{enumerate}
        \item O módulo de pré-processamento realiza a limpeza e normalização do texto, preparando os insumos para a vetorização.
        \item O sistema carrega o modelo \texttt{Sentence-BERT} e seus respectivos tokenizadores.
        \item O conteúdo textual dos documentos é convertido em \textit{tokens} e submetido à rede neural, gerando vetores densos (embeddings) de 384 dimensões.
        \item Esses vetores são armazenados em estruturas otimizadas em memória para permitir comparações rápidas.
        \item No momento da busca, a frase de consulta passa pelo mesmo processo de vetorização.
        \item Por fim, calcula-se a similaridade de cosseno entre o vetor da consulta e os vetores de todos os documentos, gerando as listas ordenadas de relevância semântica.
    \end{enumerate}

    \subsubsection{Seleção de Modelos e Mudança de Escopo}

    Uma das decisões metodológicas mais importantes deste trabalho ocorreu durante a fase de definição dos modelos de IA. Inicialmente, o projeto de pesquisa previa uma análise comparativa ampla, confrontando não apenas algoritmos estatísticos, mas também três arquiteturas distintas de representação vetorial:
    \begin{enumerate}
        \item Um modelo \textit{Transformer} robusto e de grande porte (como o BERT-Large ou RoBERTa), visando a máxima precisão teórica.
        \item O modelo Word2Vec, baseado em arquiteturas neurais preditivas rasas.
        \item Um modelo GloVe, treinado com dados da rede social \textit{Twitter}, para avaliar o impacto de um vocabulário mais informal.
    \end{enumerate}

    No entanto, durante a fase exploratória e os testes preliminares, observou-se uma inconsistência significativa nos resultados. As variações nos \textit{rankings} de relevância gerados pelos três modelos para o \textit{corpus} jurídico apresentaram níveis de divergência (entropia) tão elevados que inviabilizariam uma comparação justa. Documentos considerados altamente relevantes por um modelo apareciam frequentemente como irrelevantes em outro, criando um cenário de incerteza que não justificava a complexidade de manter e orquestrar três ambientes de inferência pesados simultaneamente.

    Diante dessa evidência, optou-se por uma mudança estratégica de escopo. Em vez de dispersar o foco da análise usando diversos modelos de IA, o trabalho concentrou-se na dicotomia principal: "Estatística (Go) versus Semântica Contextual (Python/BERT)".

    Para representar o estado da arte na busca semântica eficiente, selecionou-se unicamente o modelo \texttt{paraphrase-multilingual-MiniLM-L12-v2} (baseado em BERT).

    A escolha deste modelo específico fundamentou-se em três pilares essenciais. O primeiro é a eficiência de inferência: por pertencer à família "MiniLM", com apenas 12 camadas e vetores de 384 dimensões, o modelo viabilizou a execução de uma bateria massiva de testes em tempo hábil, dispensando o uso de \textit{hardware} de supercomputação como GPUs dedicadas. O segundo pilar é o suporte multilíngue, dado que o treinamento prévio em mais de 50 idiomas conferiu ao modelo uma capacidade robusta de interpretar a sintaxe do português sem a necessidade de ajustes finos (\textit{fine-tuning}). Por fim, destaca-se a densidade do espaço vetorial, onde as 384 dimensões demonstraram ser suficientes para codificar as nuances dos textos legislativos, estabelecendo um "gabarito" confiável e estável para a avaliação dos algoritmos estatísticos.

    \chapter{Apresentação dos Resultados e Considerações Finais}

    A etapa de validação experimental deste trabalho não consistiu apenas na execução mecânica de algoritmos mas sim em uma investigação sobre o comportamento de sistemas de recuperação de informação sob condições de estresse computacional. Para garantir a robustez dos dados executamos uma bateria de setenta e dois cenários de teste distintos onde cada um foi feito para isolar variáveis críticas do sistema. O objetivo central foi submeter a implementação desenvolvida na linguagem Go a diferentes pressões de configuração para entendermos de que maneira fatores como o tamanho dos N-gramas e a presença de saltos entre palavras influenciam tanto o desempenho da máquina quanto a qualidade da informação entregue ao usuário final.

    Os dados coletados e apresentados a seguir oferecem uma visão clara e técnica sobre como os algoritmos clássicos se comportam quando são confrontados com a infraestrutura moderna e com a complexidade inerente à linguagem jurídica. Ao longo deste capítulo detalhamos os destaques numéricos e discutimos os gargalos de armazenamento em banco de dados além de analisar a correlação de qualidade obtida frente ao gabarito semântico gerado por inteligência artificial.


    \section{Destaques e Indicadores de Performance }

    Antes de aprofundarmos a análise qualitativa é fundamental destacar os números mais expressivos que emergiram durante a fase de testes pois eles resumem os extremos de performance e consumo que definem os limites arquiteturais do sistema proposto.

    O indicador mais alarmante foi o pico de consumo de memória RAM que atingiu a marca de aproximadamente 5 GB quando o sistema foi configurado para processar Trigramas. Esse valor contrasta com o cenário de Unigramas onde o consumo foi de apenas 143 MB. Essa diferença de magnitude ilustra o custo oculto da complexidade linguística.

    No que tange à velocidade de processamento observamos uma inversão de expectativas onde o algoritmo BM25 chegou a ser 10 vezes mais rápido que o TF-IDF em cenários complexos. Isso derruba a intuição de que fórmulas matemáticas mais simples resultam necessariamente em softwares mais rápidos e coloca em evidência a importância da otimização de fluxo de código.

    Outro ponto de destaque negativo foi a performance da camada de persistência em disco. O uso do banco de dados SQLite para consultas em tempo real mostrou-se entre 10 a 40 vezes mais lento do que as operações realizadas em cache de memória. Esse gargalo inviabilizou o uso do disco para o processamento dos Trigramas e forçou uma mudança de estratégia para o uso exclusivo de memória RAM durante os testes de carga.

    Por fim, vale ressaltar a escala do experimento, que envolveu o processamento e indexação de 1.648 documentos legislativos completos. As métricas de qualidade, medidas pelo coeficiente de Spearman, flutuaram majoritariamente entre 0,02 e 0,04, o que evidencia matematicamente a distinção fundamental entre a busca estatística e a busca semântica.

    \subsection{Indicadores}

    Antes da análise qualitativa, alguns dos números que chamaram a atenção durante a fase de testes. Abaixo listamos os recordes e métricas que definem o perfil de desempenho da solução.

    \begin{itemize}
        \item \textbf{O Campeão de Velocidade} \\
        O algoritmo BM25 configurado para processar Bigramas simples sem saltos registrou o tempo de execução mais rápido de toda a bateria e completou a tarefa em \textbf{80 milissegundos}. Esse valor destaca a eficiência da estratégia de poda de dados para índices esparsos.

        \item \textbf{O Cenário de Maior Lentidão} \\
        No outro extremo a configuração de Trigramas normalizados com o uso de saltos máximos elevou o tempo de processamento para cerca de \textbf{7,5 segundos}. Isso representa uma execução quase 100 vezes mais lenta do que o melhor cenário e evidencia o custo computacional da complexidade linguística excessiva.

        \item \textbf{Eficiência de Memória} \\
        Para sistemas com poucos recursos o uso de Unigramas provou ser extremamente leve exigindo apenas \textbf{143 MB} de memória RAM no seu pico. Isso viabiliza a execução da busca até mesmo em hardwares modestos ou contêineres de nuvem básicos.

        \item \textbf{O Custo do Contexto} \\
        Em contraste a indexação de Trigramas exigiu um pico de \textbf{4,72 GB} de memória. Esse salto de 32 vezes no consumo em relação aos Unigramas serve como um alerta importante sobre a escalabilidade de índices contextuais em memória.

        \item \textbf{Melhor Aproximação Semântica} \\
        Curiosamente a maior correlação positiva com o gabarito do Sentence-BERT foi encontrada nos testes de Unigramas que atingiram um pico de similaridade de \textbf{0,06}. Isso sugere que para este corpus específico a coincidência de palavras-chave simples alinha-se melhor com a intenção semântica do que as tentativas de capturar frases complexas.
    \end{itemize}

    \subsection{O Desempenho da Persistência em Disco}

    Um capítulo à parte nesta análise diz respeito ao comportamento do banco de dados. Durante o planejamento inicial do sistema a arquitetura previa o uso do SQLite para realizar consultas em tempo real e persistir os índices de N-gramas. No entanto a realidade dos testes de carga impôs uma mudança de estratégia.

    Observamos que as operações de leitura direta no disco foram de \textbf{10 a 40 vezes mais lentas} do que as operações realizadas no cache de memória. Esse gargalo tornou-se proibitivo especialmente nos cenários de Bigramas e Trigramas onde a explosão combinatória de termos gerava milhares de consultas SQL por segundo. O disco simplesmente não conseguia acompanhar a velocidade do processador o que criava uma fila de espera que travava a execução do algoritmo.

    Por essa razão optamos por retirar o banco de dados do caminho crítico da busca em tempo real. Apesar disso a implementação não foi descartada e permanece funcional. Ela assumiu um papel vital como ferramenta de auditoria e segurança servindo como um repositório confiável para recuperação de dados em caso de falhas. Para trabalhos futuros a substituição do SQLite por tecnologias orientadas a colunas ou motores de busca dedicados como o Elasticsearch poderia mitigar essa latência e permitir uma arquitetura híbrida mais eficiente.


    \section*{A Relação entre os Modelos e o Gabarito Semântico}

    A comparação entre os rankings gerados pelos métodos estatísticos e pelo modelo semântico (Sentence-BERT) mostrou correlações baixas, ainda que positivas (média entre 0.02 e 0.04). Isso confirma que cada abordagem captura aspectos distintos:

    \begin{itemize}
        \item modelos estatísticos priorizam coincidência de termos específicos, algo comum em textos jurídicos;
        \item o modelo \textit{transformer} prioriza intenção e significado, mais útil em consultas abertas.
    \end{itemize}

    Na prática, os rankings são diferentes o suficiente para não serem intercambiáveis.

    \subsection{Estabilidade e o Papel dos \textit{Skip-grams}}

    A variação do \textit{jump} nos N-gramas maiores pouco afetou o Spearman. Mesmo ampliando o salto, os resultados permaneceram praticamente iguais. Isso mostra que, no Direito, as combinações relevantes costumam ser adjacentes (“Recurso de Apelação”, “Ação Direta”), tornando caro e pouco útil indexar palavras separadas por um ou dois termos.


    \section{Análise do Consumo de Memória RAM}

    Um dos aspectos mais críticos que observamos durante os experimentos foi a demanda de memória necessária para manter os índices invertidos operando com alta performance. Como a decisão de arquitetura priorizou carregar as estruturas de busca na memória RAM para garantir a velocidade das respostas o tamanho do vocabulário e das combinações de termos teve um impacto direto na viabilidade técnica da solução.

    Os relatórios de monitoramento revelaram um crescimento que não se comportou de forma linear mas sim exponencial à medida que aumentamos a complexidade da indexação. Quando o sistema operou apenas com Unigramas indexando palavras isoladas o consumo de memória foi extremamente modesto e eficiente ficando em torno de 143 MB no seu pico de utilização. Esse dado é extremamente positivo pois demonstra que para buscas simples baseadas apenas em palavras-chave o custo de infraestrutura é muito baixo e acessível até para servidores de entrada ou máquinas pessoais.

    A situação mudou de figura de forma drástica ao introduzirmos os Bigramas. A necessidade de armazenar pares de palavras e suas respectivas distâncias fez o consumo saltar para um pico de 2,3 GB de memória. Esse valor representa um aumento de mais de 16 vezes em relação ao cenário anterior e já começa a exigir máquinas com maior capacidade. O cenário tornou-se ainda mais crítico com os Trigramas onde o sistema exigiu quase 5 GB de memória RAM para operar corretamente.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.6\linewidth]{assets/custoMemoria}
        \caption{Consumo de memória para Unigramas, Bigramas e Trigramas}
        \label{fig:memoria_ram}
    \end{figure}

    Esses números indicam claramente que existe um custo oculto elevado na busca por precisão lexical. Enquanto indexar palavras isoladas é uma operação barata tentar capturar o contexto através de sequências de três termos exige uma quantidade de memória que pode inviabilizar a execução do sistema em ambientes com recursos limitados como dispositivos móveis ou contêineres de nuvem com pouca memória alocada.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.6\linewidth]{assets/tradeOff}
        \caption{Trade-off geral entre custo, memória e qualidade}
        \label{fig:tradeoff_final }
    \end{figure}


    \section{Desempenho Temporal}

    A análise do tempo de execução trouxe dois \textit{insights} fundamentais para o projeto que mudaram a nossa percepção sobre onde estão os verdadeiros gargalos de um sistema de busca. O primeiro diz respeito à eficiência dos algoritmos em memória e o segundo revela as limitações físicas do armazenamento em disco.

    No que tange aos algoritmos a teoria clássica sugere que o BM25 é mais complexo matematicamente do que o TF-IDF pois sua fórmula envolve mais operações logarítmicas e o cálculo da média de tamanho dos documentos. Contudo os dados mostraram o oposto em cenários de alta complexidade. Nos testes com Bigramas e Trigramas o BM25 superou o TF-IDF com uma margem larga de vantagem sendo até dez vezes mais veloz em casos específicos.

    A explicação para esse fenômeno reside na engenharia da implementação pois o código do BM25 permitiu uma otimização de fluxo conhecida como poda ou \textit{pruning}. Essa técnica descarta imediatamente termos que não aparecem no índice e economiza milhares de cálculos inúteis. Como o índice de Trigramas é muito esparso e a maioria das combinações de palavras não existe o BM25 consegue pular a maior parte do trabalho. O TF-IDF por sua vez acabou processando esses termos irrelevantes o que gerou um custo de processamento desnecessário e provou que a otimização de código é mais importante que a simplicidade da fórmula matemática.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.9\linewidth]{assets/tempoExecucao}
        \caption{Tempo de execução comparando TF-IDF e BM25}
        \label{fig:tempo_execucao}
    \end{figure}

    \subsection{A Performance do Banco de Dados versus Cache}

    Um ponto que merece uma discussão aprofundada foi a performance decepcionante da camada de persistência em disco. Durante as fases iniciais de desenvolvimento o sistema foi projetado para realizar consultas diretas ao banco de dados SQLite para cada termo da busca visando economizar memória RAM. No entanto os testes de carga revelaram que essa abordagem se tornava um gargalo severo que travava a execução do sistema.

    Observamos que as operações de leitura no banco de dados eram de dez a quarenta vezes mais lentas do que as operações realizadas diretamente no cache em memória. Essa degradação de performance foi especialmente notável nos cenários de Bigramas e Trigramas onde a quantidade de consultas SQL explodia exponencialmente para verificar cada combinação de palavras. Devido a essa latência proibitiva optamos por retirar o banco de dados do caminho crítico de execução dos testes principais focando os resultados na performance do processamento em memória.

    Apesar de não ter performado bem para a busca em tempo real neste cenário de alta frequência a implementação do banco de dados não foi descartada do projeto. Ela permanece funcional e cumpre um papel vital como ferramenta de auditoria e recuperação de dados em caso de desastres. O banco garante que os dados persistidos estejam seguros e consistentes permitindo que o índice em memória seja reconstruído rapidamente se o servidor for reiniciado. Para trabalhos futuros a substituição do SQLite por bancos de dados orientados a colunas ou soluções de busca dedicadas como o Elasticsearch poderia mitigar esse gargalo de disco e permitir buscas híbridas mais eficientes.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.6\linewidth]{assets/paralelismoQuery}
        \caption{Impacto do paralelismo na performance}
        \label{fig:impacto_paralelismo_query}
    \end{figure}


    \section{A Influência da Extensão da Consulta no Desempenho}

    Uma variável que se mostrou determinante para o comportamento do sistema foi a extensão da consulta realizada pelo usuário. Durante os testes variamos o tamanho das frases de entrada entre dez, vinte e quarenta palavras para simular desde buscas simples até a colagem de trechos inteiros de jurisprudência uma prática comum entre advogados.

    A análise dos dados revelou que o tempo de processamento cresce de forma linear em relação ao tamanho da frase. Isso era esperado pois cada palavra adicional na consulta exige que o algoritmo calcule o peso e a frequência de novos N-gramas. No entanto o que chamou a atenção foi como a execução paralela se comportou diante desse aumento de carga.

    \begin{table}[H]
        \centering
        \caption{Tempo médio de busca para diferentes tamanhos de consulta}
        \label{tab:tempo_busca_consulta}
        \begin{tabular}{lccc}
        \hline
        \textbf{Modelo} & \textbf{10 palavras} & \textbf{20 palavras} & \textbf{40 palavras} \\
        \hline
        TF-IDF      & 33 µs   & 64 µs   & 128 µs  \\
        BM25        & 23 µs   & 48 µs   & 100 µs  \\
        Embeddings  & 1.053 ms & 1.054 ms & 1.044 ms \\
        \hline
        \end{tabular}
    \end{table}



    Embora os resultados evidenciem diferenças significativas de desempenho entre abordagens
    baseadas em modelos léxicos e embeddings, especialmente na ordem de grandeza do tempo
    de busca, os dados apresentados não permitem conclusões definitivas sobre a escalabilidade
    dos modelos. Os experimentos foram conduzidos em um ambiente controlado, com um número
    limitado de consultas e parâmetros fixos, de modo que os resultados devem ser interpretados
    como indicativos do comportamento observado neste cenário específico.



    Para frases curtas de dez palavras o custo de iniciar múltiplas rotinas de processamento quase anulou os ganhos de velocidade o que manteve o modo sequencial altamente competitivo. Contudo à medida que a frase cresceu a vantagem do paralelismo não tornou-se perceptível. Nos testes com Trigramas e quarenta palavras o processamento paralelo não conseguiu ser mais rápido que a execução sequencial. Isso demonstra que o uso da concorrência deve ser condicional ativando-se apenas dentro de cenários estudados, caso contrário não haverá ganhos que compensem a sobrecarga de gerenciar as \textit{threads} pelo volume de trabalho.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.6\linewidth]{assets/tamanhoFrases}
        \caption{Comparação entre execução sequencial e paralela para diferentes tamanhos de consulta }
        \label{fig:tamanho_consulta}
    \end{figure}

    Algoritmos paralelos podem ser mais lentos quando a sobrecarga de coordenação supera o ganho obtido pela divisão do trabalho. Isso inclui custos de criação e gerenciamento de threads, comunicação entre tarefas e sincronização frequente, que podem introduzir atrasos significativos. Além disso, quando as tarefas individuais são muito pequenas, o tempo gasto para distribuí-las e organizar sua execução pode ser maior que simplesmente executá-las de forma sequencial.

    Outro fator importante é o acesso concorrente a recursos compartilhados, que pode gerar contenção, bloqueios e espera ativa. Desbalanceamento de carga também reduz eficiência, alguns núcleos podem ficar ociosos enquanto outros executam partes mais pesadas. Em sistemas com hardware limitado, como poucos núcleos ou baixa largura de banda de memória, o paralelismo pode até prejudicar o desempenho, tornando a versão paralela mais lenta do que a sequencial.

    Além do tempo observamos também o impacto na qualidade. Curiosamente aumentar o tamanho da frase não garantiu uma correlação maior com o gabarito semântico. Isso indica que adicionar mais palavras não necessariamente resolve a ambiguidade se o modelo estatístico não for capaz de entender a relação entre elas o que reforça a necessidade de modelos híbridos para consultas complexas.


    \section{Avaliação da Qualidade e Correlação Semântica}

    A qualidade dos resultados foi medida comparando a ordem de relevância gerada pelos nossos algoritmos estatísticos com a ordem sugerida pelo modelo de inteligência artificial Sentence-BERT que serviu como nosso gabarito semântico. Utilizamos o coeficiente de Spearman para quantificar essa similaridade de posicionamento.

    Os valores de correlação encontrados pelo coeficiente de Spearman foram consistentemente baixos e oscilaram próximos de zero na grande maioria dos testes. Isso é um achado científico importante pois confirma que a busca estatística e a busca semântica olham para características fundamentalmente diferentes do texto. Enquanto a busca estatística prioriza a presença exata de palavras raras e termos técnicos a busca semântica tenta entender o contexto geral e a intenção da frase. O fato de a correlação ser baixa indica que um método não substitui o outro mas sim que eles funcionam de formas complementares e capturam nuances distintas da relevância.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\linewidth]{assets/graficoSpearmsn}
        \caption{Estabilidade do coeficiente de Spearman em diferentes configurações}
        \label{fig:estabilidade_spearman}
    \end{figure}

    Uma observação curiosa foi o comportamento dos índices ao variarmos o tamanho dos saltos ou \textit{jumps} entre palavras. A hipótese inicial era que permitir saltos melhoraria a qualidade da busca ao encontrar termos que não estivessem imediatamente colados uns aos outros como por exemplo encontrar "crime de responsabilidade" ao buscar "crime responsabilidade". Contudo os dados mostraram que aumentar o tamanho dos saltos de zero para dois não alterou o coeficiente de Spearman de maneira significativa nos testes com Trigramas.

    Isso sugere que a linguagem jurídica é extremamente rígida e formal. Termos técnicos como "habeas corpus" ou "dano moral" quase sempre aparecem juntos e na mesma ordem exata. A ocorrência dessas palavras separadas por outros termos é tão rara que indexá-las com saltos apenas consumiu mais memória e processamento sem trazer nenhum ganho real de relevância para o usuário final. Essa descoberta permite otimizar futuros sistemas removendo a complexidade dos saltos sem medo de perder qualidade na busca.


    \section{Considerações Finais}

    Ao concluir este estudo podemos afirmar com segurança que o trabalho atingiu seu objetivo de avaliar de forma prática e quantitativa o equilíbrio entre custo e benefício na recuperação de informação jurídica. Os setenta e dois cenários testados desenham um caminho claro para a engenharia de software neste domínio específico.

    Ficou evidente que o algoritmo BM25 é a escolha superior para a busca estatística entregando uma performance de velocidade muito acima do TF-IDF graças à sua capacidade de lidar com a esparsidade dos dados. Além disso identificamos que o "ponto ótimo" de infraestrutura reside na utilização de Bigramas sem saltos. Essa configuração oferece um contexto lexical suficiente sem incorrer no custo proibitivo de memória dos Trigramas ou na complexidade desnecessária dos \textit{skip-grams}.

    Também concluímos que embora modelos de inteligência artificial ofereçam uma compreensão profunda do texto o seu custo computacional é elevado. A baixa correlação entre os resultados estatísticos e semânticos sugere que a arquitetura ideal para um sistema jurídico real deve ser híbrida. O sistema estatístico (BM25) pode atuar como um filtro primário de altíssima velocidade e baixo custo, entregando um subconjunto refinado de documentos para que o modelo semântico realize uma reordenação final apenas onde a precisão contextual for estritamente necessária.

    Por fim as limitações encontradas especialmente no desempenho do banco de dados em disco abrem portas para investigações futuras. A substituição da camada de persistência por tecnologias mais robustas e a ampliação do \textit{corpus} para incluir jurisprudência são os próximos passos naturais para evoluir esta pesquisa. Este trabalho deixa como legado uma base sólida de dados e uma implementação modular capaz de guiar o desenvolvimento de soluções de busca eficientes para o judiciário brasileiro.


    \section{Limitações e Trabalhos Futuros}

    Algumas limitações direcionam caminhos para pesquisas futuras:

    \begin{enumerate}
        \item O gabarito semântico utilizado foi gerado por modelos \textit{transformer}, o que limita a avaliação da “verdadeira” precisão semântica. Um gabarito manual poderia fornecer uma base mais confiável.
        \item O \textit{corpus} analisado é predominantemente legislativo. Ampliar o estudo para acórdãos e sentenças permitiria testar a robustez das técnicas sob estruturas textuais mais variadas.
    \end{enumerate}

    Em resumo, o trabalho demonstrou que algoritmos estatísticos atuais, em especial o BM25, oferecem excelente eficiência computacional e podem sustentar sistemas de busca jurídica de baixo custo. Ao mesmo tempo, a baixa concordância com modelos semânticos reforça a necessidade de abordagens híbridas que combinem velocidade com profundidade contextual, especialmente em um domínio tão exigente quanto o jurídico.


% ========================================================================
% BIBLIOGRAFIA
% ========================================================================
    \bibliography{bibliografia}


% % ========================================================================
% % APENDICES
% % ========================================================================
% \begin{apendicesenv}

% \partapendices

% \chapter{\label{AnexoA}Exemplo de seção de anexo}

% \begin{lstlisting}
% EXEMPLO DE CODIGO A SER ADICIONADO
% \end{lstlisting}

% \end{apendicesenv}

\end{document}
s