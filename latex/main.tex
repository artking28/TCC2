% ------------------------------------------------------------------------
% Senac Tex: Modelo de Trabalho Academico para o Centro Universitário
% Senac
% ------------------------------------------------------------------------

% ========================================================================
% CONFIGURAÇÃO DO DOCUMENTO
% ========================================================================


\documentclass[
% -- opções da classe memoir --
    12pt,                % tamanho da fonte
    openright,            % capítulos começam em pág ímpar (insere página vazia caso preciso)
    oneside,            % para impressão em verso e anverso. Oposto a oneside
    a4paper,            % tamanho do papel.
% -- opções da classe abntex2 --
%chapter=TITLE,		% títulos de capítulos convertidos em letras maiúsculas
%section=TITLE,		% títulos de seções convertidos em letras maiúsculas
%subsection=TITLE,	% títulos de subseções convertidos em letras maiúsculas
%subsubsection=TITLE,% títulos de subsubseções convertidos em letras maiúsculas
% -- opções do pacote babel --
    english,            % idioma adicional para hifenização
    brazil                % o último idioma é o principal do documento
]{abntex2}

% ---
% Pacotes básicos
% ---
\usepackage{lmodern}            % Usa a fonte Latin Modern
\usepackage[T1]{fontenc}        % Selecao de codigos de fonte.
\usepackage[utf8]{inputenc}        % Codificacao do documento (conversão automática dos acentos)
\usepackage{lastpage}            % Usado pela Ficha catalográfica
\usepackage{indentfirst}        % Indenta o primeiro parágrafo de cada seção.
\usepackage{color}                % Controle das cores
\usepackage{graphicx}            % Inclusão de gráficos
\usepackage{microtype}            % para melhorias de justificação
\usepackage{listings}
\usepackage[top=3cm, bottom=2cm, left=3cm, right=2cm]{geometry}
\usepackage{amsmath}
\usepackage{float}

% ---
% Pacotes de citações
% ---
\usepackage[brazilian,hyperpageref]{backref}     % Paginas com as citações na bibl
\usepackage[alf]{abntex2cite}    % Citações padrão ABNT

% CONFIGURAÇÕES DE PACOTES

% Configurações do pacote backref
% Usado sem a opção hyperpageref de backref
\renewcommand{\backrefpagesname}{Citado na(s) página(s):~}
% Texto padrão antes do número das páginas
\renewcommand{\backref}{}
% Define os textos da citação
\renewcommand*{\backrefalt}[4]{
    \ifcase
        #1 %
        Nenhuma citação no texto.%
        \or
        Citado na página #2.%
    \else
        Citado #1 vezes nas páginas #2.%
    \fi}%

% Informações de dados para CAPA e FOLHA DE ROSTO
\titulo{Avaliação de métodos estatísticos na busca de correspondência textual jurídica frente a \textit{embeddings}}
\autor{Arthur Andrade e Davi Henrique}
\local{São Paulo - Brasil}
\data{2025}
\orientador{Afonso Lelis}
%\coorientador{Nome do Coorientador}
\instituicao{
    Centro Universitário Senac - Santo Amaro
    \par
    Bacharelado em Ciência da Computação
}
\tipotrabalho{Trabalho de Conclusão de Curso}
% O preambulo deve conter o tipo do trabalho, o objetivo,
% o nome da instituição e a área de concentração
\preambulo{Monografia apresentada na disciplina Trabalho de Conclusão de Curso, como parte dos requisitos para obtenção do título de Bacharel em Ciência da Computação.}

% Configurações de aparência do PDF final

% alterando o aspecto da cor azul
\definecolor{blue}{RGB}{41,5,195}

% informações do PDF
\makeatletter
\hypersetup{
%pagebackref=true,
    pdftitle={\@title},
    pdfauthor={\@author},
    pdfsubject={\imprimirpreambulo},
    pdfcreator={LaTeX with abnTeX2},
    pdfkeywords={abnt}{latex}{abntex}{abntex2}{trabalho acadêmico},
    colorlinks=true,            % false: boxed links; true: colored links
    linkcolor=black,            % color of internal links
    citecolor=black,                % color of links to bibliography
    filecolor=magenta,            % color of file links
    urlcolor=black,
    bookmarksdepth=4
}
\makeatother

% Espaçamentos entre linhas e parágrafos

% O tamanho do parágrafo é dado por:
\setlength{\parindent}{1.25cm}

% Controle do espaçamento entre um parágrafo e outro:
\setlength{\parskip}{0.2cm}

\SingleSpacing
\makeatletter
\let\@fnsymbol\@arabic
\makeatother

% compila o indice
\makeindex

\begin{document}

% Retira espaço extra obsoleto entre as frases.
    \frenchspacing

% ========================================================================
% CAPA
% ========================================================================
    \imprimircapa

% ========================================================================
% FOLHA DE ROSTO
% ========================================================================
    \imprimirfolhaderosto

% ========================================================================
% DEDICATÓRIA
% ========================================================================
    \begin{dedicatoria}
        \vspace*{\fill}
        \centering
        \noindent
        \textit{ Texto da dedicatória.} \vspace*{\fill}
    \end{dedicatoria}

% ========================================================================
% AGRADECIMENTOS
% ========================================================================
    \begin{agradecimentos}
        Texto de agradecimento.

    \end{agradecimentos}

% ========================================================================
% EPÍGRAFE
% ========================================================================
    \begin{epigrafe}
        \vspace*{\fill}
        \begin{flushright}
            \textit{``A beleza é realmente um bom dom de Deus; mas que os bons não pensem que ela é um grande bem, pois Deus a distribui mesmo para os maus. \\
            (Santo Agostinho)}
        \end{flushright}
    \end{epigrafe}

% ========================================================================
% RESUMO
% ========================================================================
    \setlength{\absparsep}{18pt} % ajusta o espaçamento dos parágrafos do resumo
    \begin{resumo}

        Texto do resumo

        \textbf{Palavras-chaves}: palavra-chave 1, palavra-chave 2, palavra-chave 3.
    \end{resumo}

% ========================================================================
% ABSTRACT
% ========================================================================
    \begin{resumo}[Abstract]
        \begin{otherlanguage*}{english}
            Abstract text in english

            \textbf{Key-words}: keyword 1, keyword 2, keyword 3
        \end{otherlanguage*}
    \end{resumo}

% ========================================================================
% LISTA DE ILISTRAÇÕES
% ========================================================================
    \pdfbookmark[0]{\listfigurename}{lof}
    \listoffigures*
    \cleardoublepage

% ========================================================================
% LISTA DE TABELAS
% ========================================================================
% \pdfbookmark[0]{\listtablename}{lot}
% \listoftables*
% \cleardoublepage

% ========================================================================
% LISTA DE ABREVIATURAS E SIGLAS
% ========================================================================
    \begin{siglas}
        \item[API] Application Programming Interface
        \item[BERT] Bidirectional Encoder Representations from Transformers
        \item[BMSS] Boyer-Moore String Search
        \item[KMP] Knuth Morris Pratt
        \item[CBOW] Continuous Bag-of-Words
        \item[CNJ] Conselho Nacional de Justiça
        \item[CPU] Computing Processing Unit
        \item[GPU] Graphics Processing Unit
        \item[CRF] Conditional Random Field
        \item[ELMo] Embeddings from Language Model
        \item[GO] Golang Programming Language
        \item[GloVe] Global Vectors for Word Representation
        \item[HTTP] Hypertext Transfer Protocol
        \item[IDF] Inverse Document Frequency
        \item[JSON] JavaScript Object Notation
        \item[NLP] Natural Language Processing
        \item[NER] Reconhecimento de Entidade Nomeada
        \item[POS] Part-of-Speech
        \item[PJe] Processo Judicial Eletrônico
        \item[SVM] Support Vector Machine
        \item[RAG] Retrieval-Augmented Generation
        \item[REST] Representational State Transfer
        \item[TF-IDF] Term Frequency-Inverse Document Frequency
    \end{siglas}

% ========================================================================
% SUMÁRIO
% ========================================================================
    \pdfbookmark[0]{\contentsname}{toc}
    \tableofcontents*
    \cleardoublepage

    \textual
% ========================================================================
% INTRODUÇÃO
% ========================================================================


    \chapter{Introdução}


    \section{Contexto}

    A era digital transformou radicalmente o acesso à informação, gerando um volume de dados sem precedentes em praticamente todos os setores da sociedade. No campo do Direito, essa realidade se manifesta na digitalização massiva de processos, leis, jurisprudências e pareceres. O Judiciário brasileiro, por exemplo, recebeu mais de 250 milhões de processos em formato eletrônico nos últimos 15 anos \cite{citacao59}. Se por um lado essa digitalização representa um avanço em termos de transparência, por outro, impõe um desafio monumental: como navegar e extrair informações relevantes de um oceano de textos complexos e interconectados? A simples capacidade de armazenamento tornou-se insuficiente; a necessidade agora é de processamento inteligente.

    Nesse cenário, as técnicas de Processamento de Linguagem Natural (NLP), especialmente aquelas baseadas em Inteligência Artificial e aprendizado de máquina, surgiram como a solução mais promissora. Modelos de linguagem avançados, capazes de compreender nuances semânticas e contextuais, prometem revolucionar a busca jurídica. Ferramentas que utilizam \textit{embeddings} e arquiteturas de \textit{transformers} representam o estado da arte, buscando emular uma compreensão quase humana dos textos legais para tarefas como a identificação de divergências jurisprudenciais \cite{citacao60}.

    Contudo, a adoção dessas tecnologias levanta uma questão fundamental e muitas vezes negligenciada: qual o custo-benefício real dessa complexidade? O esforço computacional, financeiro e técnico para treinar, implementar e manter sistemas de IA é significativo, exigindo a aplicação de métodos para otimizar e reduzir os custos de inferência dos modelos \cite{citacao61}. Isso nos leva ao cerne deste trabalho: investigar até que ponto a sofisticação dos modelos de \textit{machine learning} é indispensável para a tarefa de busca em documentos jurídicos. Será que métodos estatísticos e algoritmos de busca mais clássicos, quando bem aplicados, não poderiam alcançar resultados comparáveis, com uma fração do custo?

    Este estudo se propõe a explorar essa fronteira. Conforme aponta \citeonline{citacao62}, modelos distribucionais como BERT não apresentam, necessariamente, um desempenho superior em todas as tarefas de classificação de textos jurídicos quando comparados a abordagens mais simples. O objetivo deste trabalho, portanto, não é desenvolver uma nova ferramenta, mas sim realizar uma análise crítica e comparativa, buscando compreender o verdadeiro \textit{trade-off} entre a precisão semântica e a eficiência computacional no domínio específico do Direito.


    \section{Justificativa}

    A busca incessante por precisão levou o campo da tecnologia jurídica a uma crescente dependência de modelos de Inteligência Artificial, cujo uso no Judiciário brasileiro cresceu 26\% apenas entre 2022 e 2023 \cite{citacao63}. A premissa é clara: quanto mais "inteligente" o modelo, melhores os resultados. No entanto, essa corrida pela sofisticação muitas vezes ignora os custos práticos associados. O treinamento de grandes modelos de linguagem exige poder de processamento massivo, geralmente dependente de hardware especializado como GPUs, o que se traduz em altos custos de infraestrutura, energia e manutenção. Para muitas instituições, essa barreira pode ser intransponível.

    Paralelamente, a evolução do hardware, com o advento das arquiteturas \textit{multicore}, não beneficiou apenas os modelos de IA. Algoritmos clássicos, antes limitados por execuções sequenciais, podem hoje ter seu desempenho drasticamente otimizado por meio de paralelismo, explorando múltiplos núcleos de processamento para aumentar a performance das aplicações \cite{citacao64}. Técnicas estatísticas como o TF-IDF e algoritmos de busca de padrões, como o Boyer-Moore, são computacionalmente mais leves e mais simples de implementar, mas seu potencial em hardware moderno é frequentemente subestimado.

    A justificativa deste trabalho reside, portanto, em uma necessidade de pragmatismo. É preciso questionar se o ganho marginal de relevância oferecido por um modelo semântico complexo justifica sua implementação em todos os cenários. Em domínios com linguagem controlada e previsível, como é o caso dos documentos jurídicos, abordagens mais simples, baseadas em regras, podem se mostrar altamente eficazes \cite{citacao65}. Em muitas tarefas cotidianas, a velocidade de resposta e o baixo custo operacional podem ser mais valiosos do que a capacidade de interpretar uma ambiguidade sutil em um texto.

    Esta pesquisa oferece uma análise quantitativa desse \textit{trade-off}. Ao comparar diretamente o desempenho, o tempo de resposta e o uso de recursos de diferentes abordagens — uma prática já adotada em trabalhos que avaliam modelos como BERT e ChatGPT por meio de métricas de acurácia, precisão e F1-score \cite{citacao66} — pretendemos fornecer dados concretos que auxiliem na tomada de decisão sobre qual tecnologia é mais adequada para diferentes necessidades do ecossistema jurídico, movendo o debate do campo teórico para uma avaliação de aplicabilidade prática.


    \section{Objetivos}

    \subsection{Objetivo Geral}

    Avaliar e comparar o desempenho de métodos estatísticos e semânticos para a indexação e busca de informações em documentos jurídicos, analisando o \textit{trade-off} entre a complexidade computacional dos modelos e a relevância dos resultados obtidos.

    \subsection{Objetivos Específicos}

    \begin{enumerate}
        \item Realizar o \textit{scrapping} de documentos jurídicos afim de construir uma forte e extensa base de documentos.
        \item Pré-processar um corpus de documentos, incluindo limpeza textual e outras técnicas pertinentes, para criar uma base de testes consistente para ambas as abordagens.
        \item Implementar um pipeline de busca baseado em \textit{embeddings} de sentenças, utilizando a similaridade de cosseno para ranquear os resultados de acordo com a proximidade semântica.
        \item Implementar um pipeline de busca baseado no modelo estatístico TF-IDF e BM25 para criar uma representação vetorial baseada na frequência de termos e ranqueá-los através da similaridade de cosseno.
        \item Definir e aplicar um conjunto de métricas para avaliar e comparar o desempenho das diferentes abordagens, considerando precisão, tempo de resposta.
        \item Analisar criticamente os resultados obtidos para identificar as vantagens, desvantagens e os cenários de aplicação mais adequados para cada método no contexto específico dos textos jurídicos.
    \end{enumerate}


    \chapter{Revisão bibliográfica}


    \section{Processamento de Linguagem Natural no Domínio Jurídico}

    A crescente produção textual na sociedade contemporânea transformou a informação em um recurso abundante, porém desafiador de ser processado. No universo jurídico, esse fenômeno é ainda mais acentuado: contratos, decisões, pareceres, petições e legislações são produtos essencialmente linguísticos. O Direito, enquanto sistema normativo, é construído, operado e interpretado por meio da linguagem \cite{citacao42, citacao43}.

    Diante do volume exponencial de documentos jurídicos, o uso de ferramentas computacionais tornou-se uma necessidade. As limitações cognitivas humanas frente à grande quantidade de dados disponíveis exigem soluções capazes de automatizar parte do processamento textual. Técnicas de NLP surgem como abordagem consolidada para enfrentar esse desafio \cite{citacao17, citacao44}.

    O NLP, subárea da inteligência artificial, permite que máquinas compreendam, processem e operem sobre dados em linguagem natural. No contexto jurídico, essas técnicas viabilizam a identificação de padrões, a extração de informações relevantes, a sumarização de documentos e o suporte à tomada de decisão. Trata-se de uma evolução que não apenas automatiza tarefas, mas incorpora mecanismos capazes de operar diretamente sobre a base textual do Direito \cite{citacao45, citacao43}.

    Atualmente, modelos computacionais já demonstram aplicações práticas no ambiente jurídico, como análise de jurisprudência, redação assistida e busca semântica em bases normativas. No contexto brasileiro, estudos como os de \citeonline{citacao17} evidenciam a consolidação dessas tecnologias no setor jurídico.

    \subsection{Características dos textos jurídicos}

    A aplicação de técnicas de NLP no domínio jurídico envolve desafios particulares, uma vez que os textos jurídicos apresentam propriedades distintas em relação a outros gêneros textuais. Em primeiro lugar, são documentos altamente especializados, redigidos com vocabulário técnico e linguagem formal, frequentemente estruturados de modo padronizado. Termos como \textit{vossa excelência}, \textit{ex positis} ou \textit{ad nutum} não apenas indicam jargão, mas também desempenham papel semântico e pragmático central na construção do sentido legal de um texto \cite{citacao42,citacao44}.

    Além disso, esses textos apresentam um alto grau de ambiguidade controlada --- ou seja, ambiguidade tolerada e, muitas vezes, intencional. Palavras como \textit{poderá}, \textit{deverá} ou \textit{caberá} têm significados normativos específicos que impactam diretamente a interpretação e aplicação das normas. Essa polissemia exige que modelos de NLP sejam sensíveis ao contexto jurídico para evitar erros semânticos que possam comprometer a análise automatizada \cite{citacao17,citacao45}.

    Outro aspecto relevante é a intertextualidade jurídica. Leis, contratos, petições e sentenças costumam referenciar outros documentos legais, criando dependências explícitas entre normas, jurisprudências e atos administrativos. Essa característica impõe a necessidade de modelos capazes de capturar relações entre documentos --- como citações, remissões e precedentes --- o que exige abordagens sofisticadas para modelagem de contexto e representação semântica \cite{citacao17}.

    Também é importante considerar a estrutura argumentativa dos textos jurídicos. Documentos como sentenças e pareceres possuem uma lógica discursiva específica, geralmente organizada em premissas, fundamentações e conclusões. Essa organização influencia tanto a segmentação quanto a priorização de informações nos sistemas de NLP aplicados ao Direito \cite{citacao43}.

    \subsection{Desafios específicos em português}

    Ao aplicar técnicas de NLP à língua portuguesa, especialmente no domínio jurídico, surgem obstáculos adicionais que não estão presentes em línguas com estruturas morfossintáticas mais simples, como o inglês. O português brasileiro apresenta uma morfologia altamente flexiva, com ampla variação de formas verbais, nominais e pronominais, o que torna tarefas como tokenização e lematização consideravelmente mais complexas \cite{citacao17}.

    No contexto jurídico, essa complexidade é intensificada pelo uso recorrente de locuções e expressões idiomáticas específicas da tradição normativa. Frases como \textit{sem prejuízo do disposto}, \textit{salvo disposição em contrário} ou \textit{nos termos da legislação aplicável} não podem ser analisadas isoladamente por palavra. Tais construções funcionam como unidades semânticas coesas, exigindo abordagens capazes de identificar e preservar o significado completo durante o pré-processamento textual \cite{citacao44,citacao17}.

    \subsection{Modelos de Linguagem para o Domínio Jurídico}

    Embora bibliotecas de NLP de propósito geral, como \textit{spaCy} ou \textit{NLTK}, ofereçam suporte para a língua portuguesa, seus modelos genéricos, treinados em textos da web ou literatura, falham em capturar as nuances terminológicas e o contexto semântico específico do universo jurídico. A palavra "competência", por exemplo, tem um significado técnico e distinto no Direito (atribuição de um órgão para julgar) que um modelo genérico não consegue priorizar.

    Para suprir essa lacuna, a comunidade acadêmica tem se concentrado no desenvolvimento de modelos de linguagem especializados. Um trabalho central nesse contexto para o português brasileiro é o \citeonline{citacao17} (LegalNLP). Os autores demonstram que modelos como Word2Vec e BERT, quando pré-treinados ou ajustados (\textit{fine-tuning}) em um vasto e específico corpus de documentos jurídicos nacionais, superam significativamente o desempenho de modelos genéricos em tarefas-chave de NLP.

    Essa abordagem de especialização é uma tendência global. O trabalho de \citeonline{citacao39}, por exemplo, introduziu o LEGAL-BERT, um modelo da família BERT treinado inteiramente em textos legais. A pesquisa comprovou que essa especialização de domínio permite uma captação muito mais precisa do vocabulário técnico e das estruturas discursivas do Direito do que o modelo BERT original.

    Portanto, a literatura estabelece que o estado da arte para a busca semântica de alta precisão no domínio jurídico envolve o uso de modelos de linguagem contextuais, pesados e de domínio específico. É justamente essa abordagem, que representa o polo de maior complexidade e custo computacional, que este trabalho utilizará como referência para a comparação com os métodos estatísticos tradicionais.


    \section{Técnicas de Pré-processamento Textual}

    Em um cenário de grande volume textual, como o jurídico, a extração de informação relevante demanda um tratamento prévio sobre os documentos. Essa etapa é conhecida como pré-processamento, e tem como objetivo transformar textos brutos em representações mais estruturadas e significativas, facilitando tanto a análise semântica quanto a indexação e recuperação posterior. Trata-se, portanto, de uma fase indispensável para qualquer sistema de recuperação de informação textual, sobretudo em domínios especializados, já que a escolha adequada das técnicas de pré-processamento pode influenciar significativamente a relevância das informações extraídas \cite{citacao16}.

    Além de normalizar o conteúdo textual, o pré-processamento também é responsável pela definição do vocabulário de termos que será utilizado na indexação e na construção de representações computacionais. Um bom pré-processamento reduz o tamanho do vocabulário, melhora o desempenho e aumenta a precisão das buscas \cite{citacao67}. Essa etapa funciona, portanto, como uma ponte entre a linguagem natural e os algoritmos, permitindo que informações relevantes sejam extraídas de forma estruturada, eficiente e precisa. No contexto deste trabalho, ela se mostra essencial para a identificação de elementos fundamentais nos documentos jurídicos, como nomes das partes, dispositivos legais e decisões.

    \subsection{Tokenização}

    No contexto do processamento de linguagem natural, um \textit{token} é definido como uma unidade básica de texto, normalmente correspondente a uma palavra, número, pontuação ou símbolo significativo. A forma como os \textit{tokens} são extraídos influencia diretamente a eficácia dos modelos computacionais, especialmente em contextos especializados como o jurídico, onde há presença de construções formais e terminologias específicas.

    O processo de tokenização refere-se, portanto, à separação do texto contínuo em unidades menores, os \textit{tokens}, que geralmente representam palavras. Essa tarefa vai além de simplesmente dividir o texto por espaços; por exemplo, expressões como “São Paulo” ou “Art. 5º” exigem regras específicas para não serem segmentadas incorretamente, especialmente em textos técnicos como os jurídicos \cite{citacao67}.

    \begin{figure}[H]
        \centering
        \caption{\label{fig:tokenizacao}Exemplo de \textit{tokenização}.}
        \includegraphics[width=0.8\textwidth]{assets/tokenizacao.png}
        \legend {Fonte: \cite{citacao10}}
    \end{figure}

    \subsubsection{Desafios específicos no contexto jurídico em português}

    A tokenização de textos jurídicos em língua portuguesa apresenta desafios particulares que precisam ser considerados para evitar a perda de informação ou a distorção semântica. Um dos principais aspectos envolve o tratamento da pontuação. Embora seja comum a remoção de sinais de pontuação na maioria dos sistemas de pré-processamento textual, certos elementos, como os números de processo — por exemplo, 1234567-89.2023.8.26.0100 — devem ser preservados como um único \textit{token}, dada sua importância semântica e recorrência em documentos legais \cite{citacao17}.

    Outro desafio é o manejo de contrações e apóstrofos. Expressões como \textit{d’água} exigem normalização ou adaptação conforme a estratégia do sistema de NLP adotado, a fim de evitar a fragmentação de termos relevantes ou a exclusão de significados culturalmente marcados. Esse tipo de problema é particularmente sensível em português, dada a frequência de contrações e formas elípticas presentes na linguagem formal e jurídica \cite{citacao17}.

    A presença de hífen também demanda atenção, especialmente em palavras compostas. Termos como \textit{auto-aplicável} podem ser mantidos como uma única unidade ou separados, dependendo do contexto e do domínio de aplicação. Em geral, recomenda-se a separação de prefixos curtos em termos técnicos, mas a preservação do hífen em nomes próprios, como em \textit{Hewlett-Packard}. Esse problema é bem documentado tanto na literatura geral de processamento de linguagem quanto nos trabalhos específicos aplicados ao domínio jurídico brasileiro, que destacam como a segmentação incorreta impacta diretamente na qualidade da indexação e da recuperação de informação \cite{citacao17}.

    Além disso, há a necessidade de identificar e manter unidas expressões fixas e nomes compostos, tais como \textit{Ministério Público}, \textit{Poder Judiciário} ou \textit{São Paulo}. Essas unidades semânticas são especialmente relevantes quando se lida com consultas baseadas em frases, nas quais a separação pode comprometer a precisão dos resultados. Modelos jurídicos treinados para o português, como os apresentados no projeto NLP Legal Brasil, reforçam a necessidade desse tipo de pré-processamento especializado, visto que expressões compostas carregam um valor semântico muito superior ao de seus termos isolados \cite{citacao17}.

    Por fim, entidades estruturadas como endereços de e-mail, URLs, datas e identificadores numéricos — incluindo CPF e CNPJ — devem ser reconhecidas e tratadas como \textit{tokens} indivisíveis. Esses elementos são frequentemente objeto direto de busca em sistemas jurídicos informatizados, e sua fragmentação pode prejudicar seriamente a eficácia dos mecanismos de recuperação de informação. A literatura destaca que a preservação de entidades estruturadas é uma das práticas essenciais para garantir a robustez de sistemas de busca no domínio jurídico, especialmente em bases documentais complexas como acervos de jurisprudência ou diários oficiais \cite{citacao17}.

    \subsection{Remoção de Stopwords}

    A remoção de \textit{stopwords} é uma das etapas principais do pré-processamento textual e consiste na exclusão de palavras que ocorrem com alta frequência, mas que carregam pouco ou nenhum valor semântico individual — como \textit{de}, \textit{que}, \textit{em}, \textit{o} e \textit{a}. Esse tipo de filtragem é comumente utilizado para reduzir o vocabulário e acelerar os algoritmos, sem perda significativa de informação relevante para a maioria das tarefas de recuperação e classificação textual. \cite{citacao67}

    Apesar de a remoção de \textit{stopwords} ser uma prática comum em tarefas de processamento de linguagem natural, no contexto jurídico é necessário cautela. Isso porque palavras funcionais — geralmente ignoradas em outros domínios — podem conter informações fundamentais para a interpretação de dispositivos legais, como apontado por \citeauthor{citacao17} (\citeyear{citacao17}), que optaram por não removê-las durante o treinamento de seus modelos linguísticos, justamente para preservar a integridade semântica dos textos judiciais.

    \begin{figure}[H]
        \centering
        \caption{Exemplo de remoção de \textit{stopwords}}
        \includegraphics[width=0.8\textwidth]{assets/Stopwords.png}
        \legend{Fonte: \cite{citacao25}}
    \end{figure}

    \subsection{Lematização e Stemização}

    A lematização reduz palavras à sua forma canônica, conhecida como \textit{lema}. Por exemplo, termos como \textit{decidir}, \textit{decisão} e \textit{decidido} são todos associados ao verbo \textit{decidir}. Esse processo considera a análise morfológica e o contexto gramatical, sendo essencial para manter o sentido das palavras nos textos jurídicos, onde pequenas variações morfológicas podem alterar significativamente o significado \cite{citacao10}.

    Por outro lado, a stemização é uma técnica mais simples que remove afixos (prefixos e sufixos) para obter uma raiz. No entanto, essa raiz nem sempre corresponde a uma palavra válida no idioma. Por exemplo, as palavras \textit{decidir}, \textit{decidindo} e \textit{decidido} podem ser reduzidas a \textit{decid}.

    \begin{figure}[H]
        \centering
        \caption{Exemplo de diferenças entre Stemização e Lematização.}
        \includegraphics[width=0.7\textwidth]{assets/lem_stem.png}
        \legend{Fonte: \cite{citacao18}}
    \end{figure}

    A lematização proporciona maior fidelidade semântica, sendo mais indicada para tarefas que exigem compreensão precisa dos textos, como análise e busca em documentos jurídicos. Essa abordagem, por depender de análise linguística completa e regras morfológicas, costuma ter maior custo computacional e necessidade de recursos linguísticos bem estruturados, especialmente em idiomas como o português, que possui alta complexidade morfológica \cite{citacao17}.

    Por outro lado, a stemização é uma solução mais simples e eficiente em termos computacionais, baseada em regras heurísticas para remoção de afixos. Embora menos precisa, pode ser útil em tarefas onde a sensibilidade semântica não é tão crítica \cite{citacao18}.

    No contexto jurídico brasileiro, a adoção de lematização se mostra mais vantajosa, uma vez que ferramentas linguísticas genéricas não são suficientes para capturar as nuances da linguagem jurídica em português \cite{citacao17}.

    \subsection{Segmentação de Sentenças}

    A segmentação de sentenças consiste em dividir um texto contínuo em unidades menores, geralmente delimitadas por frases. Esse processo é essencial para permitir que modelos de processamento de linguagem operem com unidades textuais semanticamente completas, sobretudo em tarefas que requerem maior nível de precisão, como extração de fundamentos jurídicos, sumarização automática, análise de jurisprudências ou geração de relatórios.

    Embora seja considerado um processo relativamente simples no processamento de linguagem natural, a segmentação de sentenças apresenta desafios em textos jurídicos. A alta frequência de abreviações, siglas e expressões formais, como “Art.”, “Inc.”, “V. Exa.” ou “D.J.E.”, gera ambiguidades na definição dos pontos que efetivamente representam o final de uma sentença. Pontuações como ponto final, dois-pontos e até ponto e vírgula podem assumir funções distintas dependendo do contexto \cite{citacao17}.


    \section{Representação de Documentos}

    \subsection{TF-IDF}

    Após o pré-processamento, uma das técnicas mais consolidadas para representar documentos como vetores numéricos é o TF-IDF (\textit{Term Frequency--Inverse Document Frequency}). Esse método associa a cada termo um peso que reflete tanto sua importância local — baseada na frequência com que aparece no documento — quanto sua importância global — determinada pela raridade do termo na coleção como um todo \cite{citacao10, citacao67}.

    O cálculo do TF--IDF é composto por duas etapas fundamentais. A primeira consiste na determinação da frequência de termo \textit{Term Frequency} (TF), que quantifica o número de vezes que um termo \(t\) aparece em um documento \(d\), representado por \(\mathit{tf}_{t,d}\). Para evitar que termos muito frequentes dominem a representação, especialmente em documentos longos, é comum aplicar uma normalização logarítmica, definida como:
    \[
        \mathit{tf}_{t,d} =
        \begin{cases}
            1 + \log(\mathit{tf}_{t,d}), & \text{se } \mathit{tf}_{t,d} > 0, \\
            0, & \text{caso contrário}.
        \end{cases}
    \]
    Essa transformação suaviza o impacto de termos com alta contagem, tornando a representação mais robusta e comparável entre documentos de diferentes tamanhos \cite{citacao67}.

    A segunda etapa envolve o cálculo da \textit{Inverse Document Frequency} (IDF), que mede a capacidade de um termo discriminar documentos. Dado \(N\) como o número total de documentos na coleção e \(df_t\) como o número de documentos em que o termo \(t\) aparece, a IDF é calculada como:
    \[
        \mathit{idf}_t = \log \left( \frac{N}{df_t} \right)
    \]
    Esse fator tem como objetivo reduzir o peso de termos comuns na coleção, como artigos, pronomes ou palavras genéricas, e aumentar o peso de termos mais raros, que são mais úteis para diferenciar documentos \cite{citacao67}.

    Assim, o peso final de um termo em um documento, chamado de TF--IDF, é dado pela multiplicação dos dois componentes:
    \[
        \mathrm{tf\text{-}idf}(t,d) = \mathit{tf}_{t,d} \times \mathit{idf}_t
    \]
    Esse valor será mais alto para termos que ocorrem frequentemente em um documento específico, mas que são raros na coleção como um todo, refletindo sua importância tanto local quanto global \cite{citacao67}.

    A representação de documentos no modelo vetorial é então construída a partir dos pesos TF--IDF de cada termo. Formalmente, um documento é representado como um vetor \(\mathbf{v}(d) = [\mathrm{tf\text{-}idf}(t_1,d), \dots, \mathrm{tf\text{-}idf}(t_M,d)]\), onde \(M\) é o número total de termos únicos na coleção. Consultas podem ser representadas de maneira análoga, permitindo o uso de métricas como a similaridade do cosseno para calcular a proximidade entre consultas e documentos no espaço vetorial \cite{citacao67}.

    \begin{figure}[H]
        \centering
        \caption{Exemplo da aplicação do TF-IDF em frases.}
        \includegraphics[width=\textwidth]{assets/TF_IDF.png}
        \legend{Fonte: \cite{citacao41}}
    \end{figure}

    O modelo TF--IDF é amplamente utilizado em tarefas de recuperação de informação, classificação e agrupamento de textos, justamente por sua simplicidade e eficácia. Seu principal mérito reside na combinação da frequência local com a raridade global dos termos, permitindo destacar palavras que são relevantes para o conteúdo específico de um documento, enquanto penaliza termos genéricos e pouco informativos \cite{citacao10}.

    \subsection{BM25}

    Enquanto o TF-IDF estabeleceu um modelo fundamental para a recuperação de informação, ele possui limitações intrínsecas, como a tendência de favorecer desproporcionalmente termos de alta frequência e a falta de uma normalização robusta para o tamanho dos documentos. Para superar essas questões, foi desenvolvido o algoritmo Okapi BM25 (doravante chamado apenas de BM25), um modelo de ranqueamento probabilístico que se tornou um padrão de fato em sistemas de busca modernos devido à sua eficácia e robustez \cite{citacao67}.

    O BM25 não trata a frequência de um termo (\textit{term frequency}) de forma linear. Em vez disso, ele introduz um componente de saturação, partindo da premissa de que a relevância de um termo para um documento não cresce infinitamente com sua frequência. A primeira vez que uma palavra aparece é muito significativa; a décima, um pouco menos; e a centésima vez adiciona uma relevância marginal muito pequena. O algoritmo modela esse comportamento para evitar que documentos longos e repetitivos dominem os resultados da busca.

    A pontuação de relevância de um documento \textit{D} para uma consulta \textit{Q}, que contém os termos \(q_1, q_2, \dots, q_n\), é calculada pela seguinte fórmula:

    \[
        \text{score}(D, Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
    \]

    Para entender seu funcionamento, podemos decompor a fórmula em suas três partes principais. O primeiro componente é o \textit{Inverse Document Frequency} (IDF), que, assim como no TF-IDF, mede a raridade de um termo na coleção de documentos. O BM25 utiliza uma variante da fórmula padrão de IDF, geralmente com um ajuste para evitar valores nulos ou negativos para termos que aparecem em mais da metade dos documentos \cite{citacao67}. A lógica central, no entanto, permanece a mesma: termos que são raros em toda a coleção recebem um peso maior, pois são considerados mais discriminativos.

    O segundo e mais inovador componente da fórmula lida com a \textit{saturação da frequência do termo}. Diferente do TF-IDF, o BM25 parte do princípio de que a relevância de um termo não cresce infinitamente com sua frequência. A primeira ocorrência é muito significativa, mas cada aparição subsequente contribui progressivamente menos para a pontuação final. Esse efeito de saturação é controlado pelo parâmetro de calibração \textit{k1} (geralmente entre 1.2 e 2.0), que define quão rapidamente a pontuação de um termo se estabiliza. Um valor de \textit{k1} baixo faz com que a relevância sature rapidamente, enquanto um valor mais alto permite que a frequência do termo continue a ter um impacto maior na pontuação \cite{citacao67}.

    Por fim, o BM25 introduz uma \textit{normalização pelo comprimento do documento} muito mais sofisticada. O algoritmo compara o tamanho do documento atual, \(|D|\), com o tamanho médio de todos os documentos na coleção, \textit{avgdl}. O parâmetro \textit{b} (geralmente em torno de 0.75) controla o grau de influência que o tamanho do documento tem na pontuação final. Quando \textit{b} é 1, o efeito da normalização é máximo; quando é 0, o tamanho do documento é completamente ignorado. Essa abordagem permite penalizar documentos que são muito mais longos que a média, pois eles têm uma probabilidade estatisticamente maior de conter os termos da busca por acaso, sem serem necessariamente mais relevantes \cite{citacao67}.

    Em síntese, o BM25 aprimora os conceitos do modelo vetorial ao incorporar uma visão probabilística e heurísticas mais refinadas sobre como a frequência de termos e o tamanho dos documentos influenciam a relevância. Sua capacidade de ser calibrado pelos parâmetros \textit{k1} e \textit{b} o torna altamente adaptável a diferentes tipos de coleções de texto. Essa relevância é comprovada por sua aplicação prática em diversos domínios, incluindo estudos recentes no contexto jurídico brasileiro para a identificação de similaridade em documentos oficiais \cite{citacao68}.

    \subsection{Índice inverso}

    O cálculo eficiente de medidas como o TF-IDF e o BM25, bem como a execução de buscas rápidas em grandes volumes de texto, dependem de uma estrutura de dados mais sofisticada do que a simples lista de documentos. A cada nova consulta ou para a construção dos próprios modelos estatísticos, seria computacionalmente inviável reanalisar todo o corpus para encontrar termos, suas frequências (\textit{term frequency}) e em quantos documentos eles aparecem (\textit{document frequency}). Para resolver esse problema, utiliza-se o \textbf{índice inverso} (ou \textit{inverted index}), que funciona como a principal estrutura de indexação para sistemas de recuperação de informação \cite{citacao67}.

    O índice inverso é uma estrutura de dados amplamente utilizada em sistemas de recuperação de informação, projetada para otimizar a busca de termos em grandes coleções de documentos \cite{citacao67, citacao69}. Diferentemente de uma abordagem direta, que examinaria cada documento sequencialmente (uma operação de complexidade \(O(N)\), onde N é o tamanho do corpus), o índice invertido organiza os termos do corpus de forma a mapear cada palavra ou unidade lexical às suas respectivas ocorrências. Esse mapeamento inclui informações como a identificação dos documentos e, comumente, a posição ou frequência de cada termo. Essa organização permite consultas eficientes e rápidas, fundamentais para o funcionamento de sistemas como motores de busca \cite{citacao67}. Segue uma imagem que exemplifica o índice inverso.

    \begin{figure}[H]
        \centering
        \caption{\label{fig:Indice} No índice inverso quem mapeia são os termos e n.}
        \includegraphics[width=\textwidth]{assets/Indice.png}
        \legend {Fonte: Elaboração própria}
    \end{figure}

    Uma forma de otimizar o uso de memória do índice inverso é por meio da compressão das listas de ocorrências (\textit{postings lists}). Técnicas comuns incluem a codificação de diferenças (\textit{gap encoding} ou \textit{delta encoding}), onde apenas a diferença entre IDs de documentos consecutivos é armazenada, e codificações de inteiros como \textit{varint} (\textit{Variable Byte coding}) ou \textit{Golomb coding}, que reduzem o espaço ocupado por números pequenos. Além de economizar memória, a compressão pode acelerar consultas, já que menos dados precisam ser carregados da memória ou disco, sendo uma prática padrão em motores de busca de larga escala \cite{citacao67}.

    O índice inverso oferece consultas com complexidade \(O(\log n)\) para a busca de termos no dicionário da estrutura, onde \(n\) é o número de termos únicos, caso o dicionário seja implementado com árvores balanceadas; ou \(O(1)\) em média, se forem usadas tabelas hash \cite{citacao67}. Isso se difere drasticamente da complexidade \(O(N)\) de uma varredura sequencial. Atualizações, como a inserção de novos documentos, têm um custo associado ao processamento de seus termos. No projeto, usamos uma tabelas hash para mapear termos a listas de ocorrências, armazenando para cada termo: ID do documento, frequência e posições. Isso reduz o tempo de vetorização, já que os dados são pré-computados, garantindo eficiência em consultas e escalabilidade para corpora grandes.

    O índice inverso, contudo, apresenta \textit{trade-offs}. Ele consome mais memória do que uma varredura direta, devido ao armazenamento do dicionário de termos e das listas de ocorrências. A construção inicial do índice também é um processo custoso, com complexidade tipicamente linear em relação ao tamanho total do corpus \cite{citacao67}. Em corpora dinâmicos, onde documentos são adicionados ou alterados frequentemente, as atualizações podem exigir estratégias complexas de gerenciamento, como o uso de índices auxiliares ou a fusão periódica de índices incrementais, para manter o desempenho sem reconstruir toda a estrutura \cite{citacao67}.

    \subsection{\textit{Embeddings} baseados em Inteligência Artificial}

    Modelos estatísticos clássicos de recuperação de informação, como o TF-IDF e o BM25, operam em um nível lexical. Eles são extremamente eficientes para determinar a relevância de um documento com base na frequência e raridade de \textit{palavras-chave} exatas, mas falham em capturar o \textit{significado} ou a \textit{intenção} por trás da busca. Por exemplo, uma consulta por "responsabilidade civil em acidentes de trânsito" pode não retornar um documento altamente relevante que use a frase "indenização por colisão de veículos", pois os termos não coincidem. Essa limitação, conhecida como \textit{impedância semântica} (\textit{semantic mismatch}), é o principal desafio que as representações de texto baseadas em Inteligência Artificial buscam resolver.

    Para superar essa barreira, o Processamento de Linguagem Natural (NLP) evoluiu de modelos baseados em contagem para modelos baseados em \textit{predição}, que aprendem a representar palavras em um espaço vetorial contínuo. Essas representações, conhecidas como \textit{word embeddings} (ou apenas \textit{embeddings}), são vetores densos de números que capturam relações semânticas e sintáticas complexas a partir do contexto em que as palavras aparecem em grandes volumes de texto \cite{citacao53}. Em vez de apenas contar palavras, esses modelos aprendem seu significado, impulsionando uma nova geração de sistemas de busca semântica.

    \subsubsection{Processamento de Linguagem Natural (NLP)}

    O Processamento de Linguagem Natural (NLP) é uma área da inteligência artificial que tem como objetivo permitir que computadores compreendam, interpretem e gerem a linguagem humana de forma automática e eficiente. A linguagem natural, usada por pessoas no dia a dia, é cheia de ambiguidades, variações e contextos, o que torna o trabalho do NLP bastante desafiador. Para superar essas dificuldades, o NLP aplica técnicas que vão desde regras gramaticais, análise sintática, tokenização e stemming até modelos probabilísticos, aprendizado supervisionado, redes neurais e arquiteturas avançadas como \textit{transformers} \cite{citacao52}.

    Historicamente, os sistemas de NLP eram baseados em regras linguísticas definidas manualmente, o que exigia muito esforço e apresentava limitações para lidar com a complexidade da linguagem. Com o avanço do aprendizado de máquina, especialmente dos modelos de aprendizado profundo, o campo evoluiu significativamente. Modelos modernos, como os baseados em arquiteturas \textit{transformers}, têm a capacidade de capturar relações contextuais complexas em textos, o que melhorou muito o desempenho em tarefas essenciais do NLP, como tradução automática, análise de sentimentos, reconhecimento de fala, extração de informações, sumarização automática e respostas a perguntas \cite{citacao35}. Essas técnicas permitem que sistemas interpretem o significado de frases, reconheçam intenções e até gerem textos coerentes, facilitando a interação entre humanos e máquinas.

    O NLP precisa lidar com desafios como a ambiguidade das palavras — onde uma mesma palavra pode ter vários significados dependendo do contexto —, a variação linguística entre diferentes regiões e falantes, e a necessidade de entender o contexto mais amplo para interpretar corretamente expressões e intenções. Além disso, o desenvolvimento de sistemas eficientes depende da disponibilidade de grandes volumes de dados anotados para treinar esses modelos, o que nem sempre é fácil para línguas menos representadas \cite{citacao52}.

    Na prática, o NLP está presente em muitas aplicações que usamos diariamente, desde assistentes virtuais como Siri e Alexa, passando por sistemas de tradução automática, \textit{chatbots} para atendimento ao cliente, até ferramentas que ajudam a analisar opiniões em redes sociais. Essa popularização faz do NLP uma área essencial para a interação entre humanos e máquinas, impulsionando a transformação digital em diversos setores \cite{citacao35}.

    Nesse contexto de evolução das técnicas de NLP, um dos avanços mais importantes para a representação da linguagem foi o desenvolvimento dos \textit{embeddings}. Eles surgiram como uma solução eficaz para transformar palavras, frases ou documentos em vetores numéricos que preservam relações semânticas e contextuais \cite{citacao52,citacao53}. Essa representação vetorial tornou-se fundamental para alimentar modelos de aprendizado de máquina, especialmente os modelos mais modernos. A seguir, serão explorados o conceito de \textit{embeddings} e os principais modelos utilizados para gerá-los.

    \subsubsection{O que são \textit{embeddings}}

    No contexto do Processamento de Linguagem Natural, \textit{embeddings} são representações vetoriais densas de palavras, sentenças ou documentos, projetadas para capturar aspectos semânticos da linguagem em um espaço numérico contínuo. Diferente de representações tradicionais esparsas, como \textit{one-hot encoding} ou a própria matriz TF-IDF, que apenas indicam a presença ou a frequência de palavras, os \textit{embeddings} posicionam elementos linguisticamente semelhantes mais próximos entre si em um espaço vetorial de centenas ou milhares de dimensões \cite{citacao53}. Isso permite que modelos matemáticos e algoritmos de aprendizado de máquina consigam processar e "entender" a linguagem de forma mais contextualizada e eficiente.

    Essas representações são aprendidas automaticamente a partir de grandes volumes de dados textuais e permitem que relações semânticas e sintáticas emergentes sejam capturadas de maneira distribuída. Por exemplo, vetores de palavras como “rei” e “rainha” costumam estar próximos no espaço de \textit{embeddings}, e relações analógicas como “rei – homem + mulher = rainha” podem ser observadas em modelos bem treinados \cite{citacao53}. Essa capacidade de capturar padrões complexos tornou os \textit{embeddings} um componente essencial em praticamente todas as aplicações modernas de NLP \cite{citacao54}.

    \subsubsection{A Evolução dos Modelos de \textit{Embeddings}}

    Os modelos para geração de \textit{embeddings} evoluíram significativamente, passando de representações estáticas para representações dinâmicas e contextuais.

    \subsubsubsection{Embeddings Estáticos: Word2Vec e GloVe}

    Os primeiros modelos que popularizaram os \textit{embeddings} foram o \textit{Word2Vec} \cite{citacao53} e o \textit{GloVe} \cite{citacao54}. O \textit{Word2Vec} é baseado em aprendizado preditivo local, utilizando duas arquiteturas principais: \textit{Skip-Gram} e \textit{Continuous Bag-of-Words} (CBOW). O modelo \textit{Skip-Gram} tenta prever as palavras de contexto (vizinhas) a partir de uma palavra central. Inversamente, o CBOW tenta prever a palavra central com base em seu contexto. Por outro lado, o \textit{GloVe} adota uma abordagem baseada em contagem, construindo vetores a partir de estatísticas globais de coocorrência de palavras em todo o corpus.

    A principal característica — e limitação — desses modelos é que eles geram \textit{embeddings estáticos}. Cada palavra no vocabulário é mapeada para um único vetor, independentemente do contexto em que ela é usada. Isso significa que a palavra "banco" teria a mesma representação vetorial em "sentei no banco da praça" e em "fui ao banco depositar dinheiro". Essa incapacidade de lidar com a polissemia (múltiplos significados) foi a principal motivação para o desenvolvimento da próxima geração de modelos.

    \subsubsubsection{Embeddings Contextuais: BERT e a Revolução \textit{Transformer}}

    A grande revolução nos \textit{embeddings} veio com os modelos de linguagem contextualizados, como o ELMo \cite{citacao55} e, principalmente, o BERT \cite{citacao56}. Esses modelos não atribuem um vetor fixo a uma palavra, mas geram um \textit{embedding} dinâmico para cada palavra com base na sentença completa em que ela aparece. A palavra "banco" passa a ter vetores diferentes e apropriados para cada um dos seus significados.

    Essa capacidade foi possibilitada pela arquitetura \textit{Transformer}, que introduziu um mecanismo chamado \textbf{autoatenção} (\textit{self-attention}). Diferente de modelos anteriores que liam o texto sequencialmente (da esquerda para a direita), o mecanismo de autoatenção permite que o modelo "olhe" para todas as outras palavras da sentença simultaneamente, ponderando a importância de cada uma para definir o significado da palavra atual.

    O BERT (\textit{Bidirectional Encoder Representations from Transformers}) utiliza essa arquitetura de forma "bidirecional", lendo todo o contexto de uma vez. Para alcançar essa compreensão profunda, o BERT é pré-treinado em duas tarefas principais:
    \begin{enumerate}
        \item \textbf{Masked Language Model (MLM):} O modelo recebe uma sentença onde algumas palavras foram substituídas por um \textit{token} especial `[MASK]`. A tarefa do modelo é adivinhar qual era a palavra original, forçando-o a entender o contexto gramatical e semântico de ambos os lados (esquerdo e direito) da palavra mascarada.
        \item \textbf{Next Sentence Prediction (NSP):} O modelo recebe dois segmentos de texto, A e B, e deve prever se B é a sentença que realmente se segue a A no texto original ou se é uma sentença aleatória do corpus. Isso ensina o modelo a entender a relação lógica e coesiva entre sentenças.
    \end{enumerate}

    O resultado desse pré-treinamento intensivo é um modelo capaz de gerar \textit{embeddings} contextuais de alta fidelidade, que capturam nuances da linguagem. Modelos subsequentes, como o Sentence-BERT \cite{citacao35}, foram desenvolvidos especificamente para otimizar o BERT para tarefas de comparação de sentenças, gerando representações vetoriais de frases inteiras, ideais para busca semântica.

    \subsubsection{Recuperação e Medidas de Similaridade}
% (Esta seção foi mantida como estava, pois já era muito boa)

    A recuperação de informações em contextos de linguagem natural depende da capacidade que temos de medir similaridade entre todas estas representações textuais. Com o uso de \textit{embeddings}, textos são convertidos em vetores numéricos em espaços de alta dimensão, nos permitindo aplicar técnicas matemáticas e equações para comparar conteúdos. Entre as diversas medidas de similaridade existentes — como a distância euclidiana, de \textit{Jaccard} ou de \textit{Manhattan} — a similaridade de cosseno destaca-se por sua robustez e simplicidade, sendo amplamente adotada em tarefas de busca semântica e ranqueamento de documentos. Na subseção a seguir, detalha-se seu funcionamento e aplicação no contexto de NLP e jurídico.

    \subsubsubsection{Similaridade de cosseno}

    A similaridade de cosseno é uma métrica amplamente utilizada em Processamento de Linguagem Natural (NLP) para medir o grau de similaridade entre dois vetores em um espaço vetorial multidimensional. Quando aplicada a \textit{embeddings} de palavras, frases ou documentos, essa métrica permite quantificar o quão semanticamente próximos dois textos são entre si. A ideia central é comparar a orientação dos vetores e não sua magnitude, o que torna essa medida especialmente útil para dados textuais \cite{citacao67}.

    A fórmula da similaridade de cosseno entre dois vetores ${\vec{A}}$ e ${\vec{B}}$ é dada por:

    \[
        \text{similaridade}_{\cos}(\vec{A}, \vec{B}) = \frac{\vec{A} \cdot \vec{B}}{||\vec{A}|| \cdot ||\vec{B}||}
    \]

    onde ${\vec{A} \cdot \vec{B}}$ é o produto escalar entre os vetores, e ${||\vec{A}||}$ e ${||\vec{B}|| }$ são as normas (módulos) dos respectivos vetores. O resultado varia entre -1 e 1, mas no contexto de NLP, os valores geralmente estão entre 0 (sem similaridade) e 1 (similaridade máxima), já que os vetores de \textit{embeddings} costumam estar em um espaço de dimensão positiva.

    O resultado da similaridade de cosseno varia entre -1 e 1, pois esse é o intervalo natural da função cosseno. Valores próximos de 1 indicam que os vetores estão quase na mesma direção, sugerindo alta similaridade. Se o valor estiver perto de 0, significa que os vetores são ortogonais, ou seja, não têm relação direta. Já valores próximos de -1 indicam direções opostas, o que representa dissimilaridade, como mostra a imagem abaixo em uma simplificação 2D:

    \begin{figure}[H]
        \centering
        \caption{\label{fig:vetores} O ângulo entre os vetores é diretamente proporcional à sua similaridade.}
        \includegraphics[width=\textwidth]{assets/vetores.png}
        \legend {Fonte: \cite{citacao38}}
    \end{figure}

    A vantagem da similaridade cosseno sobre outras métricas, como a distância euclidiana, está no fato de que ela é invariável à magnitude dos vetores. Isso significa que dois vetores com direções similares, mas comprimentos diferentes (por exemplo, textos curtos vs. longos com mesmo conteúdo), ainda podem ser considerados semanticamente similares. Isso é particularmente útil em tarefas de busca semântica e recuperação de informações, onde a intenção é encontrar conteúdos com sentido próximo, independentemente da sua extensão textual \cite{citacao67}.

    Na prática, ao aplicar essa métrica, os textos são primeiro convertidos em vetores numéricos usando técnicas de \textit{embeddings} (como Word2Vec, BERT ou Sentence-BERT), e depois esses vetores são comparados utilizando a similaridade cosseno para ranquear ou agrupar documentos. Essa abordagem é a base de diversos sistemas modernos de busca jurídica, recomendação de jurisprudência e agrupamento de decisões semelhantes \cite{citacao35}.

    É importante ressaltar que o cosseno não capta relações mais complexas como ironia, polissemia ou implicaturas contextuais — para esses casos, a escolha do modelo de \textit{embedding} (estático ou contextualizado) é determinante para que os vetores usados na comparação contenham essas nuances semânticas. Ainda assim, a similaridade cosseno continua sendo uma escolha padrão e eficaz em aplicações de NLP por sua simplicidade, desempenho e capacidade de generalização \cite{citacao10}.

    \subsubsection{\textit{Embeddings} aplicados ao Direito}

    No contexto jurídico, \textit{embeddings} vêm sendo utilizados para representar textos legais em formato vetorial, permitindo a aplicação de algoritmos de NLP em tarefas como busca jurisprudencial, classificação de documentos, extração de entidades e verificação de similaridade entre decisões. Ao converter petições, sentenças, leis e pareceres em vetores numéricos, torna-se possível aplicar modelos de aprendizado para identificar padrões, sugerir documentos similares ou agrupar textos por temas jurídicos \cite{citacao39}.

    Uma das principais vantagens dos \textit{embeddings} nesse domínio é a capacidade de lidar com a linguagem técnica e formal do Direito. Modelos genéricos, treinados com textos da web, podem não capturar as nuances de termos jurídicos. Por isso, modelos da família LEGAL-BERT \cite{citacao39}, que são pré-treinados ou ajustados (\textit{fine-tuned}) especificamente em corpora jurídicos, tornam-se mais eficazes na captação de vocabulário especializado e estruturas discursivas recorrentes. Contudo, o treinamento e a inferência desses modelos demandam alto custo computacional, o que levanta questões sobre o \textit{trade-off} entre precisão semântica e eficiência, cerne da investigação deste trabalho \cite{citacao35, citacao40}.


    \section{Algoritmos}

    Algoritmos são conjuntos finitos de instruções bem definidas, ordenadas e executáveis que visam resolver um problema ou realizar uma tarefa específica. Conforme definido por Cormen e outros. \cite{citacao26}, “um algoritmo é qualquer procedimento computacional bem definido que toma algum valor ou conjunto de valores como entrada e produz algum valor ou conjunto de valores como saída”. Segundo o dicionário Michaelis \cite{citacao27}, algoritmo é “Conjunto de regras e operações e procedimentos, definidos e ordenados usados na solução de um problema”. Dessa forma, os algoritmos constituem a base do raciocínio computacional e do desenvolvimento de software, estruturando a lógica para a execução de tarefas de maneira clara e eficiente.

    \subsection{Avaliação de algoritmos: Coeficiente de \textit{Spearman}}

    Neste trabalho, após ordenarmos os documentos do mais ao menos importante com base nos algoritmos analisados, é fundamental dispor de uma métrica que nos permita comparar essas listas. Essa comparação é essencial para medir a consistência dos algoritmos e avaliar quão semelhantes são os rankings que eles produzem \cite{citacao71}. Para isso, utilizaremos o coeficiente de correlação de \textit{Spearman}, uma técnica estatística não paramétrica que quantifica a força e a direção da relação entre duas variáveis ordenadas \cite{citacao70}.

    Para entender o coeficiente de \textit{Spearman}, é útil primeiro diferenciá-lo do coeficiente de correlação de \textit{Pearson}. O \textit{Pearson} é uma medida estatística que indica a força e a direção da relação \textit{linear} entre duas variáveis contínuas. Ele avalia se os pontos de dados se aproximam de uma linha reta, mas é sensível a \textit{outliers} e assume que os dados seguem uma distribuição normal \cite{citacao70}.

    O \textit{Pearson} avalia diretamente os valores dos \textit{scores} de relevância, o que não é ideal para o nosso problema. Não nos importa se o \textit{score} do BM25 foi 0.8 e o do \textit{embedding} foi 0.9; o que nos importa é se ambos concordam que aquele documento é o "primeiro mais relevante". É justamente nesse cenário que o coeficiente de \textit{Spearman} se torna mais apropriado. Ele não olha para os \textit{scores} brutos, mas sim para os \textit{postos} (ou \textit{ranks}) dos elementos, medindo a correlação monotônica entre eles \cite{citacao70, citacao71}.

    Na prática, o coeficiente de \textit{Spearman} é simplesmente definido como o \textbf{coeficiente de correlação de \textit{Pearson} calculado sobre os postos das variáveis}. O processo de cálculo envolve transformar os \textit{scores} de cada lista em uma sequência de postos (1º, 2º, 3º, ...). Caso ocorram empates (por exemplo, dois documentos com o mesmo \textit{score}), atribui-se a eles a média de suas posições. Por exemplo, se dois documentos empatam nas posições 2 e 3, ambos recebem o posto 2,5. Após a transformação, o coeficiente de \textit{Pearson} é aplicado sobre essas duas novas listas de postos \cite{citacao70}.

    O resultado, assim como o \textit{Pearson}, varia de -1 a +1. Valores próximos de +1 indicam uma forte concordância (os algoritmos produziram rankings muito semelhantes). Valores próximos de -1 indicam uma forte discordância (um algoritmo ranqueou na ordem inversa do outro). Valores próximos de 0 apontam para uma ausência de correlação, sugerindo que os rankings são aleatórios um em relação ao outro \cite{citacao70}.

    No contexto deste trabalho, o coeficiente de \textit{Spearman} fornecerá uma medida objetiva do grau de concordância entre as listas produzidas pelas abordagens estatísticas (TF-IDF, BM25) e semânticas (\textit{embeddings}). Conforme demonstrado em \citeonline{citacao71}, que utilizou essa métrica para comparar algoritmos de ranqueamento textual, o \textit{Spearman} é uma ferramenta eficaz para avaliar a correlação entre os resultados de diferentes sistemas de busca.

    \section{Programação Paralela e Escalabilidade}

    A programação paralela vem como uma das soluções propostas para os desafios modernos da computação. A ideia por trás dela foi impulsionada pela necessidade de otimizar o tempo de processamento através algoritmos eficientes para criar aplicações que sejam capazes de lidar com um grande volume de dados e cálculos intensivos \cite{citacao12}, coisa que os algoritmos sequenciais não são tão adequados.

    \subsection{Algoritmos Sequenciais e Limitações}

    Algoritmos sequenciais representam o início da computação, onde processadores e algoritmos tinham a premissa da restrição a ordem das instruções, ou seja, uma operação de cada vez, o fim de uma representa o início da próxima. Seguindo assim uma sequência lógica e clara \cite{citacao13} como é representado na imagem abaixo.

    \begin{figure}[H]
        \centering
        \caption{\label{fig:AlgSequenciais}Algoritmos sequenciais}
        \includegraphics[width=0.8\textwidth]{assets/Algoritmos sequenciais.png}
        \legend {Fonte: Elaboração própria.}
    \end{figure}

    \subsection{Algoritmos Paralelos: Modelos e Bibliotecas}

    Algoritmos paralelos são a forma de programação que se utiliza de arquiteturas paralelas e suas características para que várias operações e instruções possam ser executadas simultaneamente dentro da estrutura, podendo esta ser, \textit{multicore}, multiprocessador ou uma rede de computadores (cluster) \cite{citacao14}.

    \begin{figure}[H]
        \centering
        \caption{\label{fig:AlgParalelos}Algoritmos paralelos}
        \includegraphics[width=0.8\textwidth]{assets/Algoritmos paralelos.png}
        \legend {Fonte: Elaboração própria.}
    \end{figure}

    Com o avanço da computação, da engenharia de software, o surgimento de várias arquiteturas \textit{multicore} e sistemas distribuídos, a paralelização dentro dessas estruturas nos permite dividir tarefas complexas em subtarefas executadas simultaneamente, maximizando a eficiência e o desempenho. Contudo, o desenvolvimento de algoritmos paralelos exige lidar com questões como sincronização, concorrência e balanceamento de carga, tornando essa prática essencial para atender às demandas de rapidez e escalabilidade atualmente \cite{citacao13}.

    \section{Trabalhos Relacionados}


% --- CAPÍTULO 3: DESENVOLVIMENTO ---
    \chapter{Desenvolvimento}

    O desenvolvimento da solução foi guiado principalmente por requisitos de desempenho e escalabilidade, já que o sistema precisava lidar com um grande volume de documentos jurídicos de forma eficiente. Em vez de seguir uma arquitetura baseada em microsserviços, a aplicação foi estruturada em módulos bem definidos, onde cada um possui sua própria responsabilidade dentro do fluxo geral do sistema. Esses módulos não se comunicam por meio de APIs ou chamadas diretas, sendo conectados apenas por manipulação humana entre as etapas, o que simplifica a integração e o controle do processo.

    Embora duas linguagens tenham sido utilizadas, o núcleo da aplicação foi desenvolvido em Go. Toda a parte de coleta de dados, assim como a lógica de orquestração e indexação dos documentos, foi implementada nessa linguagem. A escolha pelo Go se deu tanto pela preferência dos desenvolvedores quanto por suas características técnicas, já que é uma linguagem com um nível ergonômico muito alto e, ao mesmo tempo, excelente desempenho computacional. Essa combinação de praticidade e velocidade foi decisiva para o projeto.

    O Python foi empregado apenas em um ponto específico do processo, na geração das \textit{embeddings} e na ordenação dos documentos de acordo com o nível de similaridade semântica. Essa etapa fazia uso de bibliotecas consolidadas de Inteligência Artificial e Processamento de Linguagem Natural, que se encaixavam melhor no ecossistema Python.

    A seguir são apresentadas as principais tecnologias utilizadas, as estratégias adotadas na coleta de dados, o fluxo de limpeza e padronização textual e a forma como os algoritmos de busca foram implementados e integrados ao restante do sistema.

    \section{Aquisição de Dados}

    A construção do \textit{corpus} documental baseou-se na coleta massiva de textos legislativos diretamente do Portal da Câmara dos Deputados. O objetivo foi compilar uma base de dados robusta e representativa, totalizando aproximadamente 5.000 documentos, abrangendo principalmente Projetos de Lei (PL) e Propostas de Emenda à Constituição (PEC).

    Dada a magnitude do volume de dados, uma abordagem sequencial seria inviável devido à latência de rede. Portanto, a solução foi implementada em Go, localizada no pacote \texttt{corpus}, explorando o modelo de concorrência da linguagem (\textit{goroutines}) para realizar o \textit{download} dos documentos de forma paralela e assíncrona.

    Para a extração das informações, utilizou-se a biblioteca \texttt{goquery}. Esta ferramenta permite realizar o \textit{parsing} das páginas HTML retornadas pelo portal e navegar pela árvore DOM utilizando seletores CSS, facilitando a identificação precisa dos \textit{links} para os arquivos PDF em meio à estrutura complexa do site da Câmara.

    O fluxo de coleta foi estruturado da seguinte forma:

    \begin{enumerate}
        \item \textbf{Definição do Escopo:} O algoritmo inicia o processo varrendo as páginas de resultados do portal com base nas categorias legislativas de interesse, identificando os documentos disponíveis para coleta.
        \item \textbf{Execução Paralela e \textit{Rate Limiting}:} O sistema dispara múltiplas requisições simultâneas para maximizar o uso da banda. Contudo, para evitar que o servidor de origem interprete o tráfego intenso como um ataque de Negação de Serviço (DoS) ou bloqueie o endereço IP da aplicação por excesso de requisições (\textit{throttling}), foi implementado um intervalo de segurança (\textit{delay}) fixo de 500 milissegundos entre as chamadas de rede. Essa estratégia garante a estabilidade do processo de coleta e respeita as políticas de uso do portal governamental.
        \item \textbf{Extração de Links:} O conteúdo HTML de cada página processada é analisado pelo \texttt{goquery}, que filtra e extrai apenas as URLs que apontam para o inteiro teor dos documentos (arquivos com extensão \texttt{.pdf}).
        \item \textbf{Download e Persistência:} Os arquivos identificados são baixados via \textit{stream} e salvos diretamente no diretório local \texttt{./misc/corpus/pdf}, preservando a nomenclatura original para garantir a rastreabilidade dos dados.
    \end{enumerate}

    \section{Pré-processamento e Normalização Textual}

    Após a etapa de aquisição, o \textit{corpus} bruto constitui-se de milhares de arquivos em formato PDF. A utilização direta desses arquivos para fins de indexação ou vetorização é inviável devido à presença de formatação binária e, principalmente, de "ruído textual" — elementos que, embora necessários para a validade jurídica do documento (como cabeçalhos, numeração de autos, datas e assinaturas), não contribuem para a identificação do conteúdo semântico da matéria legislativa.

    Para mitigar esse problema, foi implementado um módulo robusto de processamento textual em Go, localizado no pacote \texttt{corpus}. Este módulo opera como um \textit{pipeline} de transformação, convertendo documentos não estruturados em sequências de \textit{tokens} normalizados.

    \subsection{Arquitetura de Processamento Concorrente}

    Dada a alta carga de I/O (Entrada/Saída) exigida para a leitura e conversão de milhares de arquivos PDF, a implementação não seguiu uma abordagem sequencial. Em vez disso, utilizou-se o padrão de projeto \textit{Worker Pool} nativo da linguagem Go.

    A função orquestradora, \texttt{TextProcessor}, define um limite de concorrência controlado pela variável \texttt{maxWorkers}. O funcionamento baseia-se em três primitivas de sincronização:
    \begin{itemize}
        \item \textbf{Channels (Canais):} Um canal buferizado (\texttt{workerLimit}) atua como um semáforo, bloqueando a criação de novas \textit{goroutines} quando o limite de processos simultâneos é atingido. Isso impede a exaustão de recursos do sistema operacional, como descritores de arquivos e memória RAM.
        \item \textbf{Goroutines:} Cada documento é processado em sua própria \textit{thread} leve, permitindo que enquanto um arquivo aguarda a operação de disco, a CPU possa processar a limpeza textual de outro.
        \item \textbf{WaitGroup:} O mecanismo \texttt{sync.WaitGroup} assegura que o processo principal aguarde a conclusão de todas as tarefas de limpeza antes de encerrar a execução.
    \end{itemize}

    \subsection{Extração de Texto (OCR e Conversão)}

    A primeira etapa do \textit{pipeline} é a extração do conteúdo textual. Devido à complexidade do \textit{layout} de documentos legislativos (frequentemente diagramados em colunas ou contendo quebras de página irregulares), bibliotecas nativas de leitura de PDF muitas vezes falham em manter a ordem lógica do texto.

    Para garantir a fidelidade da extração, o sistema realiza uma chamada de sistema (\textit{syscall}) para a ferramenta externa \texttt{pdftotext} (parte do pacote \textit{poppler-utils}). A função \texttt{processPDF} executa este comando via \texttt{os/exec}, direcionando o fluxo de texto extraído (\textit{standard output}) diretamente para a memória da aplicação, evitando escritas intermediárias desnecessárias em disco nesta fase.

    \subsection{Pipeline de Higienização (Cleaning)}

    O texto bruto extraído é submetido à função \texttt{CleanText}, localizada no pacote \texttt{utils}. Esta função aplica uma série de transformações destrutivas e normalizadoras, projetadas especificamente para o domínio jurídico. As etapas, executadas sequencialmente, são:

    \subsection{Normalização de Caixa (Case Folding)}
    Todo o texto é convertido para letras minúsculas (\textit{lowercase}) utilizando a função \texttt{strings.ToLower}. Essa etapa reduz a dimensionalidade do vocabulário, garantindo que termos como "LEI", "Lei" e "lei" sejam tratados como a mesma entidade vetorial.

    \subsection{Remoção de Entidades Específicas via Regex}
    Utilizando Expressões Regulares (\textit{Regular Expressions}), o sistema identifica e remove padrões que não agregam valor semântico à busca temática:
    \begin{enumerate}
        \item \textbf{Numerais Romanos:} Expressões como "Inciso IV", "Capítulo XX" ou datas em numerais romanos são removidas. A regex \texttt{\^{}M\{0,3\}(CM|CD|D?C\{0,3\})...} foi empregada para cobrir a complexidade dessas formações. No contexto de busca semântica, saber que uma lei está no "Capítulo IV" é menos relevante do que o conteúdo do capítulo em si.
        \item \textbf{URLs e Links:} Endereços web (\texttt{https?://...}) são removidos para evitar que o algoritmo indexe metadados de rodapé comuns em documentos digitais.
        \item \textbf{Datas e Valores Numéricos:} Padrões de datas (ex: "12/05/2023") e valores monetários ou decimais são eliminados. A decisão de remover esses dados baseia-se no objetivo do sistema: encontrar documentos por \textit{similaridade de assunto}, não por metadados temporais ou quantitativos específicos.
    \end{enumerate}

    \subsection{Normalização de Pontuação e Caracteres}
    A função realiza uma varredura caractere a caractere (\textit{rune-wise}), removendo qualquer elemento classificado como pontuação ou símbolo (\texttt{unicode.IsPunct}, \texttt{unicode.IsSymbol}). Hífens, travessões e \textit{underscores} são substituídos por espaços em branco para evitar a aglutinação indevida de palavras (ex: transformar "decreto-lei" em "decreto lei" e não "decretolei").

    \subsection{Canonicalização de Caracteres (Acentuação)}
    A língua portuguesa possui uma rica variedade de caracteres acentuados que podem prejudicar a correspondência exata de termos. O sistema carrega um mapa de substituições a partir do arquivo \texttt{misc/replaces.json}.
    Este arquivo define um mapeamento exaustivo de caracteres latinos estendidos para seus equivalentes ASCII simples (ex: "á", "ã", "à", "ä" $\rightarrow$ "a"; "ç" $\rightarrow$ "c"). Essa normalização é crítica para lidar com erros de digitação e inconsistências de codificação (UTF-8 vs Latin-1) comuns em sistemas legados do governo.

    \subsection{Filtragem de Stopwords}
    Por fim, utiliza-se a biblioteca \texttt{bbalet/stopwords} para remover palavras funcionais da língua portuguesa (artigos, preposições, conectivos). A preservação destas palavras inflaria artificialmente a similaridade entre documentos distintos apenas pelo uso compartilhado da gramática básica, prejudicando a eficácia dos algoritmos TF-IDF e BM25.

    O resultado final desse processo é um arquivo de texto limpo (salvo com sufixo \texttt{\_clean.txt}), contendo apenas os lexemas substantivos do documento, pronto para ser consumido pelos algoritmos de indexação e vetorização.



    \section{Modelagem de Dados e Persistência}

    A eficiência de um sistema de recuperação de informação depende intrinsecamente de como os dados são estruturados e persistidos. Para este projeto, optou-se por uma abordagem híbrida que combina a rigidez de um esquema relacional para manter a integridade dos documentos com a flexibilidade de estruturas de índice invertido para a busca rápida.

    A camada de persistência foi construída sobre o banco de dados SQLite, utilizando o ORM (\textit{Object-Relational Mapping}) GORM para a manipulação das entidades em Go. A escolha do SQLite justifica-se pela sua arquitetura \textit{serverless} de arquivo único, eliminando a latência de rede típica de conexões cliente-servidor (como em PostgreSQL ou MySQL) e permitindo um desempenho de leitura extremamente elevado quando o arquivo do banco reside em discos SSD NVMe modernos.

    \subsection{Definição das Estruturas de Dados (Structs)}

    A modelagem orientada a objetos do sistema reflete-se nas \textit{structs} definidas no pacote \texttt{models}. Cada \textit{struct} mapeia diretamente para uma tabela no banco de dados, seguindo as convenções do GORM.

    \subsubsection{Entidade Documento}
    A unidade fundamental do \textit{corpus} é representada pela \textit{struct} \texttt{Document}, definida no arquivo \texttt{models/Document.go}. Esta estrutura armazena tanto os metadados quanto o conteúdo textual em seus diferentes estágios de processamento.

    \begin{verbatim}
    type Document struct {
	ID      uint16
	Name    string
	Size    uint16
	Kind    DocKind
	Content []byte // Atributo não persistido
}
    \end{verbatim}

    A inclusão de \texttt{gorm.Model} injeta automaticamente campos de controle como \texttt{ID} (chave primária autoincremental), \texttt{CreatedAt}, \texttt{UpdatedAt} e \texttt{DeletedAt}, facilitando a auditoria e o controle de versão dos registros. O campo \texttt{Content} armazena o texto bruto extraído do PDF, enquanto \texttt{CleanContent} preserva o resultado do pipeline de normalização descrito na seção anterior, servindo de base para a geração de N-gramas.

    \subsubsection{Dicionário de Palavras (Vocabulary)}
    Para otimizar o armazenamento e as comparações, as palavras não são repetidas nas tabelas de índice. Em vez disso, utiliza-se uma tabela de vocabulário único, representada pela \textit{struct} \texttt{Word}.

    \begin{verbatim}
type Word struct {
    gorm.Model
    Word string `gorm:"uniqueIndex"`
}
    \end{verbatim}

    A \textit{tag} \texttt{uniqueIndex} na coluna \texttt{Word} é crucial: ela garante que cada termo apareça apenas uma vez no banco de dados e cria um índice B-Tree físico no SQLite, permitindo que a conversão de uma \textit{string} para seu \texttt{WordID} ocorra em tempo logarítmico $O(\log N)$.

    \subsection{Estrutura do Índice Invertido e Polimorfismo}

    A implementação do índice invertido foge à abordagem trivial de mapear "Palavra $\rightarrow$ Documentos". Para capturar o contexto local e permitir buscas por frases e proximidade, o sistema modela o índice através de três entidades distintas: \textbf{Unigramas}, \textbf{Bigramas} e \textbf{Trigramas}.

    Para que os algoritmos de cálculo de relevância (TF-IDF e BM25) pudessem operar de forma agnóstica em relação ao tamanho do n-grama, definiu-se a interface polimórfica \texttt{IGram}, localizada no pacote \texttt{models/interfaces}.

    \begin{verbatim}
type IGram interface {
    GetCacheKey(jumps, doc bool) string
    GetDocId() uint16
    Increment()
    GetCount() int
    ApplyWordWheres(db *gorm.DB) *gorm.DB
    ApplyJumpWheres(db *gorm.DB) *gorm.DB
    schema.Tabler
}
    \end{verbatim}

    Esta interface é crucial para a arquitetura do sistema, pois permite abstrair a complexidade das consultas SQL. Métodos como \texttt{ApplyWordWheres} e \texttt{ApplyJumpWheres} encapsulam a lógica de filtragem do banco de dados, permitindo que uma função de busca receba um \texttt{IGram} genérico e construa a \textit{query} correta dinamicamente, seja ela para um termo único ou uma frase de três palavras.

    A materialização dessa interface ocorre nas \textit{structs} concretas. A estrutura de um trigrama, denominada \texttt{InverseTrigram}, exemplifica a otimização de memória aplicada:

    \begin{verbatim}
type InverseTrigram struct {
    Wd0Id uint16 `gorm:"uniqueIndex:compositeindex"`
    Wd1Id uint16 `gorm:"uniqueIndex:compositeindex"`
    Wd2Id uint16 `gorm:"uniqueIndex:compositeindex"`
    DocId uint16 `gorm:"uniqueIndex:compositeindex"`
    Jump0 int8   `gorm:"uniqueIndex:compositeindex"`
    Jump1 int8   `gorm:"uniqueIndex:compositeindex"`
    Count int
}
    \end{verbatim}

    Nesta modelagem, destacam-se decisões de \textit{design} focadas em integridade e performance:

    \begin{itemize}
        \item \textbf{Índice Composto (Composite Index):} Conforme observado nas \textit{tags} do GORM (\texttt{uniqueIndex:compositeindex}), os seis primeiros atributos (as três palavras, o documento e as duas distâncias) formam, em conjunto, uma chave única composta. Isso impede a duplicação de registros para a mesma ocorrência semântica e cria um índice B-Tree otimizado no SQLite, acelerando drasticamente as operações de leitura.

        \item \textbf{Chaves Estrangeiras Virtuais Leves:} Os campos \texttt{Wd...Id} referenciam a tabela de vocabulário, mas armazenam apenas inteiros sem sinal de 16 bits (\texttt{uint16}). Isso limita o vocabulário a 65.535 termos únicos (suficiente após a limpeza agressiva) e o \textit{corpus} a 65.535 documentos, mas garante que cada linha do índice ocupe uma fração mínima de espaço em disco comparado ao armazenamento de \textit{strings}.

        \item \textbf{Conceito de "Jumps" (\textit{Skip-Grams}):} Diferente de implementações simples que exigem adjacência estrita, as estruturas \texttt{Bigram} e \texttt{Trigram} utilizam inteiros de 8 bits (\texttt{int8}) para armazenar a distância entre as palavras (\texttt{Jump}). Isso permite indexar padrões não contíguos (ex: "recurso \textit{de} apelação" pode ser capturado como um bigrama "recurso apelação" com \texttt{Jump=1}), aumentando a flexibilidade semântica da busca exata sem perder a precisão da ordem dos termos.
    \end{itemize}

    \subsection{Estratégia de Isolamento e Duplicação}

    Para garantir a integridade dos testes comparativos e evitar que a execução de um cenário influenciasse os resultados de outro (por exemplo, cache de páginas do sistema operacional ou fragmentação de índices), implementou-se um mecanismo de duplicação de banco de dados sob demanda.

    Antes de iniciar uma bateria de testes, o sistema verifica a necessidade de criar um ambiente limpo ou reutilizar dados pré-processados. Caso a \textit{flag} \texttt{fromScratch} seja falsa, a função \texttt{InitDB} invoca \texttt{DuplicateFile} para criar uma cópia física do arquivo do banco de dados mestre (contendo o \textit{corpus} limpo) para um novo arquivo identificado pelo ID da execução atual (ex: \texttt{corpus\_12345.db}).

    Essa abordagem de "Sandbox por Processo" elimina a concorrência de escrita no nível do arquivo do banco de dados e permite que múltiplos testes rodem em paralelo ou em sequência sem condições de corrida e com isolamento total de transações.

    \subsection{Otimização por Esquema Único (Single-Gram Schema)}

    Uma decisão arquitetural crítica para a performance do sistema foi a restrição de armazenar \textbf{apenas um tipo de n-grama por instância de banco de dados}.

    Embora fosse possível criar tabelas distintas para Unigramas, Bigramas e Trigramas coexistindo no mesmo banco, isso implicaria em um aumento desnecessário do tamanho do arquivo e na dispersão das páginas de dados no disco, prejudicando a localidade de referência e o desempenho do cache do SQLite.

    Para mitigar isso, a função \texttt{InitDB} utiliza uma estrutura de decisão (\texttt{switch gramSize}) para definir dinamicamente qual modelo será migrado para o banco de dados daquela execução específica:

    \begin{verbatim}
switch gramSize {
case 1:
    gramModel = &models.InverseUnigram{}
    index = "(wd0Id)"
case 2:
    gramModel = &models.InverseBigram{}
    index = "(wd0Id, wd1Id)"
case 3:
    gramModel = &models.InverseTrigram{}
    index = "(wd0Id, wd1Id, wd2Id)"
}
    \end{verbatim}

    Dessa forma, se um teste avalia a performance de Trigramas, o banco de dados é instanciado contendo apenas a tabela \texttt{InverseTrigram}. Isso garante que a tabela de índices (\texttt{WORD\_DOC}) seja extremamente compacta e otimizada exclusivamente para as consultas daquele tamanho de grama, eliminando o \textit{overhead} de manutenção de índices não utilizados.

    \subsection{Indexação Dinâmica}

    Complementando a estratégia de esquema único, a criação de índices também é realizada dinamicamente via execução de SQL bruto (\textit{Raw SQL}) no momento da inicialização. O sistema executa o comando \texttt{CREATE INDEX} ajustado para as colunas específicas do n-grama selecionado (conforme a variável \texttt{index} definida no trecho de código acima).

    Isso assegura que as consultas de busca (\textit{SELECT}), que realizam junções pesadas entre o vocabulário e a tabela de ocorrências, sempre utilizem um índice de cobertura (\textit{Covering Index}) ou um índice B-Tree otimizado, reduzindo a complexidade da busca de $O(N)$ para $O(\log N)$.

    \section{Implementação dos Algoritmos de Busca e Recuperação}
    \label{sec:algoritmos_busca}

    A lógica de recuperação de informação representa o núcleo intelectual do sistema. Enquanto a maioria das soluções comerciais delega essa tarefa para motores de busca prontos (como Elasticsearch ou Solr), neste trabalho optou-se pela implementação manual ("\textit{from scratch}") dos algoritmos TF-IDF e BM25 em Go. Esta decisão permitiu um controle granular sobre a ponderação dos termos e a otimização do uso de memória através de concorrência.

    \subsection{Pipeline Estatístico: TF-IDF}

    O algoritmo TF-IDF (\textit{Term Frequency - Inverse Document Frequency}) foi implementado no pacote \texttt{utils}, especificamente no arquivo \texttt{tf\_idf.go}. A função principal, \texttt{ComputeDocPreIndexedTFIDF}, calcula o vetor de pesos para um documento dado um conjunto de N-gramas.

    A implementação matemática segue a fórmula clássica com suavização:
    \[
        \text{TF-IDF}(t, d) = \left( \frac{f_{t,d}}{\sum_{t' \in d} f_{t',d}} \right) \times \log \left( \frac{N}{1 + df_t} \right)
    \]

    O código implementa esta lógica em duas fases distintas para maximizar a eficiência:

    \begin{enumerate}
        \item \textbf{Cálculo do TF (Local):} O algoritmo itera sobre a lista de trigramas (\texttt{trigramList}) do documento, acumulando a frequência bruta de cada termo em um mapa em memória (\texttt{tf}).
        \item \textbf{Cálculo do DF (Global) Concorrente:} A etapa mais custosa é determinar em quantos documentos do \textit{corpus} cada termo aparece ($df_t$). Para evitar que essa operação I/O-bound bloqueie o processamento, utilizou-se o padrão de \textit{fan-out/fan-in} com \textit{goroutines}.
    \end{enumerate}

    \textbf{Otimização de Concorrência:}
    O trecho de código abaixo ilustra como o paralelismo foi controlado para evitar sobrecarga no banco de dados:

    \begin{verbatim}
sem := make(chan struct{}, 25) // Semáforo de capacidade 25
var wg sync.WaitGroup

for key := range tf {
    wg.Add(1)
    sem <- struct{}{} // Adquire token
    go func(key string) {
        defer wg.Done()
        defer func() { <-sem }() // Libera token
        df := len(cacheN[key])   // Acesso thread-safe ao cache
        dfChan <- dfResult{key: key, df: df}
    }(key)
}
    \end{verbatim}

    Um canal bufferizado (\texttt{sem}) atua como um semáforo limitador, garantindo que nunca haja mais de 25 \textit{goroutines} acessando o cache ou o banco de dados simultaneamente. Isso estabiliza o \textit{throughput} da CPU e previne condições de corrida em ambientes de alta carga.

    \subsection{Pipeline Probabilístico: Okapi BM25}

    Reconhecido como o estado da arte para buscas baseadas em palavras-chave, o algoritmo BM25 foi implementado no arquivo \texttt{bm25.go}. Diferente do TF-IDF, o BM25 normaliza a frequência do termo pelo tamanho do documento, penalizando textos excessivamente longos que poderiam ter altas contagens de palavras apenas por serem verbosos.

    A função \texttt{ComputeDocPosIndexedBM25} implementa a equação:

    \[
        \text{Score}(D,Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})}
    \]

    \textbf{Parametrização e Ajuste Fino:}
    Os hiperparâmetros do algoritmo foram fixados diretamente no código fonte, seguindo valores recomendados pela literatura de Recuperação de Informação para domínios gerais, mas aplicáveis ao jurídico:
    \begin{itemize}
        \item \textbf{$k_1 = 1.5$:} Controla a saturação da frequência do termo. Indica o quão rapidamente a relevância aumenta com repetições da mesma palavra.
        \item \textbf{$b = 0.75$:} Controla a intensidade da normalização pelo comprimento do documento. Um valor de 0.75 aplica uma penalização moderada a documentos longos.
    \end{itemize}

    Para calcular o comprimento médio dos documentos (\texttt{avgDL}), essencial para o denominador da equação, o sistema executa uma consulta de agregação otimizada no banco de dados SQLite (\texttt{SELECT SUM(count) FROM WORD\_DOC}), aproveitando os índices criados na etapa de modelagem.

    \subsection{Pipeline Semântico: Vetorização via Transformers}

    Enquanto os algoritmos anteriores (implementados em Go) operam no nível léxico exato, a busca semântica foi delegada a um microsserviço especializado em Python, localizado em \texttt{src/core\_python}. A escolha do Python deve-se à maturidade do ecossistema de Deep Learning, especificamente das bibliotecas \texttt{torch} e \texttt{sentence-transformers}.

    \section{Implementação dos Algoritmos de Busca}

    A lógica de recuperação de informação foi implementada do zero em Go, permitindo um controle granular sobre as estruturas de dados e o gerenciamento de memória.

    \subsection{Modelo TF-IDF e BM25}
    Para os algoritmos estatísticos, o sistema utiliza um índice invertido persistido em um banco de dados SQLite via ORM GORM. A implementação dos cálculos de TF-IDF e Okapi BM25 ocorre no pacote \texttt{utils}.

    Um destaque da implementação é o suporte ao cálculo paralelo da frequência de documentos (DF). Dado que o cálculo de relevância exige saber em quantos documentos cada termo do \textit{corpus} aparece, essa operação pode se tornar um gargalo. O código utiliza \textit{goroutines} e \textit{channels} para paralelizar essas consultas ao banco de dados, limitando a concorrência através de um mecanismo de semáforo (buffer de canal) para não exaurir as conexões com o banco.

    \subsection{Vetorização Semântica}
    Para a busca semântica, o módulo Python utiliza a biblioteca \texttt{sentence-transformers}. O modelo escolhido, \texttt{paraphrase-multilingual-MiniLM-L12-v2}, converte os textos limpos em vetores densos de 384 dimensões. A similaridade entre os documentos é então calculada através da métrica de Similaridade de Cosseno, permitindo identificar correspondências baseadas no significado e não apenas na coincidência exata de palavras.

% ========================================================================
% BIBLIOGRAFIA
% ========================================================================
    \bibliography{bibliografia}


% % ========================================================================
% % APENDICES
% % ========================================================================
% \begin{apendicesenv}

% \partapendices

% \chapter{\label{AnexoA}Exemplo de seção de anexo}

% \begin{lstlisting}
% EXEMPLO DE CODIGO A SER ADICIONADO
% \end{lstlisting}

% \end{apendicesenv}

\end{document}
