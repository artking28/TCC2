% ------------------------------------------------------------------------
% Senac Tex: Modelo de Trabalho Academico para o Centro Universitário
% Senac
% ------------------------------------------------------------------------

% ========================================================================
% CONFIGURAÇÃO DO DOCUMENTO
% ========================================================================


\documentclass[
% -- opções da classe memoir --
    12pt,                % tamanho da fonte
    openright,            % capítulos começam em pág ímpar (insere página vazia caso preciso)
    oneside,            % para impressão em verso e anverso. Oposto a oneside
    a4paper,            % tamanho do papel.
% -- opções da classe abntex2 --
%chapter=TITLE,		% títulos de capítulos convertidos em letras maiúsculas
%section=TITLE,		% títulos de seções convertidos em letras maiúsculas
%subsection=TITLE,	% títulos de subseções convertidos em letras maiúsculas
%subsubsection=TITLE,% títulos de subsubseções convertidos em letras maiúsculas
% -- opções do pacote babel --
    english,            % idioma adicional para hifenização
    brazil                % o último idioma é o principal do documento
]{abntex2}

% ---
% Pacotes básicos
% ---
\usepackage{lmodern}            % Usa a fonte Latin Modern
\usepackage[T1]{fontenc}        % Selecao de codigos de fonte.
\usepackage[utf8]{inputenc}        % Codificacao do documento (conversão automática dos acentos)
\usepackage{lastpage}            % Usado pela Ficha catalográfica
\usepackage{indentfirst}        % Indenta o primeiro parágrafo de cada seção.
\usepackage{color}                % Controle das cores
\usepackage{graphicx}            % Inclusão de gráficos
\usepackage{microtype}            % para melhorias de justificação
\usepackage{listings}
\usepackage[top=3cm, bottom=2cm, left=3cm, right=2cm]{geometry}
\usepackage{amsmath}
\usepackage{float}

% ---
% Pacotes de citações
% ---
\usepackage[brazilian,hyperpageref]{backref}     % Paginas com as citações na bibl
\usepackage[alf]{abntex2cite}    % Citações padrão ABNT

% CONFIGURAÇÕES DE PACOTES

% Configurações do pacote backref
% Usado sem a opção hyperpageref de backref
\renewcommand{\backrefpagesname}{Citado na(s) página(s):~}
% Texto padrão antes do número das páginas
\renewcommand{\backref}{}
% Define os textos da citação
\renewcommand*{\backrefalt}[4]{
    \ifcase
        #1 %
        Nenhuma citação no texto.%
        \or
        Citado na página #2.%
    \else
        Citado #1 vezes nas páginas #2.%
    \fi}%

% Informações de dados para CAPA e FOLHA DE ROSTO
\titulo{Avaliação de métodos estatísticos na busca de correspondência textual jurídica frente a \textit{embeddings}}
\autor{Arthur Andrade e Davi Henrique}
\local{São Paulo - Brasil}
\data{2025}
\orientador{Afonso Lelis}
%\coorientador{Nome do Coorientador}
\instituicao{
    Centro Universitário Senac - Santo Amaro
    \par
    Bacharelado em Ciência da Computação
}
\tipotrabalho{Trabalho de Conclusão de Curso}
% O preambulo deve conter o tipo do trabalho, o objetivo,
% o nome da instituição e a área de concentração
\preambulo{Monografia apresentada na disciplina Trabalho de Conclusão de Curso, como parte dos requisitos para obtenção do título de Bacharel em Ciência da Computação.}

% Configurações de aparência do PDF final

% alterando o aspecto da cor azul
\definecolor{blue}{RGB}{41,5,195}

% informações do PDF
\makeatletter
\hypersetup{
%pagebackref=true,
    pdftitle={\@title},
    pdfauthor={\@author},
    pdfsubject={\imprimirpreambulo},
    pdfcreator={LaTeX with abnTeX2},
    pdfkeywords={abnt}{latex}{abntex}{abntex2}{trabalho acadêmico},
    colorlinks=true,            % false: boxed links; true: colored links
    linkcolor=black,            % color of internal links
    citecolor=black,                % color of links to bibliography
    filecolor=magenta,            % color of file links
    urlcolor=black,
    bookmarksdepth=4
}
\makeatother

% Espaçamentos entre linhas e parágrafos

% O tamanho do parágrafo é dado por:
\setlength{\parindent}{1.25cm}

% Controle do espaçamento entre um parágrafo e outro:
\setlength{\parskip}{0.2cm}

\SingleSpacing
\makeatletter
\let\@fnsymbol\@arabic
\makeatother

% compila o indice
\makeindex

\begin{document}

% Retira espaço extra obsoleto entre as frases.
    \frenchspacing

% ========================================================================
% CAPA
% ========================================================================
    \imprimircapa

% ========================================================================
% FOLHA DE ROSTO
% ========================================================================
    \imprimirfolhaderosto

% ========================================================================
% DEDICATÓRIA
% ========================================================================
    \begin{dedicatoria}
        \vspace*{\fill}
        \centering
        \noindent
        \textit{ Texto da dedicatória.} \vspace*{\fill}
    \end{dedicatoria}

% ========================================================================
% AGRADECIMENTOS
% ========================================================================
    \begin{agradecimentos}
        Texto de agradecimento.

    \end{agradecimentos}

% ========================================================================
% EPÍGRAFE
% ========================================================================
    \begin{epigrafe}
        \vspace*{\fill}
        \begin{flushright}
            \textit{``A beleza é realmente um bom dom de Deus; mas que os bons não pensem que ela é um grande bem, pois Deus a distribui mesmo para os maus. \\
            (Santo Agostinho)}
        \end{flushright}
    \end{epigrafe}

% ========================================================================
% RESUMO
% ========================================================================
    \setlength{\absparsep}{18pt} % ajusta o espaçamento dos parágrafos do resumo
    \begin{resumo}

        A digitalização massiva do setor jurídico brasileiro resultou em um volume sem precedentes de documentos textuais, como PLs, PECs etc. A recuperação de informação neste cenário enfrenta um desafio central: enquanto modelos modernos de Inteligência Artificial baseados em \textit{embeddings} oferecem alta precisão semântica, seu custo computacional de implementação e inferência é significativamente elevado. Este trabalho investiga o \textit{trade-off} pragmático entre a complexidade semântica e a eficiência computacional. Para isso, foi desenvolvido um sistema de avaliação comparativa que implementa, do zero na linguagem Go, os algoritmos estatísticos clássicos TF-IDF e Okapi BM25. Um \textit{corpus} de aproximadamente 5.000 páginas de documentos legislativos de diversos temas coletados do Portal da Câmara dos Deputados através de um \textit{scraping} e submetido a um rigoroso \textit{pipeline} de higienização textual. A persistência dos dados foi otimizada em SQLite, utilizando um esquema dinâmico que instancia bancos de dados isolados por tipo de N-grama (Unigrama, Bigrama e Trigrama) para maximizar a performance, variando o tamanho dos saltos(skip-grams) e adicionando normalização deles. Por fim, os resultados de relevância gerados pelos métodos estatísticos são comparados aos de um modelo \textit{transformer} (Sentence-BERT), utilizando o Coeficiente de Correlação de Spearman para medir a similaridade ordinal dos \textit{rankings}. O estudo fornece uma análise quantitativa que auxilia na tomada de decisão sobre a arquitetura de custo-benefício mais adequada para sistemas de busca jurídica.

        \textbf{Palavras-chave}: Processamento de Linguagem Natural, Recuperação de Informação, Busca Jurídica, \textit{Skip-grams}, TF-IDF, BM25, Embeddings, Coeficiente de Spearman.
    \end{resumo}

% ========================================================================
% ABSTRACT
% ========================================================================
    \begin{resumo}[Abstract]
        \begin{otherlanguage*}{english}
            \textit{The massive digitization of the Brazilian legal sector has resulted in an unprecedented volume of textual documents, such as PLs and PECs. Information retrieval in this scenario faces a central challenge: while modern Artificial Intelligence models based on embeddings offer high semantic accuracy, their computational cost of implementation and inference is significantly elevated. This work investigates the pragmatic trade-off between semantic complexity and computational efficiency. To this end, a comparative evaluation system was developed, implementing the classic statistical algorithms TF-IDF and Okapi BM25 from scratch in the Go language. A corpus of approximately 5,000 pages of legislative documents on diverse topics was collected from the Chamber of Deputies' portal via scraping and submitted to a rigorous textual sanitization pipeline. Data persistence was optimized using SQLite, employing a dynamic schema that instantiates isolated databases by N-gram type (Unigram, Bigram, and Trigram) to maximize performance, varying the jump sizes (skip-grams) and applying normalization to them. Finally, the relevance results generated by the statistical methods are compared to those of a transformer model (Sentence-BERT), using the Spearman Correlation Coefficient to measure the ordinal similarity of the rankings. The study provides a quantitative analysis that assists in decision-making regarding the most appropriate cost-benefit architecture for legal search systems.}

            \textit{\textbf{Key-words}: Natural Language Processing, Information Retrieval, Legal Search, Skip-grams, TF-IDF, BM25, Embeddings, Spearman Coefficient.}
        \end{otherlanguage*}
    \end{resumo}

% ========================================================================
% LISTA DE ILISTRAÇÕES
% ========================================================================
    \pdfbookmark[0]{\listfigurename}{lof}
    \listoffigures*
    \cleardoublepage

% ========================================================================
% LISTA DE TABELAS
% ========================================================================
% \pdfbookmark[0]{\listtablename}{lot}
% \listoftables*
% \cleardoublepage

% ========================================================================
% LISTA DE ABREVIATURAS E SIGLAS
% ========================================================================
    \begin{siglas}
        \item[API] Application Programming Interface
        \item[BERT] Bidirectional Encoder Representations from Transformers
        \item[CBOW] Continuous Bag-of-Words
        \item[CNJ] Conselho Nacional de Justiça
        \item[CPU] Computing Processing Unit
        \item[GPU] Graphics Processing Unit
        \item[CRF] Conditional Random Field
        \item[ELMo] Embeddings from Language Model
        \item[GO] Golang Programming Language
        \item[GloVe] Global Vectors for Word Representation
        \item[HTTP] Hypertext Transfer Protocol
        \item[IDF] Inverse Document Frequency
        \item[JSON] JavaScript Object Notation
        \item[NLP] Natural Language Processing
        \item[NER] Reconhecimento de Entidade Nomeada
        \item[POS] Part-of-Speech
        \item[PJe] Processo Judicial Eletrônico
        \item[SVM] Support Vector Machine
        \item[RAG] Retrieval-Augmented Generation
        \item[REST] Representational State Transfer
        \item[TF-IDF] Term Frequency-Inverse Document Frequency
        \item[ORM] Object-Relational Mapping
        \item[GORM] Golang ORM
        \item[I/O] Input/Output
        \item[HTML] HyperText Markup Language
        \item[PL] Proposta de lei
        \item[PEC] Proposta de Emenda à Constituição
        \item[SSD] Solid-State Drive
        \item[HDD] Hard Disk Drive
        \item[RAM] Random Access Memory
        \item[NVMe] Non-Volatile Memory Express
        \item[SQL] Structured Query Language
    \end{siglas}
% ========================================================================
% SUMÁRIO
% ========================================================================
    \pdfbookmark[0]{\contentsname}{toc}
    \tableofcontents*
    \cleardoublepage

    \textual
% ========================================================================
% INTRODUÇÃO
% ========================================================================


    \chapter{Introdução}


    \section{Contexto}

    A era digital transformou radicalmente o acesso à informação, gerando um volume de dados sem precedentes em praticamente todos os setores da sociedade. No campo do Direito, essa realidade se manifesta de forma contundente na digitalização massiva de processos, leis, jurisprudências e pareceres. O Judiciário brasileiro, por exemplo, vivenciou uma explosão documental ao receber mais de 250 milhões de processos em formato eletrônico nos últimos 15 anos \cite{citacao59}. Se por um lado essa digitalização representa um avanço inegável em termos de transparência e preservação, por outro, ela impõe um desafio monumental: a simples capacidade de armazenamento tornou-se insuficiente. O problema agora desloca-se para a navegabilidade e a extração de valor desse oceano de textos complexos e interconectados, criando uma demanda urgente não apenas por memória digital, mas por capacidade de processamento inteligente.

    Para responder a essa necessidade de inteligência interpretativa, as técnicas de Processamento de Linguagem Natural (NLP), especialmente aquelas baseadas em Inteligência Artificial e aprendizado de máquina, emergiram como a fronteira tecnológica mais promissora. Diferentemente dos sistemas antigos de busca exata, os modelos de linguagem avançados — notadamente aqueles baseados na arquitetura \textit{Transformer}, como o BERT e o LEGAL-BERT — prometem "ler" os documentos, compreendendo nuances semânticas e contextuais que escapam à busca por palavras-chave. Ferramentas que utilizam arquiteturas de \textit{transformers} e representações vetoriais (\textit{embeddings}) representam o atual estado da arte, buscando emular uma compreensão quase humana dos textos legais para resolver tarefas complexas, como a identificação de similaridade entre decisões e divergências jurisprudenciais \cite{citacao60}.

    No entanto, o entusiasmo com a sofisticação da Inteligência Artificial muitas vezes ofusca uma barreira prática significativa: o custo computacional. A adoção dessas tecnologias de ponta não é gratuita; ela exige um esforço financeiro, técnico e energético considerável. O treinamento e, principalmente, a inferência de grandes modelos de linguagem demandam hardware especializado (GPUs) e infraestrutura robusta, o que pode tornar a solução inviável para muitas instituições. Conforme o uso de IA se expande, torna-se crítico não apenas buscar o modelo mais "inteligente", mas também aplicar métodos para otimizar e reduzir os custos de operação desses sistemas, garantindo sua sustentabilidade \cite{citacao61}.

    Essa tensão entre a performance semântica e o custo computacional nos leva diretamente ao cerne da investigação proposta neste trabalho. Diante de um cenário onde recursos são finitos, é imperativo questionar até que ponto a complexidade dos modelos de \textit{machine learning} é estritamente indispensável para a tarefa de busca em documentos jurídicos. Surge, então, uma dúvida metodológica relevante: será que métodos estatísticos e algoritmos de busca mais clássicos, se bem parametrizados e aplicados, não poderiam alcançar resultados comparáveis aos da IA moderna, mas consumindo apenas uma fração dos recursos?

    Este estudo se propõe, portanto, a explorar essa fronteira de eficiência. A literatura já aponta indícios nessa direção, como observado por \citeonline{citacao62}, que demonstra que modelos distribucionais complexos como o BERT não apresentam, necessariamente, um desempenho superior em todas as tarefas de classificação jurídica quando comparados a abordagens mais simples. O objetivo deste trabalho não é desenvolver uma nova ferramenta, mas sim realizar uma análise crítica e comparativa rigorosa, buscando compreender e quantificar o verdadeiro \textit{trade-off} entre a precisão semântica oferecida pela IA e a eficiência computacional dos métodos estatísticos no domínio específico do Direito.

    \section{Justificativa}

    A busca por maior precisão impulsionou o avanço da tecnologia jurídica e estimulou uma dependência crescente de modelos de Inteligência Artificial. Esse movimento ganhou força no Brasil, onde o uso dessas soluções no Judiciário aumentou 26\% entre 2022 e 2023 \cite{citacao63}. A ideia que sustenta esse crescimento é simples. Quanto mais sofisticado o modelo, melhores seriam os resultados. Só que essa expectativa costuma vir acompanhada de um custo elevado. O treinamento de grandes modelos depende de processamento intenso e de hardware especializado, o que envolve investimentos altos em infraestrutura e manutenção. Em muitos casos, o peso financeiro impede que instituições menores entrem nessa corrida tecnológica, mesmo quando há interesse.

    Ao mesmo tempo, a evolução do hardware abriu caminhos que nem sempre são lembrados. O avanço das arquiteturas \textit{multicore} não transformou apenas os modelos de IA. Ele também ampliou o alcance de algoritmos clássicos que antes operavam de forma limitada. Hoje esses métodos podem explorar vários núcleos de processamento e alcançar um desempenho muito superior ao que entregavam no passado \cite{citacao64}. Técnicas como TF-IDF, assim como algoritmos de busca de padrões como o Boyer-Moore, não exigem estruturas complexas e ainda assim se beneficiam de forma notável da paralelização. Mesmo com esse potencial, costumam ser vistos como alternativas inferiores simplesmente por não pertencerem ao universo dos modelos semânticos mais recentes.

    Essa percepção costuma levar a decisões pouco equilibradas. Por isso, a justificativa deste trabalho parte da necessidade de um olhar mais pragmático. Nem sempre o ganho marginal oferecido por um modelo semântico avançado compensa seu custo total de operação. Em ambientes com linguagem estável e previsível, como ocorre em grande parte dos documentos jurídicos, soluções mais simples podem alcançar resultados muito satisfatórios \cite{citacao65}. Em várias atividades do dia a dia, responder rápido e gastar menos pode ser mais importante do que captar nuances raras de ambiguidade.

    Dentro desse cenário, esta pesquisa busca oferecer uma análise quantitativa do equilíbrio entre custo e benefício. A proposta é comparar diferentes abordagens em termos de desempenho, tempo de resposta e uso de recursos. Trabalhos que analisam modelos como BERT e ChatGPT já fazem esse tipo de avaliação usando métricas como acurácia, precisão e F1-score \cite{citacao66}. Seguindo essa linha, pretendemos produzir dados concretos que ajudem a escolher a tecnologia mais adequada para cada necessidade do ecossistema jurídico. A intenção é mover a discussão para um campo mais objetivo, onde a escolha não dependa apenas do que é mais moderno, mas do que realmente funciona melhor dentro de cada contexto.


    \section{Objetivos}

    \subsection{Objetivo Geral}

    Avaliar computacionalmente e contextualmente comparando o desempenho de métodos estatísticos puro e embeddings para a indexação e busca de informações em documentos jurídicos, analisando o \textit{trade-off} entre a complexidade de ambos.

    \subsection{Objetivos Específicos}

    \begin{enumerate}
        \item Realizar o \textit{scrapping} de documentos jurídicos afim de construir uma forte e extensa base de documentos.
        \item Retirar \textit{stop-words}, capitalizar letras e normalizar as letras do corpus de documentos para criar uma base de testes consistente para ambas as abordagens.
        \item Implementar um pipeline de busca baseado em \textit{embeddings} de sentenças, utilizando a similaridade de cosseno para ranquear os resultados de acordo com a proximidade semântica.
        \item Implementar um pipeline de busca baseado no modelo estatístico TF-IDF e BM25 para criar uma representação vetorial baseada na frequência de termos e ranqueá-los através da similaridade de cosseno.
        \item Definir e aplicar um conjunto de métricas para avaliar e comparar o desempenho das diferentes abordagens, considerando precisão, tempo de resposta.
        \item Analisar criticamente os resultados obtidos para identificar as vantagens, desvantagens e os cenários de aplicação mais adequados para cada método no contexto específico dos textos jurídicos.
    \end{enumerate}


    \chapter{Revisão bibliográfica}


    \section{Processamento de Linguagem Natural no Domínio Jurídico}

    A crescente produção textual na sociedade contemporânea transformou a informação em um recurso abundante, porém de difícil processamento. No universo jurídico, esse fenômeno é ainda mais crítico, visto que contratos, decisões e legislações são produtos essencialmente linguísticos. O Direito, enquanto sistema normativo, é construído e operado exclusivamente por meio da linguagem, o que torna a precisão interpretativa um requisito fundamental \cite{citacao42, citacao43}.

    Para lidar com esse volume exponencial de documentos, que excede as limitações cognitivas humanas, o uso de ferramentas computacionais deixou de ser opcional para se tornar uma necessidade. Nesse cenário, as técnicas de Processamento de Linguagem Natural (NLP) surgem não apenas como uma ferramenta de automação, mas como a abordagem consolidada para enfrentar o desafio de interpretar grandes massas de dados textuais \cite{citacao17, citacao44}.

    Diferentemente de sistemas rígidos baseados em regras, o NLP permite que as máquinas compreendam e operem diretamente sobre a base textual do Direito. Essas técnicas viabilizam desde a extração de informações relevantes e identificação de padrões até a sumarização de documentos e suporte à tomada de decisão, incorporando uma camada de "inteligência" ao processamento bruto de dados \cite{citacao45, citacao43}.

    Essa evolução tecnológica já se traduz em aplicações práticas robustas no ambiente jurídico, como a análise preditiva de jurisprudência e a busca semântica em bases normativas. No contexto brasileiro, estudos como os de \citeonline{citacao17} evidenciam que essas tecnologias já estão consolidadas, estabelecendo um novo padrão de referência para a recuperação e análise de informação no setor.

    \subsection{Características dos textos jurídicos}

    A aplicação de técnicas de recuperação de informação no domínio jurídico envolve desafios particulares, uma vez que os textos legais apresentam propriedades distintas de outros gêneros textuais. Em primeiro lugar, são documentos altamente especializados, redigidos com vocabulário técnico e linguagem formal. Termos como \textit{vossa excelência}, \textit{ex positis} ou \textit{ad nutum} desempenham papel semântico central \cite{citacao42,citacao44}. Para sistemas de busca, essa especificidade lexical gera efeitos distintos dependendo da abordagem: favorece métodos baseados em palavras-chave (devido à raridade dos termos), mas pode confundir modelos semânticos genéricos que não foram treinados nesse vocabulário específico.

    Além disso, esses textos apresentam um alto grau de ambiguidade controlada. Palavras como \textit{poderá}, \textit{deverá} ou \textit{caberá} têm significados normativos específicos que impactam diretamente a interpretação das normas. Essa polissemia representa um desafio de desambiguação: métodos estatísticos tendem a tratar essas palavras apenas como \textit{tokens} distintos, ignorando sua carga normativa, enquanto modelos semânticos precisam de um ajuste fino robusto para diferenciar o "sentido jurídico" do uso coloquial \cite{citacao17,citacao45}.

    Outro aspecto crítico, e talvez o mais desafiador para a recuperação de informação, é a intertextualidade jurídica. Leis, contratos e sentenças não existem no vácuo; eles referenciam constantemente outros documentos. Isso cria um cenário onde a relevância de um documento muitas vezes depende de uma referência externa exata, e não apenas do seu conteúdo semântico interno. Esse fenômeno desafia a lógica dos \textit{embeddings}, que buscam similaridade de significado e podem perder a precisão da referência exata, e também impõe dificuldades aos métodos estatísticos, que tratam citações complexas apenas como sequências de caracteres \cite{citacao17}.

    Também é importante considerar a estrutura argumentativa dos textos. Documentos como sentenças possuem uma lógica discursiva específica. A informação mais relevante para uma busca nem sempre está distribuída uniformemente pelo texto. Isso afeta diretamente a performance dos algoritmos: o TF-IDF pode supervalorizar a repetição de termos na fundamentação, enquanto um modelo de \textit{embedding} que faz a média vetorial do documento inteiro pode diluir o "sinal" da decisão final contida na conclusão \cite{citacao43}.

    \subsection{Desafios de Processamento e Representação em Português}

    Ao aplicar técnicas de NLP à língua portuguesa, especialmente no domínio jurídico, surgem obstáculos que permeiam desde a análise morfossintática até a representação semântica vetorial. Diferentemente do inglês, o português brasileiro apresenta uma morfologia altamente flexiva, com ampla variação de formas verbais, nominais e pronominais, o que eleva a complexidade de tarefas fundamentais como a tokenização e a lematização \cite{citacao17}.

    No contexto jurídico, essa complexidade é intensificada pelo uso recorrente de locuções e expressões idiomáticas. Construções como \textit{sem prejuízo do disposto}, \textit{salvo disposição em contrário} ou \textit{nos termos da legislação aplicável} funcionam como unidades semânticas coesas. Analisá-las isoladamente, palavra por palavra, pode fragmentar o sentido original, exigindo abordagens de pré-processamento capazes de identificar e preservar o significado completo dessas expressões \cite{citacao44}.

    Essa especificidade linguística impõe desafios práticos imediatos na etapa de tokenização. O tratamento da pontuação, por exemplo, exige regras de exceção: elementos como números de processo (ex: 1234567-89.2023.8.26.0100) devem ser preservados como um único \textit{token}, dada sua importância identificadora, enquanto a pontuação gramatical comum deve ser removida \cite{citacao17}. Da mesma forma, o uso de hífens em palavras compostas (ex: \textit{auto-aplicável}) e a presença de entidades nomeadas complexas (ex: \textit{Ministério Público}, \textit{Poder Judiciário}) demandam estratégias de segmentação que evitem a perda de informação semântica, um problema bem documentado na literatura de NLP aplicada ao direito brasileiro \cite{citacao17}.

    Além da segmentação e limpeza, há o desafio da representação vetorial (\textit{embeddings}). Para que algoritmos possam realizar tarefas como busca jurisprudencial ou classificação, os textos precisam ser convertidos em vetores numéricos. No entanto, modelos de \textit{embeddings} genéricos, treinados com textos da web, frequentemente falham em capturar as nuances da linguagem técnica e formal do Direito \cite{citacao39}.

    Modelos especializados, como a família LEGAL-BERT \cite{citacao39}, que são pré-treinados ou ajustados (\textit{fine-tuned}) especificamente em corpora jurídicos, mostram-se mais eficazes na captação desse vocabulário especializado e das estruturas discursivas recorrentes. Contudo, o uso desses modelos introduz um novo obstáculo: o alto custo computacional de treinamento e inferência. Isso levanta a questão central do \textit{trade-off} entre a precisão semântica oferecida por modelos de linguagem pesados e a eficiência de métodos mais simples, cerne da investigação deste trabalho \cite{citacao35, citacao40}.

    \subsection{Modelos de Linguagem para o Domínio Jurídico}

    Embora bibliotecas de NLP de propósito geral, como \textit{spaCy} ou \textit{NLTK}, ofereçam suporte para a língua portuguesa, seus modelos genéricos, treinados em textos da web ou literatura, falham em capturar as nuances terminológicas e o contexto semântico específico do universo jurídico. A palavra "competência", por exemplo, tem um significado técnico e distinto no Direito (atribuição de um órgão para julgar) que um modelo genérico não consegue priorizar.

    Para suprir essa lacuna, a comunidade acadêmica tem se concentrado no desenvolvimento de modelos de linguagem especializados. Um trabalho central nesse contexto para o português brasileiro é o \citeonline{citacao17} (LegalNLP). Os autores demonstram que modelos como Word2Vec e BERT, quando pré-treinados ou ajustados (\textit{fine-tuning}) em um vasto e específico corpus de documentos jurídicos nacionais, superam significativamente o desempenho de modelos genéricos em tarefas-chave de NLP.

    Essa abordagem de especialização é uma tendência global. O trabalho de \citeonline{citacao39}, por exemplo, introduziu o LEGAL-BERT, um modelo da família BERT treinado inteiramente em textos legais. A pesquisa comprovou que essa especialização de domínio permite uma captação muito mais precisa do vocabulário técnico e das estruturas discursivas do Direito do que o modelo BERT original.

    Portanto, a literatura estabelece que o estado da arte para a busca semântica de alta precisão no domínio jurídico envolve o uso de modelos de linguagem contextuais, pesados e de domínio específico. É justamente essa abordagem, que representa o polo de maior complexidade e custo computacional, que este trabalho utilizará como referência para a comparação com os métodos estatísticos tradicionais.


    \section{Técnicas de Pré-processamento Textual}

    Em um cenário de grande volume textual, como o jurídico, a extração de informação relevante demanda um tratamento prévio sobre os documentos. Essa etapa é conhecida como pré-processamento, e tem como objetivo transformar textos brutos em representações mais estruturadas e significativas, facilitando tanto a análise semântica quanto a indexação e recuperação posterior. Trata-se, portanto, de uma fase indispensável para qualquer sistema de recuperação de informação textual, sobretudo em domínios especializados, já que a escolha adequada das técnicas de pré-processamento pode influenciar significativamente a relevância das informações extraídas \cite{citacao16}.

    Além de normalizar o conteúdo textual, o pré-processamento também é responsável pela definição do vocabulário de termos que será utilizado na indexação e na construção de representações computacionais. Um bom pré-processamento reduz o tamanho do vocabulário, melhora o desempenho e aumenta a precisão das buscas \cite{citacao67}. Essa etapa funciona, portanto, como uma ponte entre a linguagem natural e os algoritmos, permitindo que informações relevantes sejam extraídas de forma estruturada, eficiente e precisa. No contexto deste trabalho, ela se mostra essencial para a identificação de elementos fundamentais nos documentos jurídicos, como nomes das partes, dispositivos legais e decisões.

    \subsection{Tokenização}

    No contexto do processamento de linguagem natural, um \textit{token} é definido como uma unidade básica de texto, normalmente correspondente a uma palavra, número, pontuação ou símbolo significativo. A forma como os \textit{tokens} são extraídos influencia diretamente a eficácia dos modelos computacionais, especialmente em contextos especializados como o jurídico, onde há presença de construções formais e terminologias específicas.

    O processo de tokenização refere-se, portanto, à separação do texto contínuo em unidades menores, os \textit{tokens}, que geralmente representam palavras. Essa tarefa vai além de simplesmente dividir o texto por espaços; por exemplo, expressões como “São Paulo” ou “Art. 5º” exigem regras específicas para não serem segmentadas incorretamente, especialmente em textos técnicos como os jurídicos \cite{citacao67}.

    \begin{figure}[H]
        \centering
        \caption{\label{fig:tokenizacao}Exemplo de \textit{tokenização}.}
        \includegraphics[width=\textwidth]{assets/tokenizacao.png}
        \legend {Fonte: Elaboração própria.}
    \end{figure}

    \subsection{Remoção de Stopwords}

    A remoção de \textit{stopwords} é uma das etapas principais do pré-processamento textual e consiste na exclusão de palavras que ocorrem com alta frequência, mas que carregam pouco ou nenhum valor semântico individual — como \textit{de}, \textit{que}, \textit{em}, \textit{o} e \textit{a}. Esse tipo de filtragem é comumente utilizado para reduzir o vocabulário e acelerar os algoritmos, sem perda significativa de informação relevante para a maioria das tarefas de recuperação e classificação textual. \cite{citacao67}

    Apesar de a remoção de \textit{stopwords} ser uma prática comum em tarefas de processamento de linguagem natural, no contexto jurídico é necessário cautela. Isso porque palavras funcionais(geralmente ignoradas em outros domínios), podem conter informações fundamentais para a interpretação de dispositivos legais, como apontado por \citeauthor{citacao17} (\citeyear{citacao17}), que optaram por não removê-las durante o treinamento de seus modelos linguísticos, justamente para preservar a integridade semântica dos textos judiciais.

    \begin{figure}[H]
        \centering
        \caption{Exemplo de remoção de \textit{stopwords}}
        \includegraphics[width=\textwidth]{assets/Stopwords.png}
        \legend{Fonte: Elaboração própria.}
    \end{figure}

    \subsection{Lematização e Stemização}

    A lematização reduz palavras à sua forma canônica, conhecida como \textit{lema}. Por exemplo, termos como \textit{decidir}, \textit{decisão} e \textit{decidido} são todos associados ao verbo \textit{decidir}. Esse processo considera a análise morfológica e o contexto gramatical, sendo essencial para manter o sentido das palavras nos textos jurídicos, onde pequenas variações morfológicas podem alterar significativamente o significado \cite{citacao10}.

    Por outro lado, a stemização é uma técnica mais simples que remove afixos (prefixos e sufixos) para obter uma raiz. No entanto, essa raiz nem sempre corresponde a uma palavra válida no idioma. Por exemplo, as palavras \textit{decidir}, \textit{decidindo} e \textit{decidido} podem ser reduzidas a \textit{decid}.

    \begin{figure}[H]
        \centering
        \caption{Exemplo de diferenças entre Stemização e Lematização.}
        \includegraphics[width=\textwidth]{assets/lem_stem.png}
        \legend{Fonte: Elaboração própria, inspirada em \cite{citacao18}.}
    \end{figure}

    A lematização proporciona maior fidelidade semântica, sendo mais indicada para tarefas que exigem compreensão precisa dos textos, como análise e busca em documentos jurídicos. Essa abordagem, por depender de análise linguística completa e regras morfológicas, costuma ter maior custo computacional e necessidade de recursos linguísticos bem estruturados, especialmente em idiomas como o português, que possui alta complexidade morfológica \cite{citacao17}.

    Por outro lado, a stemização é uma solução mais simples e eficiente em termos computacionais, baseada em regras heurísticas para remoção de afixos. Embora menos precisa, pode ser útil em tarefas onde a sensibilidade semântica não é tão crítica \cite{citacao18}.

    No contexto jurídico brasileiro, a adoção de lematização se mostra mais vantajosa, uma vez que ferramentas linguísticas genéricas não são suficientes para capturar as nuances da linguagem jurídica em português \cite{citacao17}.

    \subsection{Segmentação de Sentenças}

    A segmentação de sentenças consiste em dividir um texto contínuo em unidades menores, geralmente delimitadas por frases. Esse processo é essencial para permitir que modelos de processamento de linguagem operem com unidades textuais semanticamente completas, sobretudo em tarefas que requerem maior nível de precisão, como extração de fundamentos jurídicos, sumarização automática, análise de jurisprudências ou geração de relatórios.

    Embora seja considerado um processo relativamente simples no processamento de linguagem natural, a segmentação de sentenças apresenta desafios em textos jurídicos. A alta frequência de abreviações, siglas e expressões formais, como “Art.”, “Inc.”, “V. Exa.” ou “D.J.E.”, gera ambiguidades na definição dos pontos que efetivamente representam o final de uma sentença. Pontuações como ponto final, dois-pontos e até ponto e vírgula podem assumir funções distintas dependendo do contexto \cite{citacao17}.


    \section{Representação de Documentos}

    \subsection{TF-IDF}

    Após o pré-processamento, uma das técnicas mais consolidadas para representar documentos como vetores numéricos é o TF-IDF (\textit{Term Frequency--Inverse Document Frequency}). Esse método associa a cada termo um peso que reflete tanto sua importância local(baseada na frequência com que aparece no documento), quanto sua importância global, que é determinada pela raridade do termo na coleção como um todo \cite{citacao10, citacao67}.

    O cálculo do TF--IDF é composto por duas etapas fundamentais. A primeira consiste na determinação da frequência de termo \textit{Term Frequency} (TF), que quantifica o número de vezes que um termo \(t\) aparece em um documento \(d\), representado por \(\mathit{tf}_{t,d}\). Para evitar que termos muito frequentes dominem a representação, especialmente em documentos longos, é comum aplicar uma normalização logarítmica, definida como:
    \[
        \mathit{tf}_{t,d} =
        \begin{cases}
            1 + \log(\mathit{tf}_{t,d}), & \text{se } \mathit{tf}_{t,d} > 0, \\
            0, & \text{caso contrário}.
        \end{cases}
    \]
    Essa transformação suaviza o impacto de termos com alta contagem, tornando a representação mais robusta e comparável entre documentos de diferentes tamanhos \cite{citacao67}.

    A segunda etapa envolve o cálculo da \textit{Inverse Document Frequency} (IDF), que mede a capacidade de um termo discriminar documentos. Dado \(N\) como o número total de documentos na coleção e \(df_t\) como o número de documentos em que o termo \(t\) aparece, a IDF é calculada como:
    \[
        \mathit{idf}_t = \log \left( \frac{N}{df_t} \right)
    \]
    Esse fator tem como objetivo reduzir o peso de termos comuns na coleção, como artigos, pronomes ou palavras genéricas, e aumentar o peso de termos mais raros, que são mais úteis para diferenciar documentos \cite{citacao67}.

    Assim, o peso final de um termo em um documento, chamado de TF--IDF, é dado pela multiplicação dos dois componentes:
    \[
        \mathrm{tf\text{-}idf}(t,d) = \mathit{tf}_{t,d} \times \mathit{idf}_t
    \]
    Esse valor será mais alto para termos que ocorrem frequentemente em um documento específico, mas que são raros na coleção como um todo, refletindo sua importância tanto local quanto global \cite{citacao67}.

    A representação de documentos no modelo vetorial é então construída a partir dos pesos TF--IDF de cada termo. Formalmente, um documento é representado como um vetor \(\mathbf{v}(d) = [\mathrm{tf\text{-}idf}(t_1,d), \dots, \mathrm{tf\text{-}idf}(t_M,d)]\), onde \(M\) é o número total de termos únicos na coleção. Consultas podem ser representadas de maneira análoga, permitindo o uso de métricas como a similaridade do cosseno para calcular a proximidade entre consultas e documentos no espaço vetorial \cite{citacao67}.

    \begin{figure}[H]
        \centering
        \caption{Exemplo da aplicação do TF-IDF em frases.}
        \includegraphics[width=\textwidth]{assets/TF_IDF.png}
        \legend{Fonte: Elaboração própria, inspirada em \cite{citacao41}}
    \end{figure}

    O modelo TF--IDF é amplamente utilizado em tarefas de recuperação de informação, classificação e agrupamento de textos, justamente por sua simplicidade e eficácia. Seu principal mérito reside na combinação da frequência local com a raridade global dos termos, permitindo destacar palavras que são relevantes para o conteúdo específico de um documento, enquanto penaliza termos genéricos e pouco informativos \cite{citacao10}.

    \subsection{BM25}

    Enquanto o TF-IDF estabeleceu um modelo fundamental para a recuperação de informação, ele possui limitações intrínsecas, como a tendência de favorecer desproporcionalmente termos de alta frequência e a falta de uma normalização robusta para o tamanho dos documentos. Para superar essas questões, foi desenvolvido o algoritmo Okapi BM25 (doravante chamado apenas de BM25), um modelo de ranqueamento probabilístico que se tornou um padrão de fato em sistemas de busca modernos devido à sua eficácia e robustez \cite{citacao67}.

    O BM25 não trata a frequência de um termo (\textit{term frequency}) de forma linear. Em vez disso, ele introduz um componente de saturação, partindo da premissa de que a relevância de um termo para um documento não cresce infinitamente com sua frequência. A primeira vez que uma palavra aparece é muito significativa; a décima, um pouco menos; e a centésima vez adiciona uma relevância marginal muito pequena. O algoritmo modela esse comportamento para evitar que documentos longos e repetitivos dominem os resultados da busca.

    A pontuação de relevância de um documento \textit{D} para uma consulta \textit{Q}, que contém os termos \(q_1, q_2, \dots, q_n\), é calculada pela seguinte fórmula:

    \[
        \text{score}(D, Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
    \]

    Para entender seu funcionamento, podemos decompor a fórmula em suas três partes principais. O primeiro componente é o \textit{Inverse Document Frequency} (IDF), que, assim como no TF-IDF, mede a raridade de um termo na coleção de documentos. O BM25 utiliza uma variante da fórmula padrão de IDF, geralmente com um ajuste para evitar valores nulos ou negativos para termos que aparecem em mais da metade dos documentos \cite{citacao67}. A lógica central, no entanto, permanece a mesma: termos que são raros em toda a coleção recebem um peso maior, pois são considerados mais discriminativos.

    O segundo e mais inovador componente da fórmula lida com a \textit{saturação da frequência do termo}. Diferente do TF-IDF, o BM25 parte do princípio de que a relevância de um termo não cresce infinitamente com sua frequência. A primeira ocorrência é muito significativa, mas cada aparição subsequente contribui progressivamente menos para a pontuação final. Esse efeito de saturação é controlado pelo parâmetro de calibração \textit{k1} (geralmente entre 1.2 e 2.0), que define quão rapidamente a pontuação de um termo se estabiliza. Um valor de \textit{k1} baixo faz com que a relevância sature rapidamente, enquanto um valor mais alto permite que a frequência do termo continue a ter um impacto maior na pontuação \cite{citacao67}.

    Por fim, o BM25 introduz uma \textit{normalização pelo comprimento do documento} muito mais sofisticada. O algoritmo compara o tamanho do documento atual, \(|D|\), com o tamanho médio de todos os documentos na coleção, \textit{avgdl}. O parâmetro \textit{b} (geralmente em torno de 0.75) controla o grau de influência que o tamanho do documento tem na pontuação final. Quando \textit{b} é 1, o efeito da normalização é máximo; quando é 0, o tamanho do documento é completamente ignorado. Essa abordagem permite penalizar documentos que são muito mais longos que a média, pois eles têm uma probabilidade estatisticamente maior de conter os termos da busca por acaso, sem serem necessariamente mais relevantes \cite{citacao67}.

    Em síntese, o BM25 aprimora os conceitos do modelo vetorial ao incorporar uma visão probabilística e heurísticas mais refinadas sobre como a frequência de termos e o tamanho dos documentos influenciam a relevância. Sua capacidade de ser calibrado pelos parâmetros \textit{k1} e \textit{b} o torna altamente adaptável a diferentes tipos de coleções de texto. Essa relevância é comprovada por sua aplicação prática em diversos domínios, incluindo estudos recentes no contexto jurídico brasileiro para a identificação de similaridade em documentos oficiais \cite{citacao68}.

    \subsection{Índice inverso}

    O cálculo eficiente de medidas como o TF-IDF e o BM25, bem como a execução de buscas rápidas em grandes volumes de texto, dependem de uma estrutura de dados mais sofisticada do que a simples lista de documentos. A cada nova consulta ou para a construção dos próprios modelos estatísticos, seria computacionalmente inviável reanalisar todo o corpus para encontrar termos, suas frequências (\textit{term frequency}) e em quantos documentos eles aparecem (\textit{document frequency}). Para resolver esse problema, utiliza-se o \textbf{índice inverso} (ou \textit{inverted index}), que funciona como a principal estrutura de indexação para sistemas de recuperação de informação \cite{citacao67}.

    O índice inverso é uma estrutura de dados amplamente utilizada em sistemas de recuperação de informação, projetada para otimizar a busca de termos em grandes coleções de documentos \cite{citacao67, citacao69}. Diferentemente de uma abordagem direta, que examinaria cada documento sequencialmente (uma operação de complexidade \(O(N)\), onde N é o tamanho do corpus), o índice invertido organiza os termos do corpus de forma a mapear cada palavra ou unidade lexical às suas respectivas ocorrências. Esse mapeamento inclui informações como a identificação dos documentos e, comumente, a posição ou frequência de cada termo. Essa organização permite consultas eficientes e rápidas, fundamentais para o funcionamento de sistemas como motores de busca \cite{citacao67}. Segue uma imagem que exemplifica o índice inverso.

    \begin{figure}[H]
        \centering
        \caption{\label{fig:Indice} No índice inverso quem mapeia são os termos e n.}
        \includegraphics[width=\textwidth]{assets/Indice.png}
        \legend {Fonte: Elaboração própria.}
    \end{figure}

    Uma forma de otimizar o uso de memória do índice inverso é por meio da compressão das listas de ocorrências (\textit{postings lists}). Técnicas comuns incluem a codificação de diferenças (\textit{gap encoding} ou \textit{delta encoding}), onde apenas a diferença entre IDs de documentos consecutivos é armazenada, e codificações de inteiros como \textit{varint} (\textit{Variable Byte coding}) ou \textit{Golomb coding}, que reduzem o espaço ocupado por números pequenos. Além de economizar memória, a compressão pode acelerar consultas, já que menos dados precisam ser carregados da memória ou disco, sendo uma prática padrão em motores de busca de larga escala \cite{citacao67}.

    O índice inverso oferece consultas com complexidade \(O(\log n)\) para a busca de termos no dicionário da estrutura, onde \(n\) é o número de termos únicos, caso o dicionário seja implementado com árvores balanceadas; ou \(O(1)\) em média, se forem usadas tabelas hash \cite{citacao67}. Isso se difere drasticamente da complexidade \(O(N)\) de uma varredura sequencial. Atualizações, como a inserção de novos documentos, têm um custo associado ao processamento de seus termos. No projeto, usamos uma tabelas hash para mapear termos a listas de ocorrências, armazenando para cada termo: ID do documento, frequência e posições. Isso reduz o tempo de vetorização, já que os dados são pré-computados, garantindo eficiência em consultas e escalabilidade para corpora grandes.

    O índice inverso, contudo, apresenta \textit{trade-offs}. Ele consome mais memória do que uma varredura direta, devido ao armazenamento do dicionário de termos e das listas de ocorrências. A construção inicial do índice também é um processo custoso, com complexidade tipicamente linear em relação ao tamanho total do corpus \cite{citacao67}. Em corpora dinâmicos, onde documentos são adicionados ou alterados frequentemente, as atualizações podem exigir estratégias complexas de gerenciamento, como o uso de índices auxiliares ou a fusão periódica de índices incrementais, para manter o desempenho sem reconstruir toda a estrutura \cite{citacao67}.

    \subsection{\textit{Embeddings} baseados em Inteligência Artificial}

    Modelos estatísticos clássicos de recuperação de informação, como o TF-IDF e o BM25, operam em um nível lexical. Eles são extremamente eficientes para determinar a relevância de um documento com base na frequência e raridade de \textit{palavras-chave} exatas, mas falham em capturar o \textit{significado} ou a \textit{intenção} por trás da busca. Por exemplo, uma consulta por "responsabilidade civil em acidentes de trânsito" pode não retornar um documento altamente relevante que use a frase "indenização por colisão de veículos", pois os termos não coincidem. Essa limitação, conhecida como \textit{impedância semântica} (\textit{semantic mismatch}), é o principal desafio que as representações de texto baseadas em Inteligência Artificial buscam resolver.

    Para superar essa barreira, o Processamento de Linguagem Natural (NLP) evoluiu de modelos baseados em contagem para modelos baseados em \textit{predição}, que aprendem a representar palavras em um espaço vetorial contínuo. Essas representações, conhecidas como \textit{word embeddings} (ou apenas \textit{embeddings}), são vetores densos de números que capturam relações semânticas e sintáticas complexas a partir do contexto em que as palavras aparecem em grandes volumes de texto \cite{citacao53}. Em vez de apenas contar palavras, esses modelos aprendem seu significado, impulsionando uma nova geração de sistemas de busca semântica.

    \subsubsection{Processamento de Linguagem Natural (NLP)}

    O Processamento de Linguagem Natural (NLP) é uma área da inteligência artificial que tem como objetivo permitir que computadores compreendam, interpretem e gerem a linguagem humana de forma automática e eficiente. A linguagem natural, usada por pessoas no dia a dia, é cheia de ambiguidades, variações e contextos, o que torna o trabalho do NLP bastante desafiador. Para superar essas dificuldades, o NLP aplica técnicas que vão desde regras gramaticais, análise sintática, tokenização e stemming até modelos probabilísticos, aprendizado supervisionado, redes neurais e arquiteturas avançadas como \textit{transformers} \cite{citacao52}.

    Historicamente, os sistemas de NLP eram baseados em regras linguísticas definidas manualmente, o que exigia muito esforço e apresentava limitações para lidar com a complexidade da linguagem. Com o avanço do aprendizado de máquina, especialmente dos modelos de aprendizado profundo, o campo evoluiu significativamente. Modelos modernos, como os baseados em arquiteturas \textit{transformers}, têm a capacidade de capturar relações contextuais complexas em textos, o que melhorou muito o desempenho em tarefas essenciais do NLP, como tradução automática, análise de sentimentos, reconhecimento de fala, extração de informações, sumarização automática e respostas a perguntas \cite{citacao35}. Essas técnicas permitem que sistemas interpretem o significado de frases, reconheçam intenções e até gerem textos coerentes, facilitando a interação entre humanos e máquinas.

    O NLP precisa lidar com desafios como a ambiguidade das palavras(onde uma mesma palavra pode ter vários significados dependendo do contexto), a variação linguística entre diferentes regiões e falantes, e a necessidade de entender o contexto mais amplo para interpretar corretamente expressões e intenções. Além disso, o desenvolvimento de sistemas eficientes depende da disponibilidade de grandes volumes de dados anotados para treinar esses modelos, o que nem sempre é fácil para línguas menos representadas \cite{citacao52}.

    Na prática, o NLP está presente em muitas aplicações que usamos diariamente, desde assistentes virtuais como Siri e Alexa, passando por sistemas de tradução automática, \textit{chatbots} para atendimento ao cliente, até ferramentas que ajudam a analisar opiniões em redes sociais. Essa popularização faz do NLP uma área essencial para a interação entre humanos e máquinas, impulsionando a transformação digital em diversos setores \cite{citacao35}.

    Nesse contexto de evolução das técnicas de NLP, um dos avanços mais importantes para a representação da linguagem foi o desenvolvimento dos \textit{embeddings}. Eles surgiram como uma solução eficaz para transformar palavras, frases ou documentos em vetores numéricos que preservam relações semânticas e contextuais \cite{citacao52,citacao53}. Essa representação vetorial tornou-se fundamental para alimentar modelos de aprendizado de máquina, especialmente os modelos mais modernos. A seguir, serão explorados o conceito de \textit{embeddings} e os principais modelos utilizados para gerá-los.

    \subsubsection{A Evolução dos Modelos de \textit{Embeddings}}

    Os modelos para geração de \textit{embeddings} evoluíram significativamente, passando de representações estáticas para representações dinâmicas e contextuais.

    \subsubsubsection{Embeddings Estáticos: Word2Vec e GloVe}

    Os primeiros modelos que popularizaram os \textit{embeddings} foram o \textit{Word2Vec} \cite{citacao53} e o \textit{GloVe} \cite{citacao54}. O \textit{Word2Vec} é baseado em aprendizado preditivo local, utilizando duas arquiteturas principais: \textit{Skip-Gram} e \textit{Continuous Bag-of-Words} (CBOW). O modelo \textit{Skip-Gram} tenta prever as palavras de contexto (vizinhas) a partir de uma palavra central. Inversamente, o CBOW tenta prever a palavra central com base em seu contexto. Por outro lado, o \textit{GloVe} adota uma abordagem baseada em contagem, construindo vetores a partir de estatísticas globais de coocorrência de palavras em todo o corpus.

    A principal característica e limitação desses modelos é que eles geram \textit{embeddings estáticos}. Cada palavra no vocabulário é mapeada para um único vetor, independentemente do contexto em que ela é usada. Isso significa que a palavra "banco" teria a mesma representação vetorial em "sentei no banco da praça" e em "fui ao banco depositar dinheiro". Essa incapacidade de lidar com a polissemia (múltiplos significados) foi a principal motivação para o desenvolvimento da próxima geração de modelos.

    \subsubsubsection{Embeddings Contextuais: BERT e a Revolução \textit{Transformer}}

    A grande revolução nos \textit{embeddings} veio com os modelos de linguagem contextualizados, como o ELMo \cite{citacao55} e, principalmente, o BERT \cite{citacao56}. Esses modelos não atribuem um vetor fixo a uma palavra, mas geram um \textit{embedding} dinâmico para cada palavra com base na sentença completa em que ela aparece. A palavra "banco" passa a ter vetores diferentes e apropriados para cada um dos seus significados.

    Essa capacidade foi possibilitada pela arquitetura \textit{Transformer}, que introduziu um mecanismo chamado \textbf{autoatenção} (\textit{self-attention}). Diferente de modelos anteriores que liam o texto sequencialmente (da esquerda para a direita), o mecanismo de autoatenção permite que o modelo "olhe" para todas as outras palavras da sentença simultaneamente, ponderando a importância de cada uma para definir o significado da palavra atual.

    O BERT (\textit{Bidirectional Encoder Representations from Transformers}) utiliza essa arquitetura de forma "bidirecional", lendo todo o contexto de uma vez. Para alcançar essa compreensão profunda, o BERT é pré-treinado em duas tarefas principais:
    \begin{enumerate}
        \item \textbf{Masked Language Model (MLM):} O modelo recebe uma sentença onde algumas palavras foram substituídas por um \textit{token} especial `[MASK]`. A tarefa do modelo é adivinhar qual era a palavra original, forçando-o a entender o contexto gramatical e semântico de ambos os lados (esquerdo e direito) da palavra mascarada.
        \item \textbf{Next Sentence Prediction (NSP):} O modelo recebe dois segmentos de texto, A e B, e deve prever se B é a sentença que realmente se segue a A no texto original ou se é uma sentença aleatória do corpus. Isso ensina o modelo a entender a relação lógica e coesiva entre sentenças.
    \end{enumerate}

    O resultado desse pré-treinamento intensivo é um modelo capaz de gerar \textit{embeddings} contextuais de alta fidelidade, que capturam nuances da linguagem. Modelos subsequentes, como o Sentence-BERT \cite{citacao35}, foram desenvolvidos especificamente para otimizar o BERT para tarefas de comparação de sentenças, gerando representações vetoriais de frases inteiras, ideais para busca semântica.

    \subsubsection{Recuperação e Medidas de Similaridade}
% (Esta seção foi mantida como estava, pois já era muito boa)

    A recuperação de informações em contextos de linguagem natural depende da capacidade que temos de medir similaridade entre todas estas representações textuais. Com o uso de \textit{embeddings}, textos são convertidos em vetores numéricos em espaços de alta dimensão, nos permitindo aplicar técnicas matemáticas e equações para comparar conteúdos. Entre as diversas medidas de similaridade existentes, como a distância euclidiana, de \textit{Jaccard} ou de \textit{Manhattan}, a similaridade de cosseno destaca-se por sua robustez e simplicidade, sendo amplamente adotada em tarefas de busca semântica e ranqueamento de documentos. Na subseção a seguir, detalha-se seu funcionamento e aplicação no contexto de NLP e jurídico.

    \subsubsubsection{Similaridade de cosseno}

    A similaridade de cosseno é uma métrica amplamente utilizada em Processamento de Linguagem Natural (NLP) para medir o grau de similaridade entre dois vetores em um espaço vetorial multidimensional. Quando aplicada a \textit{embeddings} de palavras, frases ou documentos, essa métrica permite quantificar o quão semanticamente próximos dois textos são entre si. A ideia central é comparar a orientação dos vetores e não sua magnitude, o que torna essa medida especialmente útil para dados textuais \cite{citacao67}.

    A fórmula da similaridade de cosseno entre dois vetores ${\vec{A}}$ e ${\vec{B}}$ é dada por:

    \[
        \text{similaridade}_{\cos}(\vec{A}, \vec{B}) = \frac{\vec{A} \cdot \vec{B}}{||\vec{A}|| \cdot ||\vec{B}||}
    \]

    onde ${\vec{A} \cdot \vec{B}}$ é o produto escalar entre os vetores, e ${||\vec{A}||}$ e ${||\vec{B}|| }$ são as normas (módulos) dos respectivos vetores. O resultado varia entre -1 e 1, mas no contexto de NLP, os valores geralmente estão entre 0 (sem similaridade) e 1 (similaridade máxima), já que os vetores de \textit{embeddings} costumam estar em um espaço de dimensão positiva.

    O resultado da similaridade de cosseno varia entre -1 e 1, pois esse é o intervalo natural da função cosseno. Valores próximos de 1 indicam que os vetores estão quase na mesma direção, sugerindo alta similaridade. Se o valor estiver perto de 0, significa que os vetores são ortogonais, ou seja, não têm relação direta. Já valores próximos de -1 indicam direções opostas, o que representa dissimilaridade, como mostra a imagem abaixo em uma simplificação 2D:

    \begin{figure}[H]
        \centering
        \caption{\label{fig:vetores} O ângulo entre os vetores é diretamente proporcional à sua similaridade.}
        \includegraphics[width=\textwidth]{assets/vetores.png}
        \legend {Fonte: Elaboração própria, inspirada em \cite{citacao38}}
    \end{figure}

    A vantagem da similaridade cosseno sobre outras métricas, como a distância euclidiana, está no fato de que ela é invariável à magnitude dos vetores. Isso significa que dois vetores com direções similares, mas comprimentos diferentes (por exemplo, textos curtos vs. longos com mesmo conteúdo), ainda podem ser considerados semanticamente similares. Isso é particularmente útil em tarefas de busca semântica e recuperação de informações, onde a intenção é encontrar conteúdos com sentido próximo, independentemente da sua extensão textual \cite{citacao67}.

    Na prática, ao aplicar essa métrica, os textos são primeiro convertidos em vetores numéricos usando técnicas de \textit{embeddings} (como Word2Vec, BERT ou Sentence-BERT), e depois esses vetores são comparados utilizando a similaridade cosseno para ranquear ou agrupar documentos. Essa abordagem é a base de diversos sistemas modernos de busca jurídica, recomendação de jurisprudência e agrupamento de decisões semelhantes \cite{citacao35}.

    É importante ressaltar que o cosseno não capta relações mais complexas como ironia, polissemia ou implicaturas contextuais, para esses casos, a escolha do modelo de \textit{embedding} (estático ou contextualizado) é determinante para que os vetores usados na comparação contenham essas nuances semânticas. Ainda assim, a similaridade cosseno continua sendo uma escolha padrão e eficaz em aplicações de NLP por sua simplicidade, desempenho e capacidade de generalização \cite{citacao10}.


    \section{Algoritmos}

    Algoritmos são conjuntos finitos de instruções bem definidas, ordenadas e executáveis que visam resolver um problema ou realizar uma tarefa específica. Conforme definido por Cormen e outros. \cite{citacao26}, “um algoritmo é qualquer procedimento computacional bem definido que toma algum valor ou conjunto de valores como entrada e produz algum valor ou conjunto de valores como saída”. Segundo o dicionário Michaelis \cite{citacao27}, algoritmo é “Conjunto de regras e operações e procedimentos, definidos e ordenados usados na solução de um problema”. Dessa forma, os algoritmos constituem a base do raciocínio computacional e do desenvolvimento de software, estruturando a lógica para a execução de tarefas de maneira clara e eficiente.

    \subsection{Avaliação de algoritmos: Coeficiente de \textit{Spearman}}

    Neste trabalho, após ordenarmos os documentos do mais ao menos importante com base nos algoritmos analisados, é fundamental dispor de uma métrica que nos permita comparar essas listas. Essa comparação é essencial para medir a consistência dos algoritmos e avaliar quão semelhantes são os rankings que eles produzem \cite{citacao71}. Para isso, utilizaremos o coeficiente de correlação de \textit{Spearman}, uma técnica estatística não paramétrica que quantifica a força e a direção da relação entre duas variáveis ordenadas \cite{citacao70}.

    Para entender o coeficiente de \textit{Spearman}, é útil primeiro diferenciá-lo do coeficiente de correlação de \textit{Pearson}. O \textit{Pearson} é uma medida estatística que indica a força e a direção da relação \textit{linear} entre duas variáveis contínuas. Ele avalia se os pontos de dados se aproximam de uma linha reta, mas é sensível a \textit{outliers} e assume que os dados seguem uma distribuição normal \cite{citacao70}.

    O \textit{Pearson} avalia diretamente os valores dos \textit{scores} de relevância, o que não é ideal para o nosso problema. Não nos importa se o \textit{score} do BM25 foi 0.8 e o do \textit{embedding} foi 0.9; o que nos importa é se ambos concordam que aquele documento é o "primeiro mais relevante". É justamente nesse cenário que o coeficiente de \textit{Spearman} se torna mais apropriado. Ele não olha para os \textit{scores} brutos, mas sim para os \textit{postos} (ou \textit{ranks}) dos elementos, medindo a correlação monotônica entre eles \cite{citacao70, citacao71}.

    Na prática, o coeficiente de \textit{Spearman} é simplesmente definido como o \textbf{coeficiente de correlação de \textit{Pearson} calculado sobre os postos das variáveis}. O processo de cálculo envolve transformar os \textit{scores} de cada lista em uma sequência de postos (1º, 2º, 3º, ...). Caso ocorram empates (por exemplo, dois documentos com o mesmo \textit{score}), atribui-se a eles a média de suas posições. Por exemplo, se dois documentos empatam nas posições 2 e 3, ambos recebem o posto 2,5. Após a transformação, o coeficiente de \textit{Pearson} é aplicado sobre essas duas novas listas de postos \cite{citacao70}.

    O resultado, assim como o \textit{Pearson}, varia de -1 a +1. Valores próximos de +1 indicam uma forte concordância (os algoritmos produziram rankings muito semelhantes). Valores próximos de -1 indicam uma forte discordância (um algoritmo ranqueou na ordem inversa do outro). Valores próximos de 0 apontam para uma ausência de correlação, sugerindo que os rankings são aleatórios um em relação ao outro \cite{citacao70}.

    No contexto deste trabalho, o coeficiente de \textit{Spearman} fornecerá uma medida objetiva do grau de concordância entre as listas produzidas pelas abordagens estatísticas (TF-IDF, BM25) e semânticas (\textit{embeddings}). Conforme demonstrado em \citeonline{citacao71}, que utilizou essa métrica para comparar algoritmos de ranqueamento textual, o \textit{Spearman} é uma ferramenta eficaz para avaliar a correlação entre os resultados de diferentes sistemas de busca.

    \begin{figure}[H]
        \centering
        \caption{\label{fig:SpearmanCalc}Demonstraçao do coeficiente de \textit{Spearman} aplicado à sequência de cores}
        \includegraphics[width=0.8\textwidth]{assets/SpearmanCalc}
        \legend {Fonte: Elaboração própria.}
    \end{figure}


    \section{Programação Paralela e Escalabilidade}

    A programação paralela vem como uma das soluções propostas para os desafios modernos da computação. A ideia por trás dela foi impulsionada pela necessidade de otimizar o tempo de processamento através algoritmos eficientes para criar aplicações que sejam capazes de lidar com um grande volume de dados e cálculos intensivos \cite{citacao12}, coisa que os algoritmos sequenciais não são tão adequados.

    \subsection{Algoritmos Sequenciais e Limitações}

    Algoritmos sequenciais representam o início da computação, onde processadores e algoritmos tinham a premissa da restrição a ordem das instruções, ou seja, uma operação de cada vez, o fim de uma representa o início da próxima. Seguindo assim uma sequência lógica e clara \cite{citacao13} como é representado na imagem abaixo.

    \begin{figure}[H]
        \centering
        \caption{\label{fig:AlgSequenciais}Algoritmos sequenciais}
        \includegraphics[width=\textwidth]{assets/Algoritmos sequenciais.png}
        \legend {Fonte: Elaboração própria.}
    \end{figure}

    \subsection{Algoritmos Paralelos: Modelos e Bibliotecas}

    Algoritmos paralelos são a forma de programação que se utiliza de arquiteturas paralelas e suas características para que várias operações e instruções possam ser executadas simultaneamente dentro da estrutura, podendo esta ser, \textit{multicore}, multiprocessador ou uma rede de computadores (cluster) \cite{citacao14}.

    \begin{figure}[H]
        \centering
        \caption{\label{fig:AlgParalelos}Algoritmos paralelos}
        \includegraphics[width=\textwidth]{assets/Algoritmos paralelos.png}
        \legend {Fonte: Elaboração própria.}
    \end{figure}

    Com o avanço da computação, da engenharia de software, o surgimento de várias arquiteturas \textit{multicore} e sistemas distribuídos, a paralelização dentro dessas estruturas nos permite dividir tarefas complexas em subtarefas executadas simultaneamente, maximizando a eficiência e o desempenho. Contudo, o desenvolvimento de algoritmos paralelos exige lidar com questões como sincronização, concorrência e balanceamento de carga, tornando essa prática essencial para atender às demandas de rapidez e escalabilidade atualmente \cite{citacao13}.


    \section{Trabalhos Relacionados}

    \subsection{Comparação de Desempenho entre TF-IDF e Okapi BM25}

    O trabalho influente de \citeonline{robertson1995okapi} apresenta os resultados do sistema Okapi na TREC-3 e acaba servindo como uma das principais evidências empíricas da superioridade do BM25 sobre o TF-IDF tradicional. Os autores não buscavam criar um sistema novo, mas sim avaliar e aprimorar um modelo probabilístico de recuperação em um ambiente de testes rigoroso.

    A metodologia consistiu em submeter o Okapi ao corpus padrão da TREC usando consultas de tópicos complexos cuja relevância já era conhecida. A eficácia dos algoritmos foi medida por métricas como Mean Average Precision e Precision@k, que verificam o quanto a ordenação dos resultados se aproxima do ideal.

    O TF-IDF aparece como linha de base do estudo. Sua limitação mais clara é a normalização insuficiente em relação ao tamanho dos documentos. Em conjuntos muito variados, como o jurídico, textos longos acabam favorecidos, já que têm mais chances de repetir os termos da consulta e inflar artificialmente sua pontuação.

    O Okapi BM25 é apresentado como a resposta probabilística a esse problema, usando dois parâmetros de calibração. O primeiro é o $k_1$, que controla a saturação da frequência do termo, evitando que aumentos repetitivos elevem a relevância de forma linear. O segundo é o $b$, que ajusta o efeito do comprimento do documento comparando seu tamanho com a média do corpus. Documentos muito maiores que a média recebem uma penalização que reduz ocorrências casuais.

    Os resultados apresentados mostram uma vantagem consistente do BM25 sobre o TF-IDF em métricas de ordenação como a nDCG. A conexão com este TCC é direta, já que o estudo reforça empiricamente a escolha do BM25 como principal método estatístico usado na análise e o coloca como o modelo lexical mais consolidado para comparação com abordagens semânticas.

    \subsection{Arquiteturas Preditivas para Representação Vetorial de Palavras}

    O trabalho de \citeonline{mikolov2013efficient}, intitulado “Efficient Estimation of Word Representations in Vector Space”, introduz uma nova forma de representar palavras que estabeleceu a base da busca semântica moderna. O estudo questiona os modelos baseados em contagem, como o TF-IDF, que tratam palavras como unidades independentes e esparsas, e apresenta o toolkit Word2Vec. O objetivo é aprender vetores densos capazes de refletir o significado da palavra a partir de seu contexto em um corpus massivo.

    Para alcançar esse resultado, os autores propõem duas arquiteturas de redes neurais rasas que resolvem tarefas opostas, CBOW e Skip-Gram.

    A arquitetura CBOW prevê a palavra central a partir do contexto. O modelo recebe uma janela de palavras vizinhas, projeta seus vetores e calcula uma média que forma o vetor de contexto. A rede então tenta adivinhar a palavra omitida. Por trabalhar com médias, o CBOW costuma ser rápido e eficiente no aprendizado de relações sintáticas.

    A arquitetura Skip-Gram faz o movimento inverso e tende a ser mais forte semanticamente. O modelo recebe uma única palavra e tenta prever as palavras ao seu redor, realizando várias predições por amostra. Essa abordagem é mais lenta no treino, mas o estudo mostra que ela captura nuances semânticas com mais precisão, especialmente em palavras raras ou termos técnicos. Esse comportamento é valioso em áreas como o direito, onde existe grande quantidade de expressões pouco frequentes.

    Os autores concluem que existe um equilíbrio claro entre velocidade e qualidade. O CBOW é mais rápido, enquanto o Skip-Gram é mais robusto semanticamente. A relevância desse estudo para o TCC é essencial, pois ele demonstra por que embeddings são superiores a modelos de contagem para representar significado e estabelece as bases conceituais que depois permitiram o surgimento dos modelos transformers usados neste projeto.

% --- CAPÍTULO 3: DESENVOLVIMENTO ---


    \chapter{Desenvolvimento}

    O desenvolvimento da solução foi guiado principalmente por requisitos de desempenho e escalabilidade, já que o sistema precisava lidar com um grande volume de documentos jurídicos de forma eficiente. Em vez de seguir uma arquitetura baseada em microsserviços, a aplicação foi estruturada em módulos bem definidos, onde cada um possui sua própria responsabilidade dentro do fluxo geral do sistema. Esses módulos não se comunicam por meio de APIs ou chamadas diretas, sendo conectados apenas por manipulação humana entre as etapas, o que simplifica a integração e o controle do processo.

    Embora duas linguagens tenham sido utilizadas, o núcleo da aplicação foi desenvolvido em Go. Toda a parte de coleta de dados, assim como a lógica de orquestração e indexação dos documentos, foi implementada nessa linguagem. A escolha pelo Go se deu tanto pela preferência dos desenvolvedores quanto por suas características técnicas, já que é uma linguagem com um nível ergonômico muito alto e, ao mesmo tempo, excelente desempenho computacional. Essa combinação de praticidade e velocidade foi decisiva para o projeto.

    O Python foi empregado apenas em um ponto específico do processo, na geração das \textit{embeddings} e na ordenação dos documentos de acordo com o nível de similaridade semântica. Essa etapa fazia uso de bibliotecas consolidadas de Inteligência Artificial e Processamento de Linguagem Natural, que se encaixavam melhor no ecossistema Python.

    A seguir são apresentadas as principais tecnologias utilizadas, as estratégias adotadas na coleta de dados, o fluxo de limpeza e padronização textual e a forma como os algoritmos de busca foram implementados e integrados ao restante do sistema.


    \section{Aquisição de Dados}

    A construção do \textit{corpus} documental baseou-se na coleta massiva de textos legislativos diretamente do Portal da Câmara dos Deputados. O objetivo foi compilar uma base de dados robusta e representativa, totalizando aproximadamente 5.000 documentos, abrangendo principalmente PLs e PECs.

    Dada a magnitude do volume de dados, uma abordagem sequencial seria inviável devido à latência de rede. Portanto, a solução foi implementada em Go, localizada no pacote \texttt{corpus}, explorando o modelo de concorrência da linguagem (\textit{goroutines}) para realizar o \textit{download} dos documentos de forma paralela e assíncrona.

    Para a extração das informações, utilizou-se a biblioteca \texttt{goquery}. Esta ferramenta permite realizar o \textit{parsing} das páginas HTML retornadas pelo portal e navegar pela árvore DOM utilizando seletores CSS, facilitando a identificação precisa dos \textit{links} para os arquivos PDF em meio à estrutura complexa do site da Câmara.

    O fluxo de coleta foi estruturado da seguinte forma:

    \begin{enumerate}
        \item \textbf{Definição do Escopo:} O algoritmo inicia o processo varrendo as páginas de resultados do portal com base nas categorias legislativas de interesse, identificando os documentos disponíveis para coleta.
        \item \textbf{Execução Paralela e \textit{Rate Limiting}:} O sistema dispara múltiplas requisições simultâneas para maximizar o uso da banda. Contudo, para evitar que o servidor de origem interprete o tráfego intenso como um ataque de Negação de Serviço (DoS) ou bloqueie o endereço IP da aplicação por excesso de requisições (\textit{throttling}), foi implementado um intervalo de segurança (\textit{delay}) fixo de 500 milissegundos entre as chamadas de rede. Essa estratégia garante a estabilidade do processo de coleta e respeita as políticas de uso do portal governamental.
        \item \textbf{Extração de Links:} O conteúdo HTML de cada página processada é analisado pelo \texttt{goquery}, que filtra e extrai apenas as URLs que apontam para o inteiro teor dos documentos (arquivos com extensão \texttt{.pdf}).
        \item \textbf{Download e Persistência:} Os arquivos identificados são baixados via \textit{stream} e salvos diretamente no diretório local \texttt{./misc/corpus/pdf}, preservando a nomenclatura original para garantir a rastreabilidade dos dados.
    \end{enumerate}


    \section{Pré-processamento e Normalização Textual}

    Após a etapa de aquisição, o \textit{corpus} bruto constitui-se de milhares de arquivos em formato PDF. A utilização direta desses arquivos para fins de indexação ou vetorização é inviável devido à presença de formatação binária e, principalmente, de "ruído textual", elementos que, embora necessários para a validade jurídica do documento (como cabeçalhos, numeração de autos, datas e assinaturas), não contribuem para a identificação do conteúdo semântico da matéria legislativa.

    Para mitigar esse problema, foi implementado um módulo robusto de processamento textual em Go, localizado no pacote \texttt{corpus}. Este módulo opera como um \textit{pipeline} de transformação, convertendo documentos não estruturados em sequências de \textit{tokens} normalizados.

    \subsection{Arquitetura de Processamento Concorrente}

    Dada a alta carga de I/O exigida para a leitura e conversão de milhares de arquivos PDF, a implementação não seguiu uma abordagem sequencial. Em vez disso, utilizou-se o padrão de projeto \textit{Worker Pool} nativo da linguagem Go.

    A função orquestradora, \texttt{TextProcessor}, define um limite de concorrência controlado pela variável \texttt{maxWorkers}. O funcionamento baseia-se em três primitivas de sincronização:
    \begin{itemize}
        \item \textbf{Channels (Canais):} Um canal buferizado (\texttt{workerLimit}) atua como um semáforo, bloqueando a criação de novas \textit{goroutines} quando o limite de processos simultâneos é atingido. Isso impede a exaustão de recursos do sistema operacional, como descritores de arquivos e memória RAM.
        \item \textbf{Goroutines:} Cada documento é processado em sua própria \textit{thread} leve, permitindo que enquanto um arquivo aguarda a operação de disco, a CPU possa processar a limpeza textual de outro.
        \item \textbf{WaitGroup:} O mecanismo \texttt{sync.WaitGroup} assegura que o processo principal aguarde a conclusão de todas as tarefas de limpeza antes de encerrar a execução.
    \end{itemize}

    \subsection{Extração de Texto}

    A primeira etapa do \textit{pipeline} é a extração do conteúdo textual. Devido à complexidade do \textit{layout} de documentos legislativos (frequentemente diagramados em colunas ou contendo quebras de página irregulares), bibliotecas nativas de leitura de PDF muitas vezes falham em manter a ordem lógica do texto.

    Para garantir a fidelidade da extração, o sistema realiza uma chamada de sistema para a ferramenta externa \texttt{pdftotext} (parte do pacote \textit{poppler-utils}). A função \texttt{processPDF} executa este comando via \texttt{os/exec}, direcionando o fluxo de texto extraído diretamente para a memória da aplicação, evitando escritas intermediárias desnecessárias em disco nesta fase.

    \subsection{Pipeline de higienização}

    O texto bruto é processado pela função \texttt{CleanText}, no pacote \texttt{utils}. A função aplica uma sequência de transformações destrutivas e normalizadoras pensadas para documentos jurídicos.

    \subsubsection{Minúsculas}

    Todo o conteúdo é convertido para letras minúsculas com \texttt{strings.ToLower}, reduzindo variação do vocabulário e unificando termos como “LEI”, “Lei” e “lei”.

    \subsubsection{Remoção por regex}

    Usamos expressões regulares para identificar e remover padrões que não contribuem para a busca por assunto:
    \begin{enumerate}
        \item \textbf{Numerais romanos:} elementos como “Inciso IV” ou “Capítulo XX” são removidos, já que a posição numérica não ajuda na similaridade semântica.
        \item \textbf{URLs e links:} endereços (\texttt{https?://...}) são eliminados para evitar indexar rodapés e metadados.
        \item \textbf{Datas e números:} datas e valores numéricos são retirados, pois o foco está na similaridade temática e não em dados específicos.
    \end{enumerate}

    \subsubsection{Normalização de pontuação e caracteres}

    O texto é percorrido caractere a caractere, removendo pontuação e símbolos com \texttt{unicode.IsPunct} e \texttt{unicode.IsSymbol}. Hífens, travessões e \texttt{underscores} viram espaços para evitar palavras unidas.
    A normalização de acentos usa um mapa carregado de \texttt{misc/replaces.json}, convertendo caracteres estendidos para equivalentes ASCII, o que reduz problemas de codificação e digitação.

    \subsubsection{Remoção de stopwords}

    Por fim, a biblioteca \texttt{bbalet/stopwords} remove palavras funcionais do português, evitando que conectivos e artigos aumentem artificialmente a similaridade entre documentos e afetem TF-IDF ou BM25.

    O resultado é um arquivo \texttt{\_clean.txt} contendo apenas os termos relevantes, pronto para indexação e vetorização.

    \subsection{Geração de N-Gramas e Skip-Grams}

    Uma vez que o conteúdo textual foi higienizado e normalizado, o documento deixa de ser tratado como uma \textit{string} única e passa a ser processado como uma sequência vetorial de \textit{tokens}. A etapa subsequente, e uma das mais críticas para a qualidade da indexação, é a segmentação dessa sequência em unidades de informação indexáveis, conhecidas como N-gramas.

    Para realizar essa tarefa, foi implementada a função utilitária \texttt{GetGramsLim}, localizada no pacote \texttt{utils}. Diferente de implementações genéricas que utilizam recursão para gerar combinações de qualquer tamanho, o que poderia ocasionar estouro de pilha ou uso excessivo de memória em textos muito longos, optou-se por uma abordagem iterativa e explícita, otimizada especificamente para os limites operacionais do projeto: Unigramas, Bigramas e Trigramas, com uma distância máxima de dois saltos (\textit{jumps}) entre as palavras.

    A função foi desenvolvida utilizando o recurso de \textit{Generics} do Go (\texttt{[T any]}), o que confere flexibilidade arquitetural ao sistema: o mesmo algoritmo é capaz de processar vetores de \textit{strings} (durante a fase de testes rápidos) ou vetores de inteiros (\texttt{uint16}), que representam os IDs das palavras no banco de dados de produção.

    A lógica de segmentação varia conforme a cardinalidade do N-grama desejado:

    \begin{enumerate}
        \item \textbf{Unigramas ($N=1$):} O tratamento é trivial e linear. O algoritmo percorre o vetor de entrada de $0$ a $N$, encapsulando cada elemento individual como um grama independente.

        \item \textbf{Bigramas com Saltos ($N=2$):} Para capturar relações entre palavras que não são necessariamente adjacentes (conceito de \textit{skip-gram}), o algoritmo emprega um laço aninhado. Enquanto o índice principal $i$ percorre as palavras do documento, um índice secundário $j$ projeta-se à frente, variando de $1$ até o limite configurado de \textit{jumps} ($jump+1$). Isso permite gerar pares como $(w_i, w_{i+1})$ (adjacente) ou $(w_i, w_{i+2})$ (com um salto), armazenando explicitamente a distância vetorial ($j$) entre os termos para fins de normalização posterior.

        \item \textbf{Trigramas ($N=3$):} A complexidade aumenta para capturar o contexto estendido. Utiliza-se uma estrutura de três laços encadeados ($i, j, k$), onde $i$ é a palavra pivô, $j$ define a distância para o segundo termo e $k$ define a distância do segundo para o terceiro. Essa abordagem "força bruta controlada" garante que todas as combinações válidas dentro da janela de saltos (como $(w_i, w_{i+1}, w_{i+2})$ ou combinações mais esparsas como $(w_i, w_{i+2}, w_{i+4})$) sejam geradas deterministicamente, sem a sobrecarga de chamadas recursivas.
    \end{enumerate}

    O retorno da função é uma estrutura dupla contendo tanto os gramas gerados quanto uma matriz de metadados (\texttt{retJumps}), que registra as distâncias relativas (\texttt{int8}) entre cada componente do grama. Esses metadados são essenciais para a estratégia de indexação, pois permitem ao motor de busca diferenciar, por exemplo, uma citação direta de uma menção espaçada no texto.


    \section{Modelagem de Dados e Persistência}

    A eficiência de um sistema de recuperação de informação depende intrinsecamente de como os dados são estruturados e persistidos. Para este projeto, optou-se por uma abordagem híbrida que combina a rigidez de um esquema relacional para manter a integridade dos documentos com a flexibilidade de estruturas de índice invertido para a busca rápida.

    A camada de persistência foi construída sobre o banco de dados SQLite, utilizando o GORM para a manipulação das entidades em Go. A escolha do SQLite justifica-se pela sua arquitetura \textit{serverless} de arquivo único, eliminando a latência de rede típica de conexões cliente-servidor (como em PostgreSQL ou MySQL) e permitindo um desempenho de leitura extremamente elevado quando o arquivo do banco reside em discos SSD NVMe modernos.

    \subsection{Definição das Estruturas de Dados (Structs)}

    A modelagem orientada a objetos do sistema reflete-se nas \textit{structs} definidas no pacote \texttt{models}. Cada \textit{struct} mapeia diretamente para uma tabela no banco de dados, seguindo as convenções do GORM.

    \subsubsection{Entidade Documento e Estratégia de Armazenamento}

    A unidade fundamental do \textit{corpus} é representada pela \textit{struct} \texttt{Document}, definida no arquivo \texttt{models/Document.go}. Contudo, diferentemente de sistemas tradicionais que armazenam o conteúdo completo (BLOBs ou Textos Longos) no banco de dados, este projeto adota uma abordagem híbrida focada em performance.

    \begin{verbatim}
type Document struct {
    gorm.Model
    Title        string
    Link         string
    CleanTitle   string
    // Content e CleanContent são processados em memória ou lidos do disco
}
    \end{verbatim}

    Para evitar o crescimento exponencial do arquivo do banco de dados SQLite, o que degradaria o tempo de resposta das consultas de índice, o conteúdo textual integral dos documentos (\textit{Full Text}) \textbf{não é persistido no banco de dados} de forma permanente.

    A estratégia adotada consiste em:
    \begin{enumerate}
        \item \textbf{Persistência em Disco:} Os documentos originais e suas versões higienizadas (\texttt{.txt}) são salvos no sistema de arquivos local (\textit{File System}) da máquina de execução. Isso aproveita a velocidade de leitura sequencial do SSD/HDD e libera o banco de dados de gerenciar grandes volumes de dados não estruturados.
        \item \textbf{Banco como Índice de Metadados:} A tabela \texttt{documents} no SQLite armazena apenas os metadados essenciais (Título, Link original, IDs de controle) e atua como um catálogo que referencia os arquivos em disco.
        \item \textbf{Leitura sob Demanda (Lazy Loading):} Durante a execução dos testes de similaridade, o sistema carrega o conteúdo necessário diretamente dos arquivos para a memória RAM apenas para o cálculo dos vetores e geração dos N-gramas. Uma vez indexado o documento (geradas as tabelas de Unigramas/Trigramas), o texto bruto é descartado da memória, mantendo o \textit{footprint} de RAM do processo eficiente.
    \end{enumerate}

    Essa decisão arquitetural garante que o banco de dados permaneça compacto, contendo apenas as estruturas otimizadas de busca (Índice Invertido), enquanto o armazenamento "pesado" é delegado ao sistema operacional.

    \subsection{Dicionário de Palavras}

    Para otimizar o armazenamento e as comparações, as palavras não são repetidas nas tabelas de índice. Em vez disso, utiliza-se uma tabela de vocabulário único, representada pela \textit{struct} \texttt{Word}.

    \begin{verbatim}
type Word struct {
    gorm.Model
    Word string `gorm:"uniqueIndex"`
}
    \end{verbatim}

    A \textit{tag} \texttt{uniqueIndex} na coluna \texttt{Word} é crucial: ela garante que cada termo apareça apenas uma vez no banco de dados e cria um índice B-Tree físico no SQLite, permitindo que a conversão de uma \textit{string} para seu \texttt{WordID} ocorra em tempo logarítmico $O(\log N)$.

    \subsection{Estrutura do Índice Invertido e Polimorfismo}

    A implementação do índice invertido foge à abordagem trivial de mapear "Palavra $\rightarrow$ Documentos". Para capturar o contexto local e permitir buscas por frases e proximidade, o sistema modela o índice através de três entidades distintas: \textbf{Unigramas}, \textbf{Bigramas} e \textbf{Trigramas}.

    Para que os algoritmos de cálculo de relevância (TF-IDF e BM25) pudessem operar de forma agnóstica em relação ao tamanho do n-grama, definiu-se a interface polimórfica \texttt{IGram}, localizada no pacote \texttt{models/interfaces}.

    \begin{verbatim}
type IGram interface {
    GetCacheKey(jumps, doc bool) string
    GetDocId() uint16
    Increment()
    GetCount() int
    ApplyWordWheres(db *gorm.DB) *gorm.DB
    ApplyJumpWheres(db *gorm.DB) *gorm.DB
    schema.Tabler
}
    \end{verbatim}

    Esta interface é crucial para a arquitetura do sistema, pois permite abstrair a complexidade das consultas SQL. Métodos como \texttt{ApplyWordWheres} e \texttt{ApplyJumpWheres} encapsulam a lógica de filtragem do banco de dados, permitindo que uma função de busca receba um \texttt{IGram} genérico e construa a \textit{query} correta dinamicamente, seja ela para um termo único ou uma frase de três palavras.

    A materialização dessa interface ocorre nas \textit{structs} concretas. A estrutura de um trigrama, denominada \texttt{InverseTrigram}, exemplifica a otimização de memória aplicada:

    \begin{verbatim}
type InverseTrigram struct {
    Wd0Id uint16 `gorm:"uniqueIndex:compositeindex"`
    Wd1Id uint16 `gorm:"uniqueIndex:compositeindex"`
    Wd2Id uint16 `gorm:"uniqueIndex:compositeindex"`
    DocId uint16 `gorm:"uniqueIndex:compositeindex"`
    Jump0 int8   `gorm:"uniqueIndex:compositeindex"`
    Jump1 int8   `gorm:"uniqueIndex:compositeindex"`
    Count int
}
    \end{verbatim}

    Nesta modelagem, destacam-se decisões de \textit{design} focadas em integridade e performance:

    \begin{itemize}
        \item \textbf{Índice Composto (Composite Index):} Conforme observado nas \textit{tags} do GORM (\texttt{uniqueIndex:compositeindex}), os seis primeiros atributos (as três palavras, o documento e as duas distâncias) formam, em conjunto, uma chave única composta. Isso impede a duplicação de registros para a mesma ocorrência semântica e cria um índice B-Tree otimizado no SQLite, acelerando drasticamente as operações de leitura.

        \item \textbf{Chaves Estrangeiras Virtuais Leves:} Os campos \texttt{Wd...Id} referenciam a tabela de vocabulário, mas armazenam apenas inteiros sem sinal de 16 bits (\texttt{uint16}). Isso limita o vocabulário a 65.535 termos únicos (suficiente após a limpeza agressiva) e o \textit{corpus} a 65.535 documentos, mas garante que cada linha do índice ocupe uma fração mínima de espaço em disco comparado ao armazenamento de \textit{strings}.

        \item \textbf{Conceito de "Jumps" (\textit{Skip-Grams}):} Diferente de implementações simples que exigem adjacência estrita, as estruturas \texttt{Bigram} e \texttt{Trigram} utilizam inteiros de 8 bits (\texttt{int8}) para armazenar a distância entre as palavras (\texttt{Jump}). Isso permite indexar padrões não contíguos (ex: "recurso \textit{de} apelação" pode ser capturado como um bigrama "recurso apelação" com \texttt{Jump=1}), aumentando a flexibilidade semântica da busca exata sem perder a precisão da ordem dos termos.
    \end{itemize}

    \subsection{Estratégia de Isolamento e Duplicação}

    Para garantir a integridade dos testes comparativos e evitar que a execução de um cenário influenciasse os resultados de outro (por exemplo, cache de páginas do sistema operacional ou fragmentação de índices), implementou-se um mecanismo de duplicação de banco de dados sob demanda.

    Antes de iniciar uma bateria de testes, o sistema verifica a necessidade de criar um ambiente limpo ou reutilizar dados pré-processados. Caso a \textit{flag} \texttt{fromScratch} seja falsa, a função \texttt{InitDB} invoca \texttt{DuplicateFile} para criar uma cópia física do arquivo do banco de dados mestre (contendo o \textit{corpus} limpo) para um novo arquivo identificado pelo ID da execução atual (ex: \texttt{corpus\_12345.db}).

    Essa abordagem de "Sandbox por Processo" elimina a concorrência de escrita no nível do arquivo do banco de dados e permite que múltiplos testes rodem em paralelo ou em sequência sem condições de corrida e com isolamento total de transações.

    \subsection{Otimização por Esquema Único (Single-Gram Schema)}

    Uma decisão arquitetural crítica para a performance do sistema foi a restrição de armazenar \textbf{apenas um tipo de n-grama por instância de banco de dados}.

    Embora fosse possível criar tabelas distintas para Unigramas, Bigramas e Trigramas coexistindo no mesmo banco, isso implicaria em um aumento desnecessário do tamanho do arquivo e na dispersão das páginas de dados no disco, prejudicando a localidade de referência e o desempenho do cache do SQLite.

    Para mitigar isso, a função \texttt{InitDB} utiliza uma estrutura de decisão (\texttt{switch gramSize}) para definir dinamicamente qual modelo será migrado para o banco de dados daquela execução específica:

    \begin{verbatim}
switch gramSize {
case 1:
    gramModel = &models.InverseUnigram{}
    index = "(wd0Id)"
case 2:
    gramModel = &models.InverseBigram{}
    index = "(wd0Id, wd1Id)"
case 3:
    gramModel = &models.InverseTrigram{}
    index = "(wd0Id, wd1Id, wd2Id)"
}
    \end{verbatim}

    Dessa forma, se um teste avalia a performance de Trigramas, o banco de dados é instanciado contendo apenas a tabela \texttt{InverseTrigram}. Isso garante que a tabela de índices (\texttt{WORD\_DOC}) seja extremamente compacta e otimizada exclusivamente para as consultas daquele tamanho de grama, eliminando o \textit{overhead} de manutenção de índices não utilizados.

    \subsection{Indexação Dinâmica}

    Complementando a estratégia de esquema único, a criação de índices também é realizada dinamicamente via execução de SQL puro no momento da inicialização. O sistema executa o comando \texttt{CREATE INDEX} ajustado para as colunas específicas do n-grama selecionado (conforme a variável \texttt{index} definida no trecho de código acima).

    Isso assegura que as consultas de busca (\textit{SELECT}), que realizam junções pesadas entre o vocabulário e a tabela de ocorrências, sempre utilizem um índice de cobertura (\textit{Covering Index}) ou um índice B-Tree otimizado, reduzindo a complexidade da busca de $O(N)$ para $O(\log N)$.


    \section{Implementação dos Algoritmos de Busca e Recuperação}
    \label{sec:algoritmos_busca}

    A lógica de recuperação de informação representa o núcleo intelectual do sistema. Enquanto a maioria das soluções comerciais delega essa tarefa para motores de busca prontos (como Elasticsearch ou Solr), neste trabalho optou-se pela implementação manual ("\textit{from scratch}") dos algoritmos TF-IDF e BM25 em Go. Esta decisão permitiu um controle granular sobre a ponderação dos termos e a otimização do uso de memória através de concorrência.

    \subsection{Pipeline Estatístico: TF-IDF}

    O algoritmo TF-IDF (\textit{Term Frequency - Inverse Document Frequency}) foi implementado no pacote \texttt{utils}, especificamente no arquivo \texttt{tf\_idf.go}. A função principal, \texttt{ComputeDocPreIndexedTFIDF}, calcula o vetor de pesos para um documento dado um conjunto de N-gramas.

    A implementação matemática segue a fórmula clássica com suavização:
    \[
        \text{TF-IDF}(t, d) = \left( \frac{f_{t,d}}{\sum_{t' \in d} f_{t',d}} \right) \times \log \left( \frac{N}{1 + df_t} \right)
    \]

    O código implementa esta lógica em duas fases distintas para maximizar a eficiência:

    \begin{enumerate}
        \item \textbf{Cálculo do TF (Local):} O algoritmo itera sobre a lista de trigramas (\texttt{trigramList}) do documento, acumulando a frequência bruta de cada termo em um mapa em memória (\texttt{tf}).
        \item \textbf{Cálculo do DF (Global) Concorrente:} A etapa mais custosa é determinar em quantos documentos do \textit{corpus} cada termo aparece ($df_t$). Para evitar que essa operação bloqueie o processamento, utilizou-se o padrão de \textit{fan-out/fan-in} com \textit{goroutines}.
    \end{enumerate}

    \textbf{Otimização de Concorrência:}
    O trecho de código abaixo ilustra como o paralelismo foi controlado para evitar sobrecarga no banco de dados:

    \begin{verbatim}
sem := make(chan struct{}, 25) // Semáforo de capacidade 25
var wg sync.WaitGroup

for key := range tf {
    wg.Add(1)
    sem <- struct{}{} // Adquire token
    go func(key string) {
        defer wg.Done()
        defer func() { <-sem }() // Libera token
        df := len(cacheN[key])   // Acesso thread-safe ao cache
        dfChan <- dfResult{key: key, df: df}
    }(key)
}
    \end{verbatim}

    Um canal bufferizado (\texttt{sem}) atua como um semáforo limitador, garantindo que nunca haja mais de 25 \textit{goroutines} acessando o cache ou o banco de dados simultaneamente. Isso estabiliza o \textit{throughput} da CPU e previne condições de corrida em ambientes de alta carga.

    \subsection{Pipeline Probabilístico: Okapi BM25}

    Reconhecido como o estado da arte para buscas baseadas em palavras-chave, o algoritmo BM25 foi implementado no arquivo \texttt{bm25.go}. Diferente do TF-IDF, o BM25 normaliza a frequência do termo pelo tamanho do documento, penalizando textos excessivamente longos que poderiam ter altas contagens de palavras apenas por serem verbosos.

    A função \texttt{ComputeDocPosIndexedBM25} implementa a equação:

    \[
        \text{Score}(D,Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})}
    \]

    \textbf{Parametrização e Ajuste Fino:}
    Os hiperparâmetros do algoritmo foram fixados diretamente no código fonte, seguindo valores recomendados pela literatura de Recuperação de Informação para domínios gerais, mas aplicáveis ao jurídico:
    \begin{itemize}
        \item \textbf{$k_1 = 1.5$:} Controla a saturação da frequência do termo. Indica o quão rapidamente a relevância aumenta com repetições da mesma palavra.
        \item \textbf{$b = 0.75$:} Controla a intensidade da normalização pelo comprimento do documento. Um valor de 0.75 aplica uma penalização moderada a documentos longos.
    \end{itemize}

    Para calcular o comprimento médio dos documentos (\texttt{avgDL}), essencial para o denominador da equação, o sistema executa uma consulta de agregação otimizada no banco de dados SQLite (\texttt{SELECT SUM(count) FROM WORD\_DOC}), aproveitando os índices criados na etapa de modelagem.

    \subsection{Pipeline Semântico: Vetorização via Transformers}

    Enquanto os algoritmos TF-IDF e BM25, implementados no núcleo em Go, operam no nível léxico, identificando a ocorrência exata de termos, a camada de busca semântica exigiu uma abordagem capaz de capturar a intenção e o significado contextual das consultas. Para essa tarefa, o processamento foi delegado a um módulo desenvolvido em Python (versão 3.11), escolha natural dada a predominância desta linguagem no ecossistema de Aprendizado Profundo (\textit{Deep Learning}), a maturidade de bibliotecas como \textit{PyTorch} e \textit{Hugging Face}, e a compatibilidade com as ferramentas necessárias para a vetorização.

    \subsubsection{Arquitetura do Componente Semântico}

    O componente, localizado no diretório \texttt{src/core\_python}, foi construído sobre o \textit{framework} \textbf{FastAPI}. A opção pelo FastAPI, em detrimento de alternativas mais tradicionais, deve-se à sua natureza assíncrona e de baixa latência, características ideais para que o desempenho do microsserviço não se tornasse um gargalo quando comparado ao módulo em Go de altíssima performance.

    A integração entre os módulos ocorre de forma desacoplada, utilizando o sistema de arquivos para a troca de dados. O fluxo de execução foi orquestrado manualmente para garantir a integridade de cada etapa:

    \begin{enumerate}
        \item O módulo em Go realiza a limpeza e normalização de todo o texto, persistindo o resultado em um diretório específico.
        \item O módulo em Python lê os arquivos processados e os submete à biblioteca \texttt{sentence-transformers}, que abstrai a complexidade da tokenização e da inferência na rede neural.
        \item O modelo gera e armazena em cache todos os vetores multidimensionais correspondentes aos documentos.
        \item O mesmo processo de vetorização é aplicado a cada frase de consulta definida nos testes.
        \item Por fim, o código calcula a similaridade de cosseno entre o vetor da frase e os vetores de todos os documentos, gerando as listas ordenadas de relevância.
    \end{enumerate}

    \subsubsection{Seleção de Modelos e Mudança de Escopo}

    Uma das decisões metodológicas mais importantes deste trabalho ocorreu durante a fase de definição dos modelos de IA. Inicialmente, o projeto de pesquisa previa uma análise comparativa ampla, confrontando não apenas algoritmos estatísticos, mas também três arquiteturas distintas de representação vetorial:
    \begin{enumerate}
        \item Um modelo \textit{Transformer} robusto e de grande porte (como o BERT-Large ou RoBERTa), visando a máxima precisão teórica.
        \item O modelo Word2Vec, baseado em arquiteturas neurais preditivas rasas.
        \item Um modelo GloVe, treinado com dados da rede social \textit{Twitter}, para avaliar o impacto de um vocabulário mais informal.
    \end{enumerate}

    No entanto, durante a fase exploratória e os testes preliminares, observou-se uma inconsistência significativa nos resultados. As variações nos \textit{rankings} de relevância gerados pelos três modelos para o \textit{corpus} jurídico apresentaram níveis de divergência (entropia) tão elevados que inviabilizariam uma comparação justa. Documentos considerados altamente relevantes por um modelo apareciam frequentemente como irrelevantes em outro, criando um cenário de incerteza que não justificava a complexidade de manter e orquestrar três ambientes de inferência pesados simultaneamente.

    Diante dessa evidência, optou-se por uma mudança estratégica de escopo. Em vez de dispersar o foco da análise usando diversos modelos de IA, o trabalho concentrou-se na dicotomia principal: "Estatística (Go) versus Semântica Contextual (Python/BERT)".

    Para representar o estado da arte na busca semântica eficiente, selecionou-se unicamente o modelo \texttt{paraphrase-multilingual-MiniLM-L12-v2} (baseado em BERT). A escolha deste modelo específico fundamentou-se em três pilares:
    \begin{itemize}
        \item \textbf{Eficiência de Inferência:} Sendo um modelo da família "MiniLM", ele possui apenas 12 camadas e gera vetores de 384 dimensões. Isso permitiu rodar a bateria massiva de testes (milhares de comparações de cosseno) em tempo hábil, sem exigir \textit{hardware} de supercomputação (GPUs dedicadas de alto custo).
        \item \textbf{Suporte Multilíngue:} O modelo foi treinado em mais de 50 idiomas, demonstrando capacidade robusta de entender a sintaxe do português sem a necessidade de \textit{fine-tuning} específico.
        \item \textbf{Espaço Vetorial Denso:} As 384 dimensões provaram ser suficientes para codificar as nuances dos textos legislativos, servindo como um "gabarito" confiável e estável para a avaliação dos algoritmos estatísticos.
    \end{itemize}

    \chapter{Apresentação dos Resultados e Considerações Finais}

    A etapa de validação experimental deste trabalho não consistiu apenas na execução mecânica de algoritmos mas sim em uma investigação sobre o comportamento de sistemas de recuperação de informação sob condições de estresse computacional. Para garantir a robustez dos dados executamos uma bateria de setenta e dois cenários de teste distintos onde cada um foi feito para isolar variáveis críticas do sistema. O objetivo central foi submeter a implementação desenvolvida na linguagem Go a diferentes pressões de configuração para entendermos de que maneira fatores como o tamanho dos N-gramas e a presença de saltos entre palavras influenciam tanto o desempenho da máquina quanto a qualidade da informação entregue ao usuário final.

    Os dados coletados e apresentados a seguir oferecem uma visão clara e técnica sobre como os algoritmos clássicos se comportam quando são confrontados com a infraestrutura moderna e com a complexidade inerente à linguagem jurídica. Ao longo deste capítulo detalhamos os destaques numéricos e discutimos os gargalos de armazenamento em banco de dados além de analisar a correlação de qualidade obtida frente ao gabarito semântico gerado por inteligência artificial.

    \section{Destaques e Indicadores de Performance }

    Antes de aprofundarmos a análise qualitativa é fundamental destacar os números mais expressivos que emergiram durante a fase de testes pois eles resumem os extremos de performance e consumo que definem os limites arquiteturais do sistema proposto.

    O indicador mais alarmante foi o pico de consumo de memória RAM que atingiu a marca de aproximadamente cinco gigabytes quando o sistema foi configurado para processar Trigramas. Esse valor contrasta com o cenário de Unigramas onde o consumo foi de apenas cento e quarenta e três megabytes. Essa diferença de magnitude ilustra o custo oculto da complexidade linguística.

    No que tange à velocidade de processamento observamos uma inversão de expectativas onde o algoritmo BM25 chegou a ser dez vezes mais rápido que o TF-IDF em cenários complexos. Isso derruba a intuição de que fórmulas matemáticas mais simples resultam necessariamente em softwares mais rápidos e coloca em evidência a importância da otimização de fluxo de código.

    Outro ponto de destaque negativo foi a performance da camada de persistência em disco. O uso do banco de dados SQLite para consultas em tempo real mostrou-se entre dez a quarenta vezes mais lento do que as operações realizadas em cache de memória. Esse gargalo inviabilizou o uso do disco para o processamento dos Trigramas e forçou uma mudança de estratégia para o uso exclusivo de memória RAM durante os testes de carga.

    Por fim vale ressaltar a escala do experimento que envolveu o processamento e indexação de 1648 documentos legislativos completos. As métricas de qualidade medidas pelo coeficiente de Spearman flutuaram majoritariamente entre zero vírgula zero dois e zero vírgula zero quatro o que evidencia matematicamente a distinção fundamental entre a busca estatística e a busca semântica.

    \subsection{Indicadores}

    Antes da análise qualitativa, alguns dos números que chamaram a atenção durante a fase de testes. Abaixo listamos os recordes e métricas que definem o perfil de desempenho da solução.

    \begin{itemize}
        \item \textbf{O Campeão de Velocidade} \\
        O algoritmo BM25 configurado para processar Bigramas simples sem saltos registrou o tempo de execução mais rápido de toda a bateria e completou a tarefa em \textbf{80 milissegundos}. Esse valor destaca a eficiência da estratégia de poda de dados para índices esparsos.

        \item \textbf{O Cenário de Maior Lentidão} \\
        No outro extremo a configuração de Trigramas normalizados com o uso de saltos máximos elevou o tempo de processamento para cerca de \textbf{7,5 segundos}. Isso representa uma execução quase cem vezes mais lenta do que o melhor cenário e evidencia o custo computacional da complexidade linguística excessiva.

        \item \textbf{Eficiência de Memória} \\
        Para sistemas com poucos recursos o uso de Unigramas provou ser extremamente leve exigindo apenas \textbf{143 megabytes} de memória RAM no seu pico. Isso viabiliza a execução da busca até mesmo em hardwares modestos ou contêineres de nuvem básicos.

        \item \textbf{O Custo do Contexto} \\
        Em contraste a indexação de Trigramas exigiu um pico de \textbf{4,72 gigabytes} de memória. Esse salto de trinta e duas vezes no consumo em relação aos Unigramas serve como um alerta importante sobre a escalabilidade de índices contextuais em memória.

        \item \textbf{Melhor Aproximação Semântica} \\
        Curiosamente a maior correlação positiva com o gabarito do Sentence-BERT foi encontrada nos testes de Unigramas que atingiram um pico de similaridade de \textbf{0,06}. Isso sugere que para este corpus específico a coincidência de palavras-chave simples alinha-se melhor com a intenção semântica do que as tentativas de capturar frases complexas.
    \end{itemize}

    \subsection{O Desempenho da Persistência em Disco}

    Um capítulo à parte nesta análise diz respeito ao comportamento do banco de dados. Durante o planejamento inicial do sistema a arquitetura previa o uso do SQLite para realizar consultas em tempo real e persistir os índices de N-gramas. No entanto a realidade dos testes de carga impôs uma mudança de estratégia.

    Observamos que as operações de leitura direta no disco foram de \textbf{dez a quarenta vezes mais lentas} do que as operações realizadas no cache de memória. Esse gargalo tornou-se proibitivo especialmente nos cenários de Bigramas e Trigramas onde a explosão combinatória de termos gerava milhares de consultas SQL por segundo. O disco simplesmente não conseguia acompanhar a velocidade do processador o que criava uma fila de espera que travava a execução do algoritmo.

    Por essa razão optamos por retirar o banco de dados do caminho crítico da busca em tempo real. Apesar disso a implementação não foi descartada e permanece funcional. Ela assumiu um papel vital como ferramenta de auditoria e segurança servindo como um repositório confiável para recuperação de dados em caso de falhas. Para trabalhos futuros a substituição do SQLite por tecnologias orientadas a colunas ou motores de busca dedicados como o Elasticsearch poderia mitigar essa latência e permitir uma arquitetura híbrida mais eficiente.

    \section*{A Relação entre os Modelos e o Gabarito Semântico}

    A comparação entre os rankings gerados pelos métodos estatísticos e pelo modelo semântico (Sentence-BERT) mostrou correlações baixas, ainda que positivas (média entre 0.02 e 0.04). Isso confirma que cada abordagem captura aspectos distintos:

    \begin{itemize}
        \item modelos estatísticos priorizam coincidência de termos específicos, algo comum em textos jurídicos;
        \item o modelo \textit{transformer} prioriza intenção e significado, mais útil em consultas abertas.
    \end{itemize}

    Na prática, os rankings são diferentes o suficiente para não serem intercambiáveis.

    \subsection{Estabilidade e o Papel dos \textit{Skip-grams}}

    A variação do \textit{jump} nos N-gramas maiores pouco afetou o Spearman. Mesmo ampliando o salto, os resultados permaneceram praticamente iguais. Isso mostra que, no Direito, as combinações relevantes costumam ser adjacentes (“Recurso de Apelação”, “Ação Direta”), tornando caro e pouco útil indexar palavras separadas por um ou dois termos.

    \section{Análise do Consumo de Memória RAM}

    Um dos aspectos mais críticos que observamos durante os experimentos foi a demanda de memória necessária para manter os índices invertidos operando com alta performance. Como a decisão de arquitetura priorizou carregar as estruturas de busca na memória RAM para garantir a velocidade das respostas o tamanho do vocabulário e das combinações de termos teve um impacto direto na viabilidade técnica da solução.

    Os relatórios de monitoramento revelaram um crescimento que não se comportou de forma linear mas sim exponencial à medida que aumentamos a complexidade da indexação. Quando o sistema operou apenas com Unigramas indexando palavras isoladas o consumo de memória foi extremamente modesto e eficiente ficando em torno de cento e quarenta e três megabytes no seu pico de utilização. Esse dado é extremamente positivo pois demonstra que para buscas simples baseadas apenas em palavras-chave o custo de infraestrutura é muito baixo e acessível até para servidores de entrada ou máquinas pessoais.

    A situação mudou de figura de forma drástica ao introduzirmos os Bigramas. A necessidade de armazenar pares de palavras e suas respectivas distâncias fez o consumo saltar para um pico de dois vírgula três gigabytes de memória. Esse valor representa um aumento de mais de dezesseis vezes em relação ao cenário anterior e já começa a exigir máquinas com maior capacidade. O cenário tornou-se ainda mais crítico com os Trigramas onde o sistema exigiu quase cinco gigabytes de memória RAM para operar corretamente.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.6\linewidth]{assets/custoMemoria}
        \caption{Consumo de memória para Unigramas, Bigramas e Trigramas}
        \label{fig:memoria_ram}
    \end{figure}

    Esses números indicam claramente que existe um custo oculto elevado na busca por precisão lexical. Enquanto indexar palavras isoladas é uma operação barata tentar capturar o contexto através de sequências de três termos exige uma quantidade de memória que pode inviabilizar a execução do sistema em ambientes com recursos limitados como dispositivos móveis ou contêineres de nuvem com pouca memória alocada.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.6\linewidth]{assets/tradeOff}
        \caption{Trade-off geral entre custo, memória e qualidade}
        \label{fig:tradeoff_final }
    \end{figure}

    \section{Desempenho Temporal}

    A análise do tempo de execução trouxe dois \textit{insights} fundamentais para o projeto que mudaram a nossa percepção sobre onde estão os verdadeiros gargalos de um sistema de busca. O primeiro diz respeito à eficiência dos algoritmos em memória e o segundo revela as limitações físicas do armazenamento em disco.

    No que tange aos algoritmos a teoria clássica sugere que o BM25 é mais complexo matematicamente do que o TF-IDF pois sua fórmula envolve mais operações logarítmicas e o cálculo da média de tamanho dos documentos. Contudo os dados mostraram o oposto em cenários de alta complexidade. Nos testes com Bigramas e Trigramas o BM25 superou o TF-IDF com uma margem larga de vantagem sendo até dez vezes mais veloz em casos específicos.

    A explicação para esse fenômeno reside na engenharia da implementação pois o código do BM25 permitiu uma otimização de fluxo conhecida como poda ou \textit{pruning}. Essa técnica descarta imediatamente termos que não aparecem no índice e economiza milhares de cálculos inúteis. Como o índice de Trigramas é muito esparso e a maioria das combinações de palavras não existe o BM25 consegue pular a maior parte do trabalho. O TF-IDF por sua vez acabou processando esses termos irrelevantes o que gerou um custo de processamento desnecessário e provou que a otimização de código é mais importante que a simplicidade da fórmula matemática.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.9\linewidth]{assets/tempoExecucao}
        \caption{Tempo de execução comparando TF-IDF e BM25}
        \label{fig:tempo_execucao}
    \end{figure}

    \subsection{A Performance do Banco de Dados versus Cache}

    Um ponto que merece uma discussão aprofundada foi a performance decepcionante da camada de persistência em disco. Durante as fases iniciais de desenvolvimento o sistema foi projetado para realizar consultas diretas ao banco de dados SQLite para cada termo da busca visando economizar memória RAM. No entanto os testes de carga revelaram que essa abordagem se tornava um gargalo severo que travava a execução do sistema.

    Observamos que as operações de leitura no banco de dados eram de dez a quarenta vezes mais lentas do que as operações realizadas diretamente no cache em memória. Essa degradação de performance foi especialmente notável nos cenários de Bigramas e Trigramas onde a quantidade de consultas SQL explodia exponencialmente para verificar cada combinação de palavras. Devido a essa latência proibitiva optamos por retirar o banco de dados do caminho crítico de execução dos testes principais focando os resultados na performance do processamento em memória.

    Apesar de não ter performado bem para a busca em tempo real neste cenário de alta frequência a implementação do banco de dados não foi descartada do projeto. Ela permanece funcional e cumpre um papel vital como ferramenta de auditoria e recuperação de dados em caso de desastres. O banco garante que os dados persistidos estejam seguros e consistentes permitindo que o índice em memória seja reconstruído rapidamente se o servidor for reiniciado. Para trabalhos futuros a substituição do SQLite por bancos de dados orientados a colunas ou soluções de busca dedicadas como o Elasticsearch poderia mitigar esse gargalo de disco e permitir buscas híbridas mais eficientes.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.6\linewidth]{assets/paralelismoQuery}
        \caption{Impacto do paralelismo na performance}
        \label{fig:impacto_paralelismo_query}
    \end{figure}

    \section{A Influência da Extensão da Consulta no Desempenho}

    Uma variável que se mostrou determinante para o comportamento do sistema foi a extensão da consulta realizada pelo usuário. Durante os testes variamos o tamanho das frases de entrada entre dez, vinte e quarenta palavras para simular desde buscas simples até a colagem de trechos inteiros de jurisprudência uma prática comum entre advogados.

    A análise dos dados revelou que o tempo de processamento cresce de forma linear em relação ao tamanho da frase. Isso era esperado pois cada palavra adicional na consulta exige que o algoritmo calcule o peso e a frequência de novos N-gramas. No entanto o que chamou a atenção foi como a execução paralela se comportou diante desse aumento de carga.

    Para frases curtas de dez palavras o custo de iniciar múltiplas rotinas de processamento quase anulou os ganhos de velocidade o que manteve o modo sequencial altamente competitivo. Contudo à medida que a frase cresceu a vantagem do paralelismo não tornou-se perceptível. Nos testes com Trigramas e quarenta palavras o processamento paralelo não conseguiu ser mais rápido que a execução sequencial. Isso demonstra que o uso da concorrência deve ser condicional ativando-se apenas dentro de cenários estudados, caso contrario não haverão ganhos que compensem a sobrecarga de gerenciar as \textit{threads} pelo volume de trabalho.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.6\linewidth]{assets/tamanhoFrases}
        \caption{Comparação entre execução sequencial e paralela para diferentes tamanhos de consulta }
        \label{fig:tamanho_consulta}
    \end{figure}

    Algoritmos paralelos podem ser mais lentos quando a sobrecarga de coordenação supera o ganho obtido pela divisão do trabalho. Isso inclui custos de criação e gerenciamento de threads, comunicação entre tarefas e sincronização frequente, que podem introduzir atrasos significativos. Além disso, quando as tarefas individuais são muito pequenas, o tempo gasto para distribuí-las e organizar sua execução pode ser maior que simplesmente executá-las de forma sequencial.

    Outro fator importante é o acesso concorrente a recursos compartilhados, que pode gerar contenção, bloqueios e espera ativa. Desbalanceamento de carga também reduz eficiência, alguns núcleos podem ficar ociosos enquanto outros executam partes mais pesadas. Em sistemas com hardware limitado, como poucos núcleos ou baixa largura de banda de memória, o paralelismo pode até prejudicar o desempenho, tornando a versão paralela mais lenta do que a sequencial.

    Além do tempo observamos também o impacto na qualidade. Curiosamente aumentar o tamanho da frase não garantiu uma correlação maior com o gabarito semântico. Isso indica que adicionar mais palavras não necessariamente resolve a ambiguidade se o modelo estatístico não for capaz de entender a relação entre elas o que reforça a necessidade de modelos híbridos para consultas complexas.

    \section{Avaliação da Qualidade e Correlação Semântica}

    A qualidade dos resultados foi medida comparando a ordem de relevância gerada pelos nossos algoritmos estatísticos com a ordem sugerida pelo modelo de inteligência artificial Sentence-BERT que serviu como nosso gabarito semântico. Utilizamos o coeficiente de Spearman para quantificar essa similaridade de posicionamento.

    Os valores de correlação encontrados pelo coeficiente de Spearman foram consistentemente baixos e oscilaram próximos de zero na grande maioria dos testes. Isso é um achado científico importante pois confirma que a busca estatística e a busca semântica olham para características fundamentalmente diferentes do texto. Enquanto a busca estatística prioriza a presença exata de palavras raras e termos técnicos a busca semântica tenta entender o contexto geral e a intenção da frase. O fato de a correlação ser baixa indica que um método não substitui o outro mas sim que eles funcionam de formas complementares e capturam nuances distintas da relevância.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\linewidth]{assets/graficoSpearmsn}
        \caption{Estabilidade do coeficiente de Spearman em diferentes configurações}
        \label{fig:estabilidade_spearman}
    \end{figure}

    Uma observação curiosa foi o comportamento dos índices ao variarmos o tamanho dos saltos ou \textit{jumps} entre palavras. A hipótese inicial era que permitir saltos melhoraria a qualidade da busca ao encontrar termos que não estivessem imediatamente colados uns aos outros como por exemplo encontrar "crime de responsabilidade" ao buscar "crime responsabilidade". Contudo os dados mostraram que aumentar o tamanho dos saltos de zero para dois não alterou o coeficiente de Spearman de maneira significativa nos testes com Trigramas.

    Isso sugere que a linguagem jurídica é extremamente rígida e formal. Termos técnicos como "habeas corpus" ou "dano moral" quase sempre aparecem juntos e na mesma ordem exata. A ocorrência dessas palavras separadas por outros termos é tão rara que indexá-las com saltos apenas consumiu mais memória e processamento sem trazer nenhum ganho real de relevância para o usuário final. Essa descoberta permite otimizar futuros sistemas removendo a complexidade dos saltos sem medo de perder qualidade na busca.

    \section{Considerações Finais}

    Ao concluir este estudo podemos afirmar com segurança que o trabalho atingiu seu objetivo de avaliar de forma prática e quantitativa o equilíbrio entre custo e benefício na recuperação de informação jurídica. Os setenta e dois cenários testados desenham um caminho claro para a engenharia de software neste domínio específico.

    Ficou evidente que o algoritmo BM25 é a escolha superior para a busca estatística entregando uma performance de velocidade muito acima do TF-IDF graças à sua capacidade de lidar com a esparsidade dos dados. Além disso identificamos que o "ponto ótimo" de infraestrutura reside na utilização de Bigramas sem saltos. Essa configuração oferece um contexto lexical suficiente sem incorrer no custo proibitivo de memória dos Trigramas ou na complexidade desnecessária dos \textit{skip-grams}.

    Também concluímos que embora modelos de inteligência artificial ofereçam uma compreensão profunda do texto o seu custo computacional é elevado. A baixa correlação entre os resultados estatísticos e semânticos sugere que a arquitetura ideal para um sistema jurídico real deve ser híbrida. O sistema em Go com BM25 pode atuar como um filtro primário de altíssima velocidade e baixo custo entregando um subconjunto de documentos para que o modelo em Python realize um refinamento semântico final apenas onde for necessário.

    Por fim as limitações encontradas especialmente no desempenho do banco de dados em disco abrem portas para investigações futuras. A substituição da camada de persistência por tecnologias mais robustas e a ampliação do \textit{corpus} para incluir jurisprudência são os próximos passos naturais para evoluir esta pesquisa. Este trabalho deixa como legado uma base sólida de dados e uma implementação modular capaz de guiar o desenvolvimento de soluções de busca eficientes para o judiciário brasileiro.

    \section{Limitações e Trabalhos Futuros}

    Algumas limitações direcionam caminhos para pesquisas futuras:

    \begin{enumerate}
        \item O gabarito semântico utilizado foi gerado por modelos \textit{transformer}, o que limita a avaliação da “verdadeira” precisão semântica. Um gabarito manual poderia fornecer uma base mais confiável.
        \item O \textit{corpus} analisado é predominantemente legislativo. Ampliar o estudo para acórdãos e sentenças permitiria testar a robustez das técnicas sob estruturas textuais mais variadas.
    \end{enumerate}

    Em resumo, o trabalho demonstrou que algoritmos estatísticos atuais, em especial o BM25, oferecem excelente eficiência computacional e podem sustentar sistemas de busca jurídica de baixo custo. Ao mesmo tempo, a baixa concordância com modelos semânticos reforça a necessidade de abordagens híbridas que combinem velocidade com profundidade contextual, especialmente em um domínio tão exigente quanto o jurídico.


% ========================================================================
% BIBLIOGRAFIA
% ========================================================================
    \bibliography{bibliografia}


% % ========================================================================
% % APENDICES
% % ========================================================================
% \begin{apendicesenv}

% \partapendices

% \chapter{\label{AnexoA}Exemplo de seção de anexo}

% \begin{lstlisting}
% EXEMPLO DE CODIGO A SER ADICIONADO
% \end{lstlisting}

% \end{apendicesenv}

\end{document}
